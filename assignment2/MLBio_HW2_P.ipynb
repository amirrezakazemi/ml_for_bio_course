{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: \n",
    " !!! If you don't fill these fields, your homework does not count !!!<by/>\n",
    " #### first name and last name :Amirreza Kazemi\n",
    " #### student number :95105827"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run cells by hitting `Shift` + `Enter` or `ctrl` + `Enter`. <br/>\n",
    "We highly recommend you to read each line of code carefully and try to understand what it exactly does. <br/>\n",
    "Just alter the parts that is between green comments and specified for you. Please do not change other parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. soft margin SVM\n",
    "### about the Data:<br/>\n",
    "The purpose of this project is to classify tumors into malignant or benign. The following dataset is constructed based on images of tumors. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
    "For more details about the features of this dataset you can visit this link:\n",
    "https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset<br/>\n",
    "This dataset contains 30 features and 1 label that is called target. We should find a proper hyperplane that separates malignant and benign samples.\n",
    "The original dataset labels is 0 and 1 and in the following code boxes we change it to -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension   ...    worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871   ...            17.33           184.60      2019.0   \n",
       "1                 0.05667   ...            23.41           158.80      1956.0   \n",
       "2                 0.05999   ...            25.53           152.50      1709.0   \n",
       "3                 0.09744   ...            26.50            98.87       567.7   \n",
       "4                 0.05883   ...            16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.8804920913884 %\n",
      "71.8804920913884 %\n",
      "20.035149384885763 %\n",
      "20.035149384885763 %\n",
      "8.084358523725834 %\n",
      "8.084358523725834 %\n"
     ]
    }
   ],
   "source": [
    "cancer.target = np.where(cancer.target==0, -1, cancer.target) \n",
    "X_train ,X_test ,X_val ,y_train ,y_test ,y_val = None ,None ,None ,None ,None ,None\n",
    "################################################################################\n",
    "# TODO: using train_test_split package, split your data into 3 numpy array     #\n",
    "# called X_train, X_test, and X_val and also split the corresponding labels as #\n",
    "# y_train, y_test, and y_val. After spliting, the ratio of your data should be # \n",
    "# approximately like this:                                                     #\n",
    "#  Train : 72%     test : 20%       validation : 8%                            #\n",
    "################################################################################\n",
    "#write your code here\n",
    "#random state number is given as train_test_split function input to output data that are tested before.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2, random_state = 9)\n",
    "X_train, X_val , y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.1, random_state = 9)\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "print((X_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft margin SVM optimization:<br/>\n",
    "We add 1 at the beginning of each Xs data (X_train, X_val , ...) and then the bias will be calculated implicitly.\n",
    "Then you should minimize the following SVM loss function (using gradient descent) with changing parameters of model.<br>\n",
    "In this notation: \n",
    "\\begin{equation}\n",
    "x_i , y_i\n",
    "\\end{equation}\n",
    "refers to feature vector of the sample and the label of our training data<br>\n",
    "and this is SVM loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "J(W) = \\frac{1}{N} \\sum_{i=1}^{N}{L^{(i)}} + \\frac{\\lambda}{2} ||W||^2\\\\\n",
    "\\large\n",
    "L^{(i)} ={max(0, 1 - y_i(w^{T}x_i)})\n",
    "\\;\\\\\n",
    "\\end{equation} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 31)\n",
      "(46, 31)\n",
      "(114, 31)\n"
     ]
    }
   ],
   "source": [
    "# >>>>>WARNING: RUN THIS CELL ONLY ONCE!<<<<<\n",
    "\n",
    "# adding 1s to the end of feature vectors to be multiplied by bias term of weights\n",
    "X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "print(X_train.shape)  \n",
    "print(X_val.shape)  \n",
    "print(X_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following functions in SVM class. In the part that you should compute loss function of this class, you are not allowed to use \"for\" loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T16:30:29.561420Z",
     "start_time": "2020-03-12T16:30:29.538696Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, n_features: int, std: float):\n",
    "        \"\"\"\n",
    "        n_features: number of features in (or the dimension of) each instance\n",
    "        std: standard deviation used in the initialization of the weights of svm\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        ################################################################################\n",
    "        # TODO: Initialize the weights of svm using random normal distribution with    #\n",
    "        # standard deviation equals to std.                                            #\n",
    "        ################################################################################\n",
    "        #write your code here\n",
    "        self.weights = np.random.normal(0, std, self.n_features)\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray, reg_coeff: float):\n",
    "        \"\"\"\n",
    "        X: training instances as a 2d-array with shape (num_train, n_features)\n",
    "        y: labels corresponsing to the given training instances as a 1d-array with shape (num_train,)\n",
    "        reg_coeff: L2-regularization coefficient\n",
    "        \"\"\"\n",
    "        #################################################################################\n",
    "        # TODO: Compute the hinge loss specified in the notebook and save it in the loss#                                                   # loss variable.                                                               #\n",
    "        # NOTE: YOU ARE NOT ALLOWED TO USE FOR LOOPS!                                   #\n",
    "        # Don't forget L2-regularization term in your implementation!                   #\n",
    "        #################################################################################\n",
    "        \n",
    "        #write your code here\n",
    "        loss = 0.0\n",
    "        term1 = 1 - y * np.dot(self.weights.T, X.T)\n",
    "        term1 = np.where(term1 < 0, 0, term1)\n",
    "        term2 = np.sum(term1) / np.size(term1)\n",
    "        loss = term2 + (reg_coeff/2) * np.dot(self.weights, self.weights)\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return loss\n",
    "        \n",
    "    def update_weights(self,  X: np.ndarray, y: np.ndarray, learning_rate: float , reg_coeff: float):\n",
    "        \"\"\"\n",
    "        Updates the weights of the svm using the gradient of computed loss with respect to the weights. \n",
    "        learning_rate: learning rate that will be used in gradient descent to update the weights\n",
    "        \"\"\"\n",
    "        ################################################################################\n",
    "        # TODO: Compute the gradient of loss computed above w.r.t the svm weights.     #\n",
    "        # and then update self.w with the computed gradient.                           #\n",
    "        # (don't forget learning rate and reg_coeff in update rule)                    #\n",
    "        # Don't forget L2-regularization term in your implementation!                  #\n",
    "        ################################################################################\n",
    "        #write your code here\n",
    "        term1 = 1 - y * np.dot(self.weights.T, X.T)\n",
    "        mask = np.where(term1 < 0)\n",
    "        term2 = np.multiply(X, -1 * y[:, np.newaxis])\n",
    "        term2[mask] = 0\n",
    "        dterm2 = np.sum(term2, axis=0) / X.shape[0]\n",
    "        dloss = dterm2 + reg_coeff * self.weights\n",
    "        self.weights -= learning_rate * dloss\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: Numpy 2d-array of instances\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        ################################################################################\n",
    "        # TODO: predict the labels for the instances in X and save them in y_pred.     #                                      #\n",
    "        ################################################################################\n",
    "        #write your code here\n",
    "        term1 = np.dot(self.weights.T, X.T)\n",
    "        y_pred = np.where(term1 >= 0, 1, -1)\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains your hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.0001\n",
    "num_iters = 15000\n",
    "reg_coeff = 20\n",
    "learning_rate=1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell using your SVM class, we want to train our model for cancer data:<br/>\n",
    "In every iteration you should see your training loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 0.969887, val acc 41.30%\n",
      "iteration 100, loss 0.916257, val acc 41.30%\n",
      "iteration 200, loss 0.900738, val acc 41.30%\n",
      "iteration 300, loss 0.886220, val acc 41.30%\n",
      "iteration 400, loss 0.872381, val acc 41.30%\n",
      "iteration 500, loss 0.859083, val acc 41.30%\n",
      "iteration 600, loss 0.845812, val acc 41.30%\n",
      "iteration 700, loss 0.832548, val acc 43.48%\n",
      "iteration 800, loss 0.819304, val acc 50.00%\n",
      "iteration 900, loss 0.806208, val acc 63.04%\n",
      "iteration 1000, loss 0.793181, val acc 65.22%\n",
      "iteration 1100, loss 0.780209, val acc 76.09%\n",
      "iteration 1200, loss 0.767242, val acc 84.78%\n",
      "iteration 1300, loss 0.754347, val acc 89.13%\n",
      "iteration 1400, loss 0.741523, val acc 93.48%\n",
      "iteration 1500, loss 0.728703, val acc 91.30%\n",
      "iteration 1600, loss 0.715932, val acc 89.13%\n",
      "iteration 1700, loss 0.703309, val acc 89.13%\n",
      "iteration 1800, loss 0.690717, val acc 89.13%\n",
      "iteration 1900, loss 0.678130, val acc 89.13%\n",
      "iteration 2000, loss 0.665556, val acc 86.96%\n",
      "iteration 2100, loss 0.653068, val acc 84.78%\n",
      "iteration 2200, loss 0.640617, val acc 82.61%\n",
      "iteration 2300, loss 0.628185, val acc 80.43%\n",
      "iteration 2400, loss 0.615792, val acc 80.43%\n",
      "iteration 2500, loss 0.603464, val acc 80.43%\n",
      "iteration 2600, loss 0.591416, val acc 80.43%\n",
      "iteration 2700, loss 0.579403, val acc 80.43%\n",
      "iteration 2800, loss 0.567410, val acc 80.43%\n",
      "iteration 2900, loss 0.555636, val acc 80.43%\n",
      "iteration 3000, loss 0.544098, val acc 80.43%\n",
      "iteration 3100, loss 0.533115, val acc 80.43%\n",
      "iteration 3200, loss 0.522632, val acc 80.43%\n",
      "iteration 3300, loss 0.512769, val acc 80.43%\n",
      "iteration 3400, loss 0.503436, val acc 80.43%\n",
      "iteration 3500, loss 0.494674, val acc 80.43%\n",
      "iteration 3600, loss 0.486451, val acc 80.43%\n",
      "iteration 3700, loss 0.478543, val acc 80.43%\n",
      "iteration 3800, loss 0.470875, val acc 80.43%\n",
      "iteration 3900, loss 0.463537, val acc 80.43%\n",
      "iteration 4000, loss 0.456588, val acc 80.43%\n",
      "iteration 4100, loss 0.450231, val acc 80.43%\n",
      "iteration 4200, loss 0.444530, val acc 80.43%\n",
      "iteration 4300, loss 0.439264, val acc 80.43%\n",
      "iteration 4400, loss 0.434151, val acc 80.43%\n",
      "iteration 4500, loss 0.429346, val acc 80.43%\n",
      "iteration 4600, loss 0.424736, val acc 80.43%\n",
      "iteration 4700, loss 0.420314, val acc 80.43%\n",
      "iteration 4800, loss 0.416121, val acc 80.43%\n",
      "iteration 4900, loss 0.412080, val acc 80.43%\n",
      "iteration 5000, loss 0.408170, val acc 80.43%\n",
      "iteration 5100, loss 0.404501, val acc 80.43%\n",
      "iteration 5200, loss 0.401102, val acc 80.43%\n",
      "iteration 5300, loss 0.397871, val acc 80.43%\n",
      "iteration 5400, loss 0.394720, val acc 80.43%\n",
      "iteration 5500, loss 0.391629, val acc 80.43%\n",
      "iteration 5600, loss 0.388746, val acc 80.43%\n",
      "iteration 5700, loss 0.386052, val acc 80.43%\n",
      "iteration 5800, loss 0.383382, val acc 80.43%\n",
      "iteration 5900, loss 0.380712, val acc 82.61%\n",
      "iteration 6000, loss 0.378078, val acc 82.61%\n",
      "iteration 6100, loss 0.375555, val acc 82.61%\n",
      "iteration 6200, loss 0.373176, val acc 84.78%\n",
      "iteration 6300, loss 0.370834, val acc 84.78%\n",
      "iteration 6400, loss 0.368493, val acc 84.78%\n",
      "iteration 6500, loss 0.366156, val acc 86.96%\n",
      "iteration 6600, loss 0.363901, val acc 86.96%\n",
      "iteration 6700, loss 0.361754, val acc 86.96%\n",
      "iteration 6800, loss 0.359614, val acc 86.96%\n",
      "iteration 6900, loss 0.357476, val acc 86.96%\n",
      "iteration 7000, loss 0.355371, val acc 86.96%\n",
      "iteration 7100, loss 0.353340, val acc 86.96%\n",
      "iteration 7200, loss 0.351385, val acc 86.96%\n",
      "iteration 7300, loss 0.349488, val acc 86.96%\n",
      "iteration 7400, loss 0.347600, val acc 86.96%\n",
      "iteration 7500, loss 0.345727, val acc 86.96%\n",
      "iteration 7600, loss 0.343911, val acc 86.96%\n",
      "iteration 7700, loss 0.342112, val acc 86.96%\n",
      "iteration 7800, loss 0.340320, val acc 86.96%\n",
      "iteration 7900, loss 0.338568, val acc 86.96%\n",
      "iteration 8000, loss 0.336874, val acc 86.96%\n",
      "iteration 8100, loss 0.335198, val acc 86.96%\n",
      "iteration 8200, loss 0.333526, val acc 86.96%\n",
      "iteration 8300, loss 0.331854, val acc 86.96%\n",
      "iteration 8400, loss 0.330183, val acc 86.96%\n",
      "iteration 8500, loss 0.328513, val acc 86.96%\n",
      "iteration 8600, loss 0.326848, val acc 86.96%\n",
      "iteration 8700, loss 0.325198, val acc 86.96%\n",
      "iteration 8800, loss 0.323559, val acc 86.96%\n",
      "iteration 8900, loss 0.321930, val acc 86.96%\n",
      "iteration 9000, loss 0.320304, val acc 86.96%\n",
      "iteration 9100, loss 0.318682, val acc 86.96%\n",
      "iteration 9200, loss 0.317096, val acc 86.96%\n",
      "iteration 9300, loss 0.315594, val acc 86.96%\n",
      "iteration 9400, loss 0.314133, val acc 86.96%\n",
      "iteration 9500, loss 0.312704, val acc 86.96%\n",
      "iteration 9600, loss 0.311315, val acc 86.96%\n",
      "iteration 9700, loss 0.310007, val acc 86.96%\n",
      "iteration 9800, loss 0.308735, val acc 86.96%\n",
      "iteration 9900, loss 0.307472, val acc 86.96%\n",
      "iteration 10000, loss 0.306234, val acc 86.96%\n",
      "iteration 10100, loss 0.305035, val acc 86.96%\n",
      "iteration 10200, loss 0.303855, val acc 86.96%\n",
      "iteration 10300, loss 0.302728, val acc 86.96%\n",
      "iteration 10400, loss 0.301643, val acc 86.96%\n",
      "iteration 10500, loss 0.300578, val acc 86.96%\n",
      "iteration 10600, loss 0.299522, val acc 86.96%\n",
      "iteration 10700, loss 0.298472, val acc 86.96%\n",
      "iteration 10800, loss 0.297464, val acc 86.96%\n",
      "iteration 10900, loss 0.296503, val acc 86.96%\n",
      "iteration 11000, loss 0.295571, val acc 86.96%\n",
      "iteration 11100, loss 0.294646, val acc 86.96%\n",
      "iteration 11200, loss 0.293723, val acc 86.96%\n",
      "iteration 11300, loss 0.292860, val acc 86.96%\n",
      "iteration 11400, loss 0.292050, val acc 86.96%\n",
      "iteration 11500, loss 0.291252, val acc 86.96%\n",
      "iteration 11600, loss 0.290454, val acc 86.96%\n",
      "iteration 11700, loss 0.289664, val acc 86.96%\n",
      "iteration 11800, loss 0.288890, val acc 86.96%\n",
      "iteration 11900, loss 0.288140, val acc 86.96%\n",
      "iteration 12000, loss 0.287402, val acc 86.96%\n",
      "iteration 12100, loss 0.286675, val acc 86.96%\n",
      "iteration 12200, loss 0.285964, val acc 86.96%\n",
      "iteration 12300, loss 0.285254, val acc 86.96%\n",
      "iteration 12400, loss 0.284550, val acc 86.96%\n",
      "iteration 12500, loss 0.283855, val acc 86.96%\n",
      "iteration 12600, loss 0.283163, val acc 86.96%\n",
      "iteration 12700, loss 0.282485, val acc 86.96%\n",
      "iteration 12800, loss 0.281836, val acc 86.96%\n",
      "iteration 12900, loss 0.281194, val acc 86.96%\n",
      "iteration 13000, loss 0.280551, val acc 86.96%\n",
      "iteration 13100, loss 0.279909, val acc 86.96%\n",
      "iteration 13200, loss 0.279277, val acc 86.96%\n",
      "iteration 13300, loss 0.278658, val acc 86.96%\n",
      "iteration 13400, loss 0.278039, val acc 86.96%\n",
      "iteration 13500, loss 0.277428, val acc 86.96%\n",
      "iteration 13600, loss 0.276821, val acc 86.96%\n",
      "iteration 13700, loss 0.276224, val acc 86.96%\n",
      "iteration 13800, loss 0.275652, val acc 86.96%\n",
      "iteration 13900, loss 0.275098, val acc 86.96%\n",
      "iteration 14000, loss 0.274545, val acc 86.96%\n",
      "iteration 14100, loss 0.274001, val acc 86.96%\n",
      "iteration 14200, loss 0.273471, val acc 86.96%\n",
      "iteration 14300, loss 0.272944, val acc 86.96%\n",
      "iteration 14400, loss 0.272429, val acc 86.96%\n",
      "iteration 14500, loss 0.271929, val acc 86.96%\n",
      "iteration 14600, loss 0.271436, val acc 86.96%\n",
      "iteration 14700, loss 0.270945, val acc 86.96%\n",
      "iteration 14800, loss 0.270453, val acc 86.96%\n",
      "iteration 14900, loss 0.269982, val acc 86.96%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model = SVM(n_features=X_train.shape[1], std= std )\n",
    "loss_history = []\n",
    "loss_val_history = []\n",
    "for it in range(num_iters):\n",
    "    loss = model.loss(X_train, y_train, reg_coeff)\n",
    "    loss_val = model.loss(X_val, y_val, reg_coeff)\n",
    "    if it % 100 == 0:\n",
    "        val_preds =  model.predict(X_val)\n",
    "        print('iteration %d, loss %f, val acc %.2f%%' % (it, loss,  accuracy_score(y_val,val_preds) * 100))\n",
    "    model.update_weights(X_train, y_train, learning_rate , reg_coeff)\n",
    "    loss_history.append(loss)\n",
    "    loss_val_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2792b84a6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################################################\n",
    "# TODO: using matplotlib.pyplot package plot the training loss and validation loss #\n",
    "# using loss_loss_history and loss_val_history                                     #\n",
    "####################################################################################\n",
    "\n",
    "#write your code here\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss_val_history, label='validation')\n",
    "plt.plot(loss_history, label='train')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "#                                 END OF YOUR CODE                                 #\n",
    "####################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "With changing your hyper parameters, find a configuration of hyper parameters that cause your loss to increase after each iteration and then report that configuration in the next cell. Explain why our loss increases?\n",
    "Write your answer in \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "std = \"0.0001\" <br>\n",
    "num_iters = \"15000\"<br>\n",
    "reg_coeff = \"20\"<br>\n",
    "learning_rate = \"1e-1\"<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "In this cell please explain the reason of this event<br>\n",
    "Answer:\n",
    "if alpha hyperparameter set too big , it causes that loss function diverges and therefore in each iteration loss increaces . it derives from taylor's formula in a way that if alpha is greater that epsilon , $f(x - \\alpha * \\nabla f) >= f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74, 8], [0, 32]]\n",
      "0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "val_preds =  model.predict(X_test)\n",
    "###########################################################################################\n",
    "# TODO: find the Confusion Matrix between val_preds and real labels (y_test) for test data#\n",
    "# then report the accuracy of the model.                                                  #\n",
    "# you are not allowed to use any premade function for accuracy and confusion matrix       #\n",
    "###########################################################################################\n",
    "\n",
    "#write your code here\n",
    "def confusion_matrix(val_preds, y_test):\n",
    "    numOfSamples = np.size(val_preds)\n",
    "    numOfSamples2 = np.size(y_test)\n",
    "    if numOfSamples is not numOfSamples2:\n",
    "        return \"invalid state to compute accuracy\"\n",
    "    FP, FN, TN, TP = 0, 0, 0, 0\n",
    "    for i in range(numOfSamples):\n",
    "        if val_preds[i] == 1 and y_test[i] == 1:\n",
    "            TP += 1\n",
    "        elif val_preds[i] == 1 and y_test[i] == -1:\n",
    "            FP += 1\n",
    "        elif val_preds[i] == -1 and y_test[i] == 1:\n",
    "            FN += 1\n",
    "        elif val_preds[i] == -1 and y_test[i] == -1:\n",
    "            TN += 1\n",
    "    array = [[TP, FP], [FN, TN]]       \n",
    "    return array\n",
    "def accuracy(confusion_matrix):\n",
    "    acc = (array[0][0] + array[1][1]) / (array[0][0] + array[1][1] + array[0][1] + array[1][0])\n",
    "    return acc\n",
    "array = confusion_matrix(val_preds, y_test)\n",
    "print(array)\n",
    "print(accuracy(array))\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "#                                END OF YOUR EXPLANATION                                  #\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we sample from training data with certain size (batch size) instead of using all the training data in each iteration, and train our model on batch data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.0001\n",
    "batch_size = 200\n",
    "num_iters = 15000\n",
    "reg_coeff = 20\n",
    "learning_rate=1e-8\n",
    "model = SVM(n_features=X_train.shape[1], std= std )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 0.961895, val acc 41.30%\n",
      "iteration 100, loss 0.902825, val acc 41.30%\n",
      "iteration 200, loss 0.946422, val acc 41.30%\n",
      "iteration 300, loss 0.959690, val acc 41.30%\n",
      "iteration 400, loss 0.993068, val acc 41.30%\n",
      "iteration 500, loss 0.904941, val acc 41.30%\n",
      "iteration 600, loss 0.921123, val acc 41.30%\n",
      "iteration 700, loss 0.905418, val acc 41.30%\n",
      "iteration 800, loss 0.869951, val acc 41.30%\n",
      "iteration 900, loss 0.894840, val acc 41.30%\n",
      "iteration 1000, loss 0.949034, val acc 41.30%\n",
      "iteration 1100, loss 0.906046, val acc 41.30%\n",
      "iteration 1200, loss 0.915586, val acc 41.30%\n",
      "iteration 1300, loss 0.887216, val acc 41.30%\n",
      "iteration 1400, loss 0.898429, val acc 41.30%\n",
      "iteration 1500, loss 0.928627, val acc 41.30%\n",
      "iteration 1600, loss 0.879738, val acc 41.30%\n",
      "iteration 1700, loss 0.857742, val acc 41.30%\n",
      "iteration 1800, loss 0.971214, val acc 41.30%\n",
      "iteration 1900, loss 0.899007, val acc 41.30%\n",
      "iteration 2000, loss 0.885353, val acc 41.30%\n",
      "iteration 2100, loss 0.936155, val acc 41.30%\n",
      "iteration 2200, loss 0.941868, val acc 41.30%\n",
      "iteration 2300, loss 0.905341, val acc 41.30%\n",
      "iteration 2400, loss 0.876472, val acc 41.30%\n",
      "iteration 2500, loss 0.832689, val acc 41.30%\n",
      "iteration 2600, loss 0.934284, val acc 41.30%\n",
      "iteration 2700, loss 0.842245, val acc 41.30%\n",
      "iteration 2800, loss 0.908483, val acc 41.30%\n",
      "iteration 2900, loss 0.857446, val acc 41.30%\n",
      "iteration 3000, loss 0.846869, val acc 41.30%\n",
      "iteration 3100, loss 0.923454, val acc 41.30%\n",
      "iteration 3200, loss 0.869760, val acc 41.30%\n",
      "iteration 3300, loss 0.887228, val acc 41.30%\n",
      "iteration 3400, loss 0.882738, val acc 41.30%\n",
      "iteration 3500, loss 0.807055, val acc 41.30%\n",
      "iteration 3600, loss 0.929794, val acc 41.30%\n",
      "iteration 3700, loss 0.862549, val acc 41.30%\n",
      "iteration 3800, loss 0.890530, val acc 41.30%\n",
      "iteration 3900, loss 0.857501, val acc 41.30%\n",
      "iteration 4000, loss 0.818102, val acc 41.30%\n",
      "iteration 4100, loss 0.842171, val acc 41.30%\n",
      "iteration 4200, loss 0.956130, val acc 41.30%\n",
      "iteration 4300, loss 0.836819, val acc 41.30%\n",
      "iteration 4400, loss 0.830228, val acc 41.30%\n",
      "iteration 4500, loss 0.888176, val acc 41.30%\n",
      "iteration 4600, loss 0.838858, val acc 41.30%\n",
      "iteration 4700, loss 0.858990, val acc 41.30%\n",
      "iteration 4800, loss 0.818345, val acc 41.30%\n",
      "iteration 4900, loss 0.877317, val acc 41.30%\n",
      "iteration 5000, loss 0.817393, val acc 41.30%\n",
      "iteration 5100, loss 0.826955, val acc 41.30%\n",
      "iteration 5200, loss 0.865076, val acc 41.30%\n",
      "iteration 5300, loss 0.844631, val acc 41.30%\n",
      "iteration 5400, loss 0.905539, val acc 41.30%\n",
      "iteration 5500, loss 0.796587, val acc 41.30%\n",
      "iteration 5600, loss 0.816327, val acc 41.30%\n",
      "iteration 5700, loss 0.847685, val acc 41.30%\n",
      "iteration 5800, loss 0.837991, val acc 43.48%\n",
      "iteration 5900, loss 0.856517, val acc 43.48%\n",
      "iteration 6000, loss 0.840430, val acc 43.48%\n",
      "iteration 6100, loss 0.863091, val acc 43.48%\n",
      "iteration 6200, loss 0.783868, val acc 43.48%\n",
      "iteration 6300, loss 0.865978, val acc 43.48%\n",
      "iteration 6400, loss 0.808929, val acc 43.48%\n",
      "iteration 6500, loss 0.885977, val acc 43.48%\n",
      "iteration 6600, loss 0.868278, val acc 43.48%\n",
      "iteration 6700, loss 0.784442, val acc 43.48%\n",
      "iteration 6800, loss 0.837878, val acc 43.48%\n",
      "iteration 6900, loss 0.846159, val acc 43.48%\n",
      "iteration 7000, loss 0.852456, val acc 43.48%\n",
      "iteration 7100, loss 0.793173, val acc 43.48%\n",
      "iteration 7200, loss 0.791249, val acc 45.65%\n",
      "iteration 7300, loss 0.829226, val acc 47.83%\n",
      "iteration 7400, loss 0.798248, val acc 45.65%\n",
      "iteration 7500, loss 0.830069, val acc 47.83%\n",
      "iteration 7600, loss 0.797130, val acc 47.83%\n",
      "iteration 7700, loss 0.841830, val acc 47.83%\n",
      "iteration 7800, loss 0.839309, val acc 50.00%\n",
      "iteration 7900, loss 0.804699, val acc 50.00%\n",
      "iteration 8000, loss 0.811001, val acc 50.00%\n",
      "iteration 8100, loss 0.806352, val acc 58.70%\n",
      "iteration 8200, loss 0.825132, val acc 58.70%\n",
      "iteration 8300, loss 0.839600, val acc 58.70%\n",
      "iteration 8400, loss 0.804313, val acc 58.70%\n",
      "iteration 8500, loss 0.828393, val acc 60.87%\n",
      "iteration 8600, loss 0.851701, val acc 58.70%\n",
      "iteration 8700, loss 0.784208, val acc 60.87%\n",
      "iteration 8800, loss 0.766811, val acc 60.87%\n",
      "iteration 8900, loss 0.804495, val acc 63.04%\n",
      "iteration 9000, loss 0.829363, val acc 63.04%\n",
      "iteration 9100, loss 0.810690, val acc 63.04%\n",
      "iteration 9200, loss 0.825735, val acc 63.04%\n",
      "iteration 9300, loss 0.804192, val acc 65.22%\n",
      "iteration 9400, loss 0.789663, val acc 63.04%\n",
      "iteration 9500, loss 0.793973, val acc 63.04%\n",
      "iteration 9600, loss 0.819251, val acc 65.22%\n",
      "iteration 9700, loss 0.871744, val acc 63.04%\n",
      "iteration 9800, loss 0.752385, val acc 65.22%\n",
      "iteration 9900, loss 0.792156, val acc 67.39%\n",
      "iteration 10000, loss 0.760764, val acc 71.74%\n",
      "iteration 10100, loss 0.782595, val acc 71.74%\n",
      "iteration 10200, loss 0.786451, val acc 71.74%\n",
      "iteration 10300, loss 0.754196, val acc 69.57%\n",
      "iteration 10400, loss 0.783355, val acc 69.57%\n",
      "iteration 10500, loss 0.791313, val acc 69.57%\n",
      "iteration 10600, loss 0.748023, val acc 71.74%\n",
      "iteration 10700, loss 0.778389, val acc 76.09%\n",
      "iteration 10800, loss 0.776630, val acc 78.26%\n",
      "iteration 10900, loss 0.776739, val acc 78.26%\n",
      "iteration 11000, loss 0.801251, val acc 78.26%\n",
      "iteration 11100, loss 0.758447, val acc 80.43%\n",
      "iteration 11200, loss 0.751725, val acc 80.43%\n",
      "iteration 11300, loss 0.790348, val acc 82.61%\n",
      "iteration 11400, loss 0.728462, val acc 84.78%\n",
      "iteration 11500, loss 0.761411, val acc 82.61%\n",
      "iteration 11600, loss 0.756487, val acc 84.78%\n",
      "iteration 11700, loss 0.739734, val acc 84.78%\n",
      "iteration 11800, loss 0.794105, val acc 84.78%\n",
      "iteration 11900, loss 0.769291, val acc 84.78%\n",
      "iteration 12000, loss 0.777153, val acc 84.78%\n",
      "iteration 12100, loss 0.761433, val acc 84.78%\n",
      "iteration 12200, loss 0.781213, val acc 86.96%\n",
      "iteration 12300, loss 0.789949, val acc 84.78%\n",
      "iteration 12400, loss 0.776464, val acc 89.13%\n",
      "iteration 12500, loss 0.744163, val acc 84.78%\n",
      "iteration 12600, loss 0.717128, val acc 89.13%\n",
      "iteration 12700, loss 0.766195, val acc 89.13%\n",
      "iteration 12800, loss 0.779986, val acc 86.96%\n",
      "iteration 12900, loss 0.712255, val acc 89.13%\n",
      "iteration 13000, loss 0.781586, val acc 89.13%\n",
      "iteration 13100, loss 0.740260, val acc 86.96%\n",
      "iteration 13200, loss 0.742660, val acc 89.13%\n",
      "iteration 13300, loss 0.724796, val acc 86.96%\n",
      "iteration 13400, loss 0.746365, val acc 86.96%\n",
      "iteration 13500, loss 0.736095, val acc 93.48%\n",
      "iteration 13600, loss 0.759044, val acc 93.48%\n",
      "iteration 13700, loss 0.763467, val acc 93.48%\n",
      "iteration 13800, loss 0.780393, val acc 93.48%\n",
      "iteration 13900, loss 0.755439, val acc 93.48%\n",
      "iteration 14000, loss 0.705517, val acc 93.48%\n",
      "iteration 14100, loss 0.743675, val acc 93.48%\n",
      "iteration 14200, loss 0.738638, val acc 93.48%\n",
      "iteration 14300, loss 0.672623, val acc 93.48%\n",
      "iteration 14400, loss 0.723761, val acc 93.48%\n",
      "iteration 14500, loss 0.768988, val acc 91.30%\n",
      "iteration 14600, loss 0.726573, val acc 91.30%\n",
      "iteration 14700, loss 0.751847, val acc 91.30%\n",
      "iteration 14800, loss 0.724337, val acc 91.30%\n",
      "iteration 14900, loss 0.737423, val acc 91.30%\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "loss_val_history = []\n",
    "for it in range(num_iters):\n",
    "    X_batch = None\n",
    "    y_batch = None\n",
    "    ################################################################################\n",
    "    # TODO: Sample batch_size elements from the training data and their            #\n",
    "    # corresponding labels to use in this round of gradient descent.               #\n",
    "    # Store the data in X_batch and their corresponding labels in                  #\n",
    "    # y_batch; after sampling X_batch should have shape (batch_size, n_features)   #\n",
    "    # and y_batch should have shape (batch_size,)                                  #\n",
    "    #                                                                              #\n",
    "    # Hint: Use np.random.choice to generate indices. Sampling with                #\n",
    "    # replacement is faster than sampling without replacement.                     #\n",
    "    ################################################################################\n",
    "    #write your code here\n",
    "    mask = np.random.choice(X_train.shape[0], batch_size, replace=True)\n",
    "    X_batch = X_train[mask]\n",
    "    y_batch = y_train[mask]\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "    loss = model.loss(X_batch, y_batch, reg_coeff)\n",
    "    loss_val = model.loss(X_val, y_val, reg_coeff)\n",
    "    if it % 100 == 0:\n",
    "        val_preds =  model.predict(X_val)\n",
    "        print('iteration %d, loss %f, val acc %.2f%%' % (it, loss,  accuracy_score(y_val,val_preds) * 100))\n",
    "    model.update_weights(X_batch, y_batch, learning_rate , reg_coeff)\n",
    "    loss_history.append(loss)\n",
    "    loss_val_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2792cb1e668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################################################\n",
    "# TODO: using matplotlib.pyplot package plot the training loss and validation loss #\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "#write your code here\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss_val_history, label='validation')\n",
    "plt.plot(loss_history, label='train')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "####################################################################################\n",
    "#                                 END OF YOUR CODE                                 #\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "Explain why we see fluctuation in this plot?<br>\n",
    "Answer: we see fluctuation because we just used a batch of data to update weights and however loss for this batch would decrease but whole data loss may increase and we save whole data loss , so there are fluctuations .  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you should know how to use and how to implement SVM from scratch.\n",
    "In fact, for perceptron we can use premade functions as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60 14]\n",
      " [ 2 38]]\n",
      "test acc 85.96%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "val_preds = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,val_preds,[1,-1]))\n",
    "print('test acc %.2f%%' % (accuracy_score(y_test,val_preds) * 100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "It is time for you to write your own code instead of completing some parts of a premade code.<br>\n",
    "So you can not use any premade functions like the previous cell.\n",
    "Write your code in the end of this .ipynb file <br>\n",
    "You should make your model and use that to build these outputs:<br>\n",
    "1- Report loss of training and accuracy of validation data on each epoch of training process.<br>\n",
    "2- You are allowed to use any normalization approach if need be.<br>\n",
    "3- Plot your training and validation loss vs number of iterations in one plot.<br>\n",
    "4- Finally print your confusion matrix and accuracy for your testing set.<br>\n",
    "With changing your hyperparameters try to get a good and reasonable accuracy and confusion matrix on testing set (similar to the accuracy when we used the package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8043478260869565 train loss 1891292.460010605 val loss 12467151.616021529\n",
      "validation accuracy 0.5869565217391305 train loss 22689197.49376817 val loss 188297149.58243844\n",
      "validation accuracy 0.6086956521739131 train loss 16271025.997713814 val loss 128345184.820003\n",
      "validation accuracy 0.6086956521739131 train loss 14589528.80209918 val loss 114279890.20706485\n",
      "validation accuracy 0.6086956521739131 train loss 15541465.490748834 val loss 121810727.55407636\n",
      "validation accuracy 0.8043478260869565 train loss 3406013.686792938 val loss 19115704.929657087\n",
      "validation accuracy 0.6739130434782609 train loss 11071774.073575761 val loss 85327659.38581845\n",
      "validation accuracy 0.6739130434782609 train loss 10761452.148693562 val loss 83010713.4485977\n",
      "validation accuracy 0.6739130434782609 train loss 11008868.11798347 val loss 85017875.48484933\n",
      "validation accuracy 0.7608695652173914 train loss 8862702.79289803 val loss 65083683.84701512\n",
      "validation accuracy 0.6739130434782609 train loss 13507623.489520695 val loss 104278278.79394375\n",
      "validation accuracy 0.6739130434782609 train loss 13247567.605348034 val loss 102018707.25447485\n",
      "validation accuracy 0.7391304347826086 train loss 11241676.468790755 val loss 84811743.19472644\n",
      "validation accuracy 0.7391304347826086 train loss 11938135.437686246 val loss 90364203.25424844\n",
      "validation accuracy 0.8043478260869565 train loss 8012002.656699385 val loss 51305460.209696725\n",
      "validation accuracy 0.8043478260869565 train loss 8253727.368266467 val loss 53165712.0864097\n",
      "validation accuracy 0.8043478260869565 train loss 7727844.941967462 val loss 47508173.28632944\n",
      "validation accuracy 0.8043478260869565 train loss 7577676.384969896 val loss 45765672.02167037\n",
      "validation accuracy 0.782608695652174 train loss 10722986.94126001 val loss 77000558.85788938\n",
      "validation accuracy 0.8043478260869565 train loss 7594006.83473218 val loss 44970766.31099667\n",
      "validation accuracy 0.8043478260869565 train loss 7479366.907747333 val loss 43793819.81592491\n",
      "validation accuracy 0.8043478260869565 train loss 8063419.515010157 val loss 48348981.03389385\n",
      "validation accuracy 0.8043478260869565 train loss 7335006.7868119 val loss 42309216.03243093\n",
      "validation accuracy 0.8695652173913043 train loss 2808116.044697745 val loss 13565803.844691038\n",
      "validation accuracy 0.8043478260869565 train loss 7172038.65745892 val loss 40706693.15035953\n",
      "validation accuracy 0.8043478260869565 train loss 6917605.96155129 val loss 38709485.01353344\n",
      "validation accuracy 0.8043478260869565 train loss 7290332.05350486 val loss 41206664.35906485\n",
      "validation accuracy 0.8043478260869565 train loss 8389774.73251401 val loss 49358757.704785295\n",
      "validation accuracy 0.8043478260869565 train loss 7983644.166379435 val loss 46056552.887290195\n",
      "validation accuracy 0.8043478260869565 train loss 7255231.438181134 val loss 40631766.1702683\n",
      "validation accuracy 0.8043478260869565 train loss 8354674.117190281 val loss 48522555.61441962\n",
      "validation accuracy 0.8043478260869565 train loss 7524541.41926722 val loss 42288700.56043256\n",
      "validation accuracy 0.8043478260869565 train loss 8449036.660246685 val loss 48962694.74557485\n",
      "validation accuracy 0.8043478260869565 train loss 8625603.855707569 val loss 50225728.18982924\n",
      "validation accuracy 0.8043478260869565 train loss 7981963.386295249 val loss 45308586.39433615\n",
      "validation accuracy 0.8043478260869565 train loss 7739039.007182494 val loss 43463779.81630883\n",
      "validation accuracy 0.8043478260869565 train loss 8276047.676291413 val loss 47219912.69660541\n",
      "validation accuracy 0.8043478260869565 train loss 8452614.871752292 val loss 48443285.096702024\n",
      "validation accuracy 0.8043478260869565 train loss 8629182.067213168 val loss 49673709.34094611\n",
      "validation accuracy 0.8043478260869565 train loss 7985541.597800842 val loss 44859272.538287774\n",
      "validation accuracy 0.8043478260869565 train loss 7579411.03166623 val loss 41828215.81028028\n",
      "validation accuracy 0.8043478260869565 train loss 8201030.681733966 val loss 46178878.19320434\n",
      "validation accuracy 0.8043478260869565 train loss 8377597.877194833 val loss 47362160.65325832\n",
      "validation accuracy 0.8043478260869565 train loss 8554165.07265571 val loss 48571696.392034546\n",
      "validation accuracy 0.8043478260869565 train loss 7952254.218592733 val loss 44142686.31013578\n",
      "validation accuracy 0.8043478260869565 train loss 6747084.205739172 val loss 36014886.608523004\n",
      "validation accuracy 0.8043478260869565 train loss 7564446.35983778 val loss 41273238.97618055\n",
      "validation accuracy 0.8043478260869565 train loss 6719717.820632296 val loss 35780394.61397855\n",
      "validation accuracy 0.8043478260869565 train loss 7111099.635725602 val loss 38207648.189619854\n",
      "validation accuracy 0.8043478260869565 train loss 10453541.959857136 val loss 63471131.54145326\n",
      "validation accuracy 0.8043478260869565 train loss 7477978.50531188 val loss 40424717.9361994\n",
      "validation accuracy 0.8043478260869565 train loss 7208458.835112213 val loss 38660405.19172673\n",
      "validation accuracy 0.8043478260869565 train loss 10550901.159243733 val loss 63890327.37425263\n",
      "validation accuracy 0.8043478260869565 train loss 7575337.704698475 val loss 40877474.93830618\n",
      "validation accuracy 0.8043478260869565 train loss 7532958.325072933 val loss 40593265.35965283\n",
      "validation accuracy 0.8043478260869565 train loss 7490578.945447392 val loss 40309055.780999474\n",
      "validation accuracy 0.8043478260869565 train loss 7448199.565821863 val loss 40024846.20234618\n",
      "validation accuracy 0.8043478260869565 train loss 7405820.186196338 val loss 39740636.62369294\n",
      "validation accuracy 0.8043478260869565 train loss 7363440.806570809 val loss 39456427.04503967\n",
      "validation accuracy 0.8043478260869565 train loss 7093921.136371154 val loss 37752208.092258856\n",
      "validation accuracy 0.8043478260869565 train loss 8534864.821531553 val loss 47158791.772734106\n",
      "validation accuracy 0.8043478260869565 train loss 8438317.499300934 val loss 46390425.85073903\n",
      "validation accuracy 0.8043478260869565 train loss 10311621.564424202 val loss 60741372.75974348\n",
      "validation accuracy 0.8043478260869565 train loss 7336058.109878933 val loss 38993856.61893861\n",
      "validation accuracy 0.8043478260869565 train loss 7066538.43967928 val loss 37339778.08474751\n",
      "validation accuracy 0.8260869565217391 train loss 6910463.174877714 val loss 36227503.61325682\n",
      "validation accuracy 0.8043478260869565 train loss 7416491.414744517 val loss 39471412.36280551\n",
      "validation accuracy 0.8043478260869565 train loss 7809604.289286566 val loss 41941306.61924274\n",
      "validation accuracy 0.8043478260869565 train loss 7818420.087728477 val loss 41833624.64081371\n",
      "validation accuracy 0.8043478260869565 train loss 7216509.233665504 val loss 38093008.022429466\n",
      "validation accuracy 0.8260869565217391 train loss 6979802.105778141 val loss 36458022.058626145\n",
      "validation accuracy 0.8260869565217391 train loss 6848461.408809771 val loss 35504915.46984917\n",
      "validation accuracy 0.8043478260869565 train loss 7323794.13782062 val loss 38716857.560740285\n",
      "validation accuracy 0.8043478260869565 train loss 7716907.012362668 val loss 41103105.14454354\n",
      "validation accuracy 0.8043478260869565 train loss 7725722.810804594 val loss 41044956.70691036\n",
      "validation accuracy 0.8260869565217391 train loss 7143821.83077613 val loss 37339539.752384655\n",
      "validation accuracy 0.8260869565217391 train loss 7012481.133807759 val loss 36370780.910578825\n",
      "validation accuracy 0.8260869565217391 train loss 6881140.436839391 val loss 35422994.333935335\n",
      "validation accuracy 0.8260869565217391 train loss 6983234.665518842 val loss 36153019.05925204\n",
      "validation accuracy 0.8260869565217391 train loss 6851893.968550477 val loss 35214485.801238045\n",
      "validation accuracy 0.8043478260869565 train loss 8233951.637150877 val loss 44177910.00016695\n",
      "validation accuracy 0.8043478260869565 train loss 8894966.3616166 val loss 48656832.53716241\n",
      "validation accuracy 0.8260869565217391 train loss 7205623.342515536 val loss 37358878.99256058\n",
      "validation accuracy 0.8260869565217391 train loss 7074282.64554717 val loss 36386649.01174491\n",
      "validation accuracy 0.8260869565217391 train loss 7176376.874226608 val loss 37133750.42784865\n",
      "validation accuracy 0.8260869565217391 train loss 7045036.177258242 val loss 36173168.575375944\n",
      "validation accuracy 0.8043478260869565 train loss 9234153.066266194 val loss 50920391.03069125\n",
      "validation accuracy 0.8043478260869565 train loss 7511753.369634335 val loss 39373158.623505875\n",
      "validation accuracy 0.8043478260869565 train loss 9175870.37482947 val loss 50387505.04060296\n",
      "validation accuracy 0.8260869565217391 train loss 7464567.851384537 val loss 38940825.64714295\n",
      "validation accuracy 0.8043478260869565 train loss 9117587.683392745 val loss 49857994.34191339\n",
      "validation accuracy 0.8260869565217391 train loss 7420879.017423043 val loss 38508492.67078002\n",
      "validation accuracy 0.8043478260869565 train loss 9059304.991956014 val loss 49341872.472748384\n",
      "validation accuracy 0.8260869565217391 train loss 7377190.183461547 val loss 38076159.69441709\n",
      "validation accuracy 0.8043478260869565 train loss 9001022.300519288 val loss 48825750.60358338\n",
      "validation accuracy 0.8260869565217391 train loss 7333501.349500047 val loss 37647463.66255339\n",
      "validation accuracy 0.8043478260869565 train loss 9107963.042500243 val loss 49497896.0994036\n",
      "validation accuracy 0.8260869565217391 train loss 7067094.527221818 val loss 35672079.241571695\n",
      "validation accuracy 0.8043478260869565 train loss 9222746.129450774 val loss 50249333.77213615\n",
      "validation accuracy 0.8043478260869565 train loss 9017218.624844594 val loss 48750138.85850287\n",
      "validation accuracy 0.8260869565217391 train loss 7357505.409373634 val loss 37564770.53533063\n",
      "validation accuracy 0.8043478260869565 train loss 8958935.933407865 val loss 48234725.677539736\n",
      "validation accuracy 0.8260869565217391 train loss 7313816.575412138 val loss 37138057.496037945\n",
      "validation accuracy 0.8043478260869565 train loss 8777773.25235046 val loss 46949240.39628941\n",
      "validation accuracy 0.8043478260869565 train loss 9905371.239054332 val loss 54978552.27969591\n",
      "validation accuracy 0.8043478260869565 train loss 8588229.695284367 val loss 45643879.507065445\n",
      "validation accuracy 0.8260869565217391 train loss 7277185.583303029 val loss 36763354.90976876\n",
      "validation accuracy 0.8043478260869565 train loss 8574946.174759097 val loss 45469509.42483844\n",
      "validation accuracy 0.8260869565217391 train loss 7173462.906396537 val loss 35909734.77775947\n",
      "validation accuracy 0.8043478260869565 train loss 9874255.178919725 val loss 54589658.56467505\n",
      "validation accuracy 0.8043478260869565 train loss 9877672.482449379 val loss 54636683.050227106\n",
      "validation accuracy 0.8260869565217391 train loss 6924373.781422731 val loss 34004542.142876804\n",
      "validation accuracy 0.8260869565217391 train loss 6849255.9397410415 val loss 33507238.035798807\n",
      "validation accuracy 0.8260869565217391 train loss 6774138.0980593655 val loss 33016129.80383069\n",
      "validation accuracy 0.8260869565217391 train loss 7677453.229968409 val loss 39341136.54475306\n",
      "validation accuracy 0.8043478260869565 train loss 10340722.218252648 val loss 58003222.36131884\n",
      "validation accuracy 0.8043478260869565 train loss 10273100.897495795 val loss 57493787.98197079\n",
      "validation accuracy 0.8043478260869565 train loss 9885783.89181564 val loss 54469687.88072921\n",
      "validation accuracy 0.8043478260869565 train loss 8568642.348045666 val loss 45257615.60332971\n",
      "validation accuracy 0.8260869565217391 train loss 7179006.194426895 val loss 35767820.62062379\n",
      "validation accuracy 0.8043478260869565 train loss 8440409.071012812 val loss 44350799.67797205\n",
      "validation accuracy 0.8043478260869565 train loss 8835649.478399556 val loss 46951368.38479686\n",
      "validation accuracy 0.8043478260869565 train loss 9963247.465103433 val loss 54883339.209277704\n",
      "validation accuracy 0.8043478260869565 train loss 8646105.92133346 val loss 45644497.533122644\n",
      "validation accuracy 0.8043478260869565 train loss 9773703.908037337 val loss 53398703.96669628\n",
      "validation accuracy 0.8043478260869565 train loss 8456562.36426737 val loss 44358657.028868094\n",
      "validation accuracy 0.8043478260869565 train loss 8851802.771654116 val loss 46950680.619554095\n",
      "validation accuracy 0.8260869565217391 train loss 6809904.885024987 val loss 32894841.690494765\n",
      "validation accuracy 0.8043478260869565 train loss 10244845.527109638 val loss 56895045.37307395\n",
      "validation accuracy 0.8043478260869565 train loss 10177224.206352785 val loss 56377528.29055238\n",
      "validation accuracy 0.8043478260869565 train loss 8860082.662582805 val loss 46959250.890583724\n",
      "validation accuracy 0.8260869565217391 train loss 6814953.777243223 val loss 32907288.199923698\n",
      "validation accuracy 0.8043478260869565 train loss 10253125.418038322 val loss 56922923.696668275\n",
      "validation accuracy 0.8043478260869565 train loss 8935983.874268334 val loss 47422377.31078084\n",
      "validation accuracy 0.8043478260869565 train loss 10216402.152471801 val loss 56621543.33244988\n",
      "validation accuracy 0.8043478260869565 train loss 8899260.60870181 val loss 47125605.219870985\n",
      "validation accuracy 0.8260869565217391 train loss 6848558.961857544 val loss 32976436.52691697\n",
      "validation accuracy 0.8260869565217391 train loss 6924720.023258444 val loss 33415703.374180853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8043478260869565 train loss 10230970.625417555 val loss 56631271.24442776\n",
      "validation accuracy 0.8043478260869565 train loss 8913829.081647558 val loss 47102735.023347974\n",
      "validation accuracy 0.8260869565217391 train loss 6865060.87630929 val loss 32881338.942764167\n",
      "validation accuracy 0.8260869565217391 train loss 6941221.937710179 val loss 33314341.36067771\n",
      "validation accuracy 0.8260869565217391 train loss 7017382.999111072 val loss 33756067.96080831\n",
      "validation accuracy 0.8260869565217391 train loss 6764691.840990629 val loss 32158608.69987688\n",
      "validation accuracy 0.8260869565217391 train loss 6840852.902391523 val loss 32590538.757908918\n",
      "validation accuracy 0.8260869565217391 train loss 6917013.963792412 val loss 33023541.175822474\n",
      "validation accuracy 0.8260869565217391 train loss 6993175.0251933 val loss 33456543.59373601\n",
      "validation accuracy 0.8478260869565217 train loss 6592459.941306133 val loss 30784772.601885952\n",
      "validation accuracy 0.8478260869565217 train loss 6626817.473118649 val loss 31039498.05395329\n",
      "validation accuracy 0.8478260869565217 train loss 6561009.328142331 val loss 30563838.07967174\n",
      "validation accuracy 0.8043478260869565 train loss 10292468.568515811 val loss 56553495.739112526\n",
      "validation accuracy 0.8478260869565217 train loss 6612171.903656148 val loss 30921213.928023107\n",
      "validation accuracy 0.8478260869565217 train loss 6646529.435468664 val loss 31175939.38009045\n",
      "validation accuracy 0.8260869565217391 train loss 6682210.470157184 val loss 31442444.007039294\n",
      "validation accuracy 0.8260869565217391 train loss 7721908.892099471 val loss 38573521.91086297\n",
      "validation accuracy 0.8043478260869565 train loss 10255389.380788527 val loss 56138220.764134504\n",
      "validation accuracy 0.8478260869565217 train loss 6627513.833135935 val loss 30979136.62394902\n",
      "validation accuracy 0.8260869565217391 train loss 7481934.791332563 val loss 36764049.907963485\n",
      "validation accuracy 0.8043478260869565 train loss 8481327.000383142 val loss 43746957.85069755\n",
      "validation accuracy 0.8260869565217391 train loss 8240183.242667593 val loss 42151464.765022635\n",
      "validation accuracy 0.8043478260869565 train loss 8403798.68300061 val loss 43195888.34828049\n",
      "validation accuracy 0.8260869565217391 train loss 7455167.530283847 val loss 36437459.87376131\n",
      "validation accuracy 0.8043478260869565 train loss 8446341.524134796 val loss 43423090.88257017\n",
      "validation accuracy 0.8260869565217391 train loss 8213415.981618881 val loss 41828275.459160715\n",
      "validation accuracy 0.8043478260869565 train loss 8368813.206752267 val loss 42880765.320325285\n",
      "validation accuracy 0.8260869565217391 train loss 7428400.269235131 val loss 36111093.867736995\n",
      "validation accuracy 0.8043478260869565 train loss 8411356.047886444 val loss 43106916.24392489\n",
      "validation accuracy 0.8260869565217391 train loss 8088505.454445142 val loss 40813470.04905407\n",
      "validation accuracy 0.8260869565217391 train loss 7650777.826635955 val loss 37639186.76922813\n",
      "validation accuracy 0.8043478260869565 train loss 8662865.94685783 val loss 44696738.24734925\n",
      "validation accuracy 0.8043478260869565 train loss 10310315.701290049 val loss 56091306.67866304\n",
      "validation accuracy 0.8260869565217391 train loss 7598295.171260137 val loss 37148043.226078995\n",
      "validation accuracy 0.8043478260869565 train loss 8602832.311289622 val loss 44216639.8477041\n",
      "validation accuracy 0.8260869565217391 train loss 7603721.650307007 val loss 37105637.23582947\n",
      "validation accuracy 0.8043478260869565 train loss 8607730.872445911 val loss 44200006.13513127\n",
      "validation accuracy 0.8260869565217391 train loss 7609148.129353862 val loss 37063231.24557987\n",
      "validation accuracy 0.8043478260869565 train loss 8404490.902437229 val loss 42813290.50680065\n",
      "validation accuracy 0.8043478260869565 train loss 10462616.757862639 val loss 57136815.33175033\n",
      "validation accuracy 0.8260869565217391 train loss 7789365.559412834 val loss 38261108.4085881\n",
      "validation accuracy 0.8260869565217391 train loss 7635597.216099032 val loss 37090353.8851844\n",
      "validation accuracy 0.8043478260869565 train loss 8425091.474246388 val loss 42798666.27199931\n",
      "validation accuracy 0.8260869565217391 train loss 8382856.476396417 val loss 42424285.269667074\n",
      "validation accuracy 0.8260869565217391 train loss 8342304.895680714 val loss 42049904.26733485\n",
      "validation accuracy 0.8260869565217391 train loss 8301753.314965014 val loss 41690244.741045445\n",
      "validation accuracy 0.8043478260869565 train loss 10325262.127695534 val loss 55578977.22398396\n",
      "validation accuracy 0.8260869565217391 train loss 7767866.733323433 val loss 37739632.09727479\n",
      "validation accuracy 0.8260869565217391 train loss 7790376.828296973 val loss 37831173.48103431\n",
      "validation accuracy 0.8260869565217391 train loss 7812886.923270514 val loss 37923554.52351943\n",
      "validation accuracy 0.8260869565217391 train loss 8457800.20131891 val loss 42620041.27286015\n",
      "validation accuracy 0.8260869565217391 train loss 8274351.0576385325 val loss 41191786.11667614\n",
      "validation accuracy 0.8043478260869565 train loss 10280887.732057719 val loss 54996974.402006686\n",
      "validation accuracy 0.8260869565217391 train loss 8362867.659071803 val loss 41763308.5727112\n",
      "validation accuracy 0.8043478260869565 train loss 10378905.401425065 val loss 55624976.7844379\n",
      "validation accuracy 0.8260869565217391 train loss 8082835.018675234 val loss 39726292.9937982\n",
      "validation accuracy 0.8260869565217391 train loss 8001253.864593288 val loss 39054098.94259208\n",
      "validation accuracy 0.8260869565217391 train loss 7919672.710511335 val loss 38386682.84984657\n",
      "validation accuracy 0.8043478260869565 train loss 9845310.748359619 val loss 51622853.14082974\n",
      "validation accuracy 0.8260869565217391 train loss 7923264.329792768 val loss 38303747.99133381\n",
      "validation accuracy 0.8043478260869565 train loss 9844069.130299602 val loss 51548554.41172634\n",
      "validation accuracy 0.8260869565217391 train loss 7926855.949074197 val loss 38220813.13282104\n",
      "validation accuracy 0.8260869565217391 train loss 7845274.794992246 val loss 37560058.72958498\n",
      "validation accuracy 0.8043478260869565 train loss 8972921.373002743 val loss 45598111.169283986\n",
      "validation accuracy 0.8260869565217391 train loss 7942967.756655213 val loss 38271992.606096506\n",
      "validation accuracy 0.8260869565217391 train loss 7861386.602573266 val loss 37612584.65972597\n",
      "validation accuracy 0.8043478260869565 train loss 8990209.013038382 val loss 45649408.15673335\n",
      "validation accuracy 0.8260869565217391 train loss 7959079.56423622 val loss 38323172.07937193\n",
      "validation accuracy 0.8043478260869565 train loss 9805424.743442256 val loss 51099958.64815878\n",
      "validation accuracy 0.8260869565217391 train loss 7899217.931196133 val loss 37827651.136124164\n",
      "validation accuracy 0.8043478260869565 train loss 9032132.984521799 val loss 45876580.39697783\n",
      "validation accuracy 0.8260869565217391 train loss 7996910.892859099 val loss 38539295.71692246\n",
      "validation accuracy 0.8043478260869565 train loss 9765035.623626094 val loss 50776412.29574704\n",
      "validation accuracy 0.8260869565217391 train loss 8047276.471348694 val loss 38790826.27464524\n",
      "validation accuracy 0.8260869565217391 train loss 7965695.3172667455 val loss 38124061.698978655\n",
      "validation accuracy 0.8260869565217391 train loss 7820660.91086328 val loss 37072183.53276422\n",
      "validation accuracy 0.8043478260869565 train loss 8933554.472202836 val loss 45054784.5166491\n",
      "validation accuracy 0.8260869565217391 train loss 8420562.838193644 val loss 41406841.18810455\n",
      "validation accuracy 0.8260869565217391 train loss 8237113.694513258 val loss 39995991.297202945\n",
      "validation accuracy 0.8043478260869565 train loss 9221339.690343805 val loss 46914447.52980439\n",
      "validation accuracy 0.8260869565217391 train loss 8501244.891047068 val loss 41872364.470769666\n",
      "validation accuracy 0.8043478260869565 train loss 9734357.518366082 val loss 50318552.58568092\n",
      "validation accuracy 0.8260869565217391 train loss 8033915.836884741 val loss 38389214.45721193\n",
      "validation accuracy 0.8260869565217391 train loss 7952334.682802787 val loss 37743840.07096441\n",
      "validation accuracy 0.8043478260869565 train loss 9072710.41418784 val loss 45773186.07824199\n",
      "validation accuracy 0.8260869565217391 train loss 8050027.64446575 val loss 38442099.34636161\n",
      "validation accuracy 0.8260869565217391 train loss 7898805.666473582 val loss 37346708.46042232\n",
      "validation accuracy 0.8043478260869565 train loss 9007684.9629239 val loss 45309606.165731676\n",
      "validation accuracy 0.8260869565217391 train loss 8498707.593803952 val loss 41662620.756729715\n",
      "validation accuracy 0.8260869565217391 train loss 8490045.416348984 val loss 41593852.02703294\n",
      "validation accuracy 0.8260869565217391 train loss 8481383.238894012 val loss 41525083.29733617\n",
      "validation accuracy 0.8260869565217391 train loss 8472721.061439041 val loss 41458311.13923082\n",
      "validation accuracy 0.8260869565217391 train loss 8464058.883984074 val loss 41393965.22146355\n",
      "validation accuracy 0.8260869565217391 train loss 8455396.706529107 val loss 41330975.369213745\n",
      "validation accuracy 0.8043478260869565 train loss 9671011.46160318 val loss 49713807.61110909\n",
      "validation accuracy 0.8260869565217391 train loss 7918426.828456566 val loss 37460043.55338435\n",
      "validation accuracy 0.8043478260869565 train loss 9024907.264501773 val loss 45398610.303345315\n",
      "validation accuracy 0.8260869565217391 train loss 8341116.685425799 val loss 40423736.26472598\n",
      "validation accuracy 0.8260869565217391 train loss 8331517.583291747 val loss 40311153.56445247\n",
      "validation accuracy 0.8260869565217391 train loss 8321918.481157694 val loss 40198570.86417896\n",
      "validation accuracy 0.8260869565217391 train loss 8312319.379023643 val loss 40085988.16390546\n",
      "validation accuracy 0.8043478260869565 train loss 9618356.92854348 val loss 49208777.38584936\n",
      "validation accuracy 0.8260869565217391 train loss 7954126.785413068 val loss 37390431.17854969\n",
      "validation accuracy 0.8043478260869565 train loss 9054565.82274166 val loss 45390963.20091536\n",
      "validation accuracy 0.8260869565217391 train loss 8376816.642382318 val loss 40388350.481683955\n",
      "validation accuracy 0.8260869565217391 train loss 8731863.036560712 val loss 43019586.263862506\n",
      "validation accuracy 0.8260869565217391 train loss 8069133.997505314 val loss 38097157.52549568\n",
      "validation accuracy 0.8260869565217391 train loss 7854458.767191632 val loss 36594536.09450163\n",
      "validation accuracy 0.8260869565217391 train loss 8939877.663216278 val loss 44558433.106535286\n",
      "validation accuracy 0.8260869565217391 train loss 8277148.624160886 val loss 39555820.38730392\n",
      "validation accuracy 0.8260869565217391 train loss 8062473.393847204 val loss 37995983.19962034\n",
      "validation accuracy 0.8043478260869565 train loss 9186766.579326192 val loss 46118603.30095525\n",
      "validation accuracy 0.8260869565217391 train loss 8133946.572257191 val loss 38477349.33799623\n",
      "validation accuracy 0.8260869565217391 train loss 7919271.34194351 val loss 36963295.604199916\n",
      "validation accuracy 0.8043478260869565 train loss 9017077.954344902 val loss 44970253.60163753\n",
      "validation accuracy 0.8260869565217391 train loss 8341961.198912765 val loss 39967640.88240616\n",
      "validation accuracy 0.8260869565217391 train loss 6979644.683202801 val loss 31031255.11676027\n",
      "validation accuracy 0.8260869565217391 train loss 8059538.725069594 val loss 37849142.53395045\n",
      "validation accuracy 0.8043478260869565 train loss 9181600.897568928 val loss 45992286.87599451\n",
      "validation accuracy 0.8260869565217391 train loss 8131011.903479573 val loss 38330508.67232624\n",
      "validation accuracy 0.8043478260869565 train loss 9264059.411132235 val loss 46520658.75157532\n",
      "validation accuracy 0.8260869565217391 train loss 8202485.081889564 val loss 38811874.810702145\n",
      "validation accuracy 0.8260869565217391 train loss 7987809.8515758775 val loss 37287277.17054038\n",
      "validation accuracy 0.8043478260869565 train loss 9094370.786150951 val loss 45341838.652392596\n",
      "validation accuracy 0.8260869565217391 train loss 7883727.343075279 val loss 36560109.23139146\n",
      "validation accuracy 0.8043478260869565 train loss 8972044.932767259 val loss 44534343.30393437\n",
      "validation accuracy 0.8260869565217391 train loss 8201464.976304444 val loss 38758153.25402517\n",
      "validation accuracy 0.8043478260869565 train loss 9344613.522733804 val loss 47008754.592773765\n",
      "validation accuracy 0.8260869565217391 train loss 8272938.154714443 val loss 39251267.039239675\n",
      "validation accuracy 0.8260869565217391 train loss 8627984.548892828 val loss 41876437.54275006\n",
      "validation accuracy 0.8260869565217391 train loss 8119393.76479208 val loss 38080593.66267692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8043478260869565 train loss 9239583.922626052 val loss 46214977.5471344\n",
      "validation accuracy 0.8260869565217391 train loss 8013654.872840948 val loss 37299001.67131491\n",
      "validation accuracy 0.8043478260869565 train loss 9113903.905024398 val loss 45329657.37814893\n",
      "validation accuracy 0.8260869565217391 train loss 7907915.980889818 val loss 36542473.99878201\n",
      "validation accuracy 0.8260869565217391 train loss 8993334.876914473 val loss 44469856.17090538\n",
      "validation accuracy 0.8260869565217391 train loss 7803833.472389219 val loss 35833564.31720939\n",
      "validation accuracy 0.8260869565217391 train loss 8889252.36841387 val loss 43669885.69742382\n",
      "validation accuracy 0.8260869565217391 train loss 8121571.105618356 val loss 37967812.498676196\n",
      "validation accuracy 0.8043478260869565 train loss 9238466.624005554 val loss 46093716.98353658\n",
      "validation accuracy 0.8260869565217391 train loss 8015832.213667228 val loss 37197511.68936018\n",
      "validation accuracy 0.8043478260869565 train loss 9112786.606403891 val loss 45210127.30237125\n",
      "validation accuracy 0.8260869565217391 train loss 7911749.705166623 val loss 36481263.56067461\n",
      "validation accuracy 0.8260869565217391 train loss 8997168.601191275 val loss 44397654.426481545\n",
      "validation accuracy 0.8260869565217391 train loss 7807667.196666014 val loss 35774066.71214169\n",
      "validation accuracy 0.8260869565217391 train loss 8066194.288749511 val loss 37468828.28777957\n",
      "validation accuracy 0.8043478260869565 train loss 9170995.92854727 val loss 45543970.80626116\n",
      "validation accuracy 0.8260869565217391 train loss 7962111.780248906 val loss 36752580.15909399\n",
      "validation accuracy 0.8043478260869565 train loss 9048670.075163577 val loss 44729243.27780351\n",
      "validation accuracy 0.8260869565217391 train loss 7858029.271748301 val loss 36040781.01851365\n",
      "validation accuracy 0.8260869565217391 train loss 8116556.3638318125 val loss 37743830.23887551\n",
      "validation accuracy 0.8043478260869565 train loss 9229205.250690661 val loss 45883405.77798529\n",
      "validation accuracy 0.8260869565217391 train loss 8012473.855331205 val loss 37023896.75751345\n",
      "validation accuracy 0.8043478260869565 train loss 9106879.39730696 val loss 45060832.12912547\n",
      "validation accuracy 0.8260869565217391 train loss 7908391.346830592 val loss 36307648.628827885\n",
      "validation accuracy 0.8260869565217391 train loss 8166918.438914113 val loss 38026028.81597627\n",
      "validation accuracy 0.8260869565217391 train loss 8521964.83309254 val loss 40591207.42369066\n",
      "validation accuracy 0.8260869565217391 train loss 7696172.000200664 val loss 34894920.91124481\n",
      "validation accuracy 0.8260869565217391 train loss 7954699.092284178 val loss 36483310.34166969\n",
      "validation accuracy 0.8260869565217391 train loss 9040117.988308825 val loss 44453239.289622486\n",
      "validation accuracy 0.8260869565217391 train loss 7850616.583783578 val loss 35811959.54687914\n",
      "validation accuracy 0.8260869565217391 train loss 8109143.675867083 val loss 37466567.525752306\n",
      "validation accuracy 0.8043478260869565 train loss 9212143.292086426 val loss 45603681.59606528\n",
      "validation accuracy 0.8260869565217391 train loss 8005061.167366477 val loss 36748126.22475636\n",
      "validation accuracy 0.8260869565217391 train loss 9090480.06339113 val loss 44784828.14094456\n",
      "validation accuracy 0.8260869565217391 train loss 7900978.658865875 val loss 36076775.42996583\n",
      "validation accuracy 0.8260869565217391 train loss 8159505.750949384 val loss 37775947.28292704\n",
      "validation accuracy 0.8260869565217391 train loss 8337340.074766665 val loss 39000579.07849075\n",
      "validation accuracy 0.8260869565217391 train loss 7511547.241874771 val loss 33651105.41124479\n",
      "validation accuracy 0.8260869565217391 train loss 8494865.257627718 val loss 40093954.72466816\n",
      "validation accuracy 0.8260869565217391 train loss 7669072.42473584 val loss 34575561.062225536\n",
      "validation accuracy 0.8260869565217391 train loss 7004668.354510679 val loss 30571404.654111706\n",
      "validation accuracy 0.8260869565217391 train loss 8065710.3732269285 val loss 37051079.39493115\n",
      "validation accuracy 0.8260869565217391 train loss 8007368.926366957 val loss 36629338.74469496\n",
      "validation accuracy 0.8260869565217391 train loss 8265896.018450476 val loss 38364054.17354834\n",
      "validation accuracy 0.8260869565217391 train loss 7440103.185558619 val loss 33139864.19613199\n",
      "validation accuracy 0.8260869565217391 train loss 8061209.30241525 val loss 36915005.52548967\n",
      "validation accuracy 0.8260869565217391 train loss 7316109.237789616 val loss 32356947.676075347\n",
      "validation accuracy 0.8260869565217391 train loss 7014284.19233756 val loss 30517382.554112278\n",
      "validation accuracy 0.8260869565217391 train loss 8075326.211053797 val loss 37001376.74741665\n",
      "validation accuracy 0.8260869565217391 train loss 7330226.146428159 val loss 32399195.831796892\n",
      "validation accuracy 0.8260869565217391 train loss 7028401.10097613 val loss 30559630.709833957\n",
      "validation accuracy 0.8260869565217391 train loss 8402146.159583818 val loss 39160938.39966009\n",
      "validation accuracy 0.8260869565217391 train loss 7576353.326691974 val loss 33833185.53543073\n",
      "validation accuracy 0.8260869565217391 train loss 6911949.256466816 val loss 29841807.618709072\n",
      "validation accuracy 0.8260869565217391 train loss 8285694.315074501 val loss 38305400.9901841\n",
      "validation accuracy 0.8260869565217391 train loss 7459901.482182629 val loss 33102583.952913497\n",
      "validation accuracy 0.8260869565217391 train loss 8081007.599039277 val loss 36885389.72680174\n",
      "validation accuracy 0.8260869565217391 train loss 7335907.534413643 val loss 32319667.432857\n",
      "validation accuracy 0.8260869565217391 train loss 7034082.488961616 val loss 30480102.310894087\n",
      "validation accuracy 0.8260869565217391 train loss 8407827.547569301 val loss 39020212.91392484\n",
      "validation accuracy 0.8260869565217391 train loss 7582034.71467744 val loss 33753657.13649071\n",
      "validation accuracy 0.8260869565217391 train loss 8634993.554340634 val loss 40633456.16801535\n",
      "validation accuracy 0.8260869565217391 train loss 7253433.544967501 val loss 31752714.1573764\n",
      "validation accuracy 0.8260869565217391 train loss 6951608.49951547 val loss 29929392.61279413\n",
      "validation accuracy 0.8260869565217391 train loss 8325353.558123158 val loss 38372945.863143\n",
      "validation accuracy 0.8260869565217391 train loss 7499560.725231294 val loss 33186703.861010112\n",
      "validation accuracy 0.8260869565217391 train loss 8946720.89982861 val loss 42779235.10968865\n",
      "validation accuracy 0.8260869565217391 train loss 7417235.169931322 val loss 32667654.75344272\n",
      "validation accuracy 0.8260869565217391 train loss 8038341.28678796 val loss 36386203.34440218\n",
      "validation accuracy 0.8260869565217391 train loss 7293241.222162312 val loss 31884738.233386107\n",
      "validation accuracy 0.8260869565217391 train loss 8708779.086598638 val loss 40898716.842184566\n",
      "validation accuracy 0.8260869565217391 train loss 7327219.077225479 val loss 32048386.5404225\n",
      "validation accuracy 0.8260869565217391 train loss 8742756.94166181 val loss 41085552.92496547\n",
      "validation accuracy 0.8260869565217391 train loss 7361196.932288652 val loss 32212034.847458966\n",
      "validation accuracy 0.8260869565217391 train loss 8776734.796724988 val loss 41274768.095635325\n",
      "validation accuracy 0.8260869565217391 train loss 7395174.787351825 val loss 32375683.154495414\n",
      "validation accuracy 0.8260869565217391 train loss 8842334.961949147 val loss 41671312.00276581\n",
      "validation accuracy 0.8260869565217391 train loss 8928634.967124734 val loss 42279473.69175735\n",
      "validation accuracy 0.8260869565217391 train loss 7399149.237227446 val loss 32365242.863106325\n",
      "validation accuracy 0.8260869565217391 train loss 8452108.076890642 val loss 38959827.22784768\n",
      "validation accuracy 0.8260869565217391 train loss 7626315.243998784 val loss 33697253.14332558\n",
      "validation accuracy 0.8260869565217391 train loss 8679274.083661985 val loss 40514681.25961888\n",
      "validation accuracy 0.8260869565217391 train loss 7297714.074288839 val loss 31696310.16421125\n",
      "validation accuracy 0.8260869565217391 train loss 8744874.248886155 val loss 40890377.71121922\n",
      "validation accuracy 0.8260869565217391 train loss 7363314.239512998 val loss 32056931.004011072\n",
      "validation accuracy 0.8260869565217391 train loss 8810474.41411031 val loss 41270725.2493395\n",
      "validation accuracy 0.8260869565217391 train loss 8896774.419285899 val loss 41854014.250864476\n",
      "validation accuracy 0.8260869565217391 train loss 7196143.0592976315 val loss 31021005.800618682\n",
      "validation accuracy 0.8260869565217391 train loss 8611680.923733955 val loss 39884828.65493482\n",
      "validation accuracy 0.8260869565217391 train loss 7230120.914360786 val loss 31182256.245225437\n",
      "validation accuracy 0.8260869565217391 train loss 8677281.08895811 val loss 40260525.106535055\n",
      "validation accuracy 0.8260869565217391 train loss 7295721.079584946 val loss 31536087.96411834\n",
      "validation accuracy 0.8260869565217391 train loss 8742881.254182268 val loss 40636221.55813533\n",
      "validation accuracy 0.8260869565217391 train loss 8829181.259357858 val loss 41207670.319533005\n",
      "validation accuracy 0.8260869565217391 train loss 8958231.113819417 val loss 42057767.56892119\n",
      "validation accuracy 0.8260869565217391 train loss 8896605.398470892 val loss 41620209.24323345\n",
      "validation accuracy 0.8260869565217391 train loss 8834979.68312237 val loss 41189413.803005494\n",
      "validation accuracy 0.8260869565217391 train loss 8964029.537583929 val loss 42046468.46696012\n",
      "validation accuracy 0.8260869565217391 train loss 8902403.822235376 val loss 41608910.14127218\n",
      "validation accuracy 0.8260869565217391 train loss 8840778.106886832 val loss 41171351.81558426\n",
      "validation accuracy 0.8913043478260869 train loss 1640162.5554294782 val loss 10599157.384991359\n",
      "validation accuracy 0.8260869565217391 train loss 8873102.162356567 val loss 41407843.575815395\n",
      "validation accuracy 0.8260869565217391 train loss 8811476.447008014 val loss 40978346.651098\n",
      "validation accuracy 0.8260869565217391 train loss 9328606.732269663 val loss 44591315.49375328\n",
      "validation accuracy 0.8260869565217391 train loss 8732126.815196753 val loss 40414024.01195826\n",
      "validation accuracy 0.8260869565217391 train loss 7587262.631689764 val loss 33191982.45305167\n",
      "validation accuracy 0.8260869565217391 train loss 8671843.781513926 val loss 40001326.49450226\n",
      "validation accuracy 0.8260869565217391 train loss 8758143.786689518 val loss 40572775.2558999\n",
      "validation accuracy 0.8260869565217391 train loss 9275274.071951173 val loss 44119226.03811587\n",
      "validation accuracy 0.8260869565217391 train loss 9298250.772516154 val loss 44255076.80721451\n",
      "validation accuracy 0.8260869565217391 train loss 9321227.473081134 val loss 44390927.57631314\n",
      "validation accuracy 0.8260869565217391 train loss 9344204.173646118 val loss 44526778.345411785\n",
      "validation accuracy 0.8260869565217391 train loss 9367180.874211103 val loss 44669401.24666433\n",
      "validation accuracy 0.8260869565217391 train loss 7417764.402261777 val loss 32107916.97568444\n",
      "validation accuracy 0.8260869565217391 train loss 8470723.24192496 val loss 38478816.97272784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8260869565217391 train loss 7644930.409033093 val loss 33431787.82352136\n",
      "validation accuracy 0.8260869565217391 train loss 8729511.558857262 val loss 40190211.61997986\n",
      "validation accuracy 0.8913043478260869 train loss 1625476.7106003496 val loss 10763854.48432048\n",
      "validation accuracy 0.8260869565217391 train loss 7487921.576358438 val loss 32474351.122913525\n",
      "validation accuracy 0.8260869565217391 train loss 8572502.726182617 val loss 39182257.56370234\n",
      "validation accuracy 0.8260869565217391 train loss 8658802.731358197 val loss 39744911.82035233\n",
      "validation accuracy 0.8260869565217391 train loss 7513938.547851209 val loss 32624951.24893164\n",
      "validation accuracy 0.8260869565217391 train loss 8598519.697675372 val loss 39340174.75162044\n",
      "validation accuracy 0.8260869565217391 train loss 8684819.702850964 val loss 39903663.06429399\n",
      "validation accuracy 0.8260869565217391 train loss 9201949.988112606 val loss 43377176.80738582\n",
      "validation accuracy 0.8260869565217391 train loss 9224926.688677594 val loss 43503982.52306291\n",
      "validation accuracy 0.8260869565217391 train loss 9247903.389242576 val loss 43630788.23873998\n",
      "validation accuracy 0.8260869565217391 train loss 9270880.089807563 val loss 43757593.95441708\n",
      "validation accuracy 0.8260869565217391 train loss 9293856.790372545 val loss 43884399.67009418\n",
      "validation accuracy 0.8260869565217391 train loss 7344440.318423209 val loss 31613310.764381632\n",
      "validation accuracy 0.8260869565217391 train loss 8454949.37185596 val loss 38217876.73425557\n",
      "validation accuracy 0.8913043478260869 train loss 1853413.6620366164 val loss 10799407.921569949\n",
      "validation accuracy 0.8260869565217391 train loss 7532108.810825263 val loss 32629123.24004686\n",
      "validation accuracy 0.8260869565217391 train loss 7877760.024351781 val loss 34647046.66209165\n",
      "validation accuracy 0.8260869565217391 train loss 9238260.712877795 val loss 43485934.13267188\n",
      "validation accuracy 0.8260869565217391 train loss 9261237.413442776 val loss 43612739.84834897\n",
      "validation accuracy 0.8260869565217391 train loss 9284214.114007764 val loss 43739545.56402605\n",
      "validation accuracy 0.8260869565217391 train loss 7334797.642058427 val loss 31492786.61486902\n",
      "validation accuracy 0.8260869565217391 train loss 8445306.695491169 val loss 38099125.56142664\n",
      "validation accuracy 0.8260869565217391 train loss 7619513.862599293 val loss 33051347.674129896\n",
      "validation accuracy 0.8260869565217391 train loss 7965165.076125822 val loss 35117551.84089219\n",
      "validation accuracy 0.8260869565217391 train loss 7596685.408976241 val loss 32924555.368014142\n",
      "validation accuracy 0.8260869565217391 train loss 8681266.55880041 val loss 39699564.89195703\n",
      "validation accuracy 0.8260869565217391 train loss 7536402.375293416 val loss 32581027.525987715\n",
      "validation accuracy 0.8260869565217391 train loss 8620983.525117604 val loss 39296601.80301067\n",
      "validation accuracy 0.8260869565217391 train loss 9138113.810379254 val loss 42729815.21633369\n",
      "validation accuracy 0.8260869565217391 train loss 7246806.880580875 val loss 30991135.041656435\n",
      "validation accuracy 0.8260869565217391 train loss 9075951.666781146 val loss 42316853.92185034\n",
      "validation accuracy 0.8478260869565217 train loss 7187195.585681428 val loss 30633736.54127082\n",
      "validation accuracy 0.8260869565217391 train loss 8295153.790415513 val loss 37173777.12618758\n",
      "validation accuracy 0.8260869565217391 train loss 7469360.957523642 val loss 32170760.29583408\n",
      "validation accuracy 0.8260869565217391 train loss 8553942.10734782 val loss 38860129.65899547\n",
      "validation accuracy 0.8260869565217391 train loss 9028322.543323524 val loss 42008657.63630961\n",
      "validation accuracy 0.8478260869565217 train loss 7146427.575794077 val loss 30359705.38566151\n",
      "validation accuracy 0.8260869565217391 train loss 7845245.851824818 val loss 34331597.886123344\n",
      "validation accuracy 0.8478260869565217 train loss 7117327.476809455 val loss 30156910.624794\n",
      "validation accuracy 0.8260869565217391 train loss 9071442.54680961 val loss 42228216.50155914\n",
      "validation accuracy 0.8478260869565217 train loss 7182999.508612425 val loss 30549015.051396646\n",
      "validation accuracy 0.8260869565217391 train loss 9009280.403211514 val loss 41815255.207075864\n",
      "validation accuracy 0.8478260869565217 train loss 7126404.904221343 val loss 30191616.551011115\n",
      "validation accuracy 0.8260869565217391 train loss 7826203.711712797 val loss 34162503.427031964\n",
      "validation accuracy 0.8478260869565217 train loss 7097304.805236698 val loss 29988821.790143427\n",
      "validation accuracy 0.8260869565217391 train loss 7789333.885386815 val loss 33938827.693438075\n",
      "validation accuracy 0.8478260869565217 train loss 7068204.706252053 val loss 29786027.02927582\n",
      "validation accuracy 0.8260869565217391 train loss 7752464.059060824 val loss 33718242.3048666\n",
      "validation accuracy 0.8478260869565217 train loss 7039104.60726742 val loss 29583232.268408243\n",
      "validation accuracy 0.8260869565217391 train loss 7524009.665162336 val loss 32364978.43714627\n",
      "validation accuracy 0.8260869565217391 train loss 8996671.245786622 val loss 41700241.038524285\n",
      "validation accuracy 0.8478260869565217 train loss 6973241.528422702 val loss 29147415.29741604\n",
      "validation accuracy 0.8260869565217391 train loss 7774833.65325053 val loss 33808719.05016511\n",
      "validation accuracy 0.8478260869565217 train loss 7049832.172660736 val loss 29644951.289773874\n",
      "validation accuracy 0.8260869565217391 train loss 7546379.259352052 val loss 32451983.79051998\n",
      "validation accuracy 0.8260869565217391 train loss 9019040.839976337 val loss 41825490.23148827\n",
      "validation accuracy 0.8478260869565217 train loss 6983969.093816023 val loss 29208993.56790369\n",
      "validation accuracy 0.8260869565217391 train loss 7797203.247440264 val loss 33899195.795463726\n",
      "validation accuracy 0.8478260869565217 train loss 7060559.738054065 val loss 29706670.3111396\n",
      "validation accuracy 0.8260869565217391 train loss 7892718.1501678815 val loss 34483104.83388492\n",
      "validation accuracy 0.8260869565217391 train loss 7147618.0855422355 val loss 30204347.054375425\n",
      "validation accuracy 0.8260869565217391 train loss 8982858.690939642 val loss 41531440.13573584\n",
      "validation accuracy 0.8478260869565217 train loss 6952780.740297091 val loss 28956759.978335958\n",
      "validation accuracy 0.8260869565217391 train loss 7623503.865116381 val loss 32838783.325967334\n",
      "validation accuracy 0.8260869565217391 train loss 9096165.445740677 val loss 42296151.83567225\n",
      "validation accuracy 0.8260869565217391 train loss 9054111.442400184 val loss 42022879.7455717\n",
      "validation accuracy 0.8478260869565217 train loss 7004752.599997463 val loss 29318015.311828155\n",
      "validation accuracy 0.8260869565217391 train loss 7832273.849864122 val loss 34074737.481092855\n",
      "validation accuracy 0.8260869565217391 train loss 7087173.7852384895 val loss 29815692.055064093\n",
      "validation accuracy 0.8260869565217391 train loss 7927788.752591742 val loss 34667227.47715069\n",
      "validation accuracy 0.8260869565217391 train loss 7182688.687966107 val loss 30313368.79829991\n",
      "validation accuracy 0.8260869565217391 train loss 8023303.655319361 val loss 35259717.47320848\n",
      "validation accuracy 0.8260869565217391 train loss 7278203.5906937225 val loss 30811045.541535743\n",
      "validation accuracy 0.8695652173913043 train loss 1211588.9467629557 val loss 11440232.627925362\n",
      "validation accuracy 0.8260869565217391 train loss 7365487.310094569 val loss 31274514.81327481\n",
      "validation accuracy 0.8260869565217391 train loss 7228334.74297765 val loss 30509395.491788056\n",
      "validation accuracy 0.8260869565217391 train loss 9063575.348375056 val loss 41906124.321790114\n",
      "validation accuracy 0.8260869565217391 train loss 9021521.34503455 val loss 41633606.89339294\n",
      "validation accuracy 0.8260869565217391 train loss 8979467.34169405 val loss 41361089.464995794\n",
      "validation accuracy 0.8260869565217391 train loss 8091586.567906225 val loss 35615720.10336824\n",
      "validation accuracy 0.8260869565217391 train loss 8047076.197382769 val loss 35352535.901675925\n",
      "validation accuracy 0.8260869565217391 train loss 7301976.132757117 val loss 30884167.566346735\n",
      "validation accuracy 0.8260869565217391 train loss 7164823.565640203 val loss 30122852.516870704\n",
      "validation accuracy 0.8260869565217391 train loss 7867921.299706286 val loss 34200609.33160506\n",
      "validation accuracy 0.8260869565217391 train loss 7122821.235080619 val loss 29881997.36958836\n",
      "validation accuracy 0.8260869565217391 train loss 7825918.969146712 val loss 33943556.53856954\n",
      "validation accuracy 0.8260869565217391 train loss 7080818.904521063 val loss 29641142.222306173\n",
      "validation accuracy 0.8260869565217391 train loss 7783916.638587148 val loss 33686503.74553409\n",
      "validation accuracy 0.8260869565217391 train loss 7038816.573961511 val loss 29400287.07502404\n",
      "validation accuracy 0.8260869565217391 train loss 7741914.3080276 val loss 33429450.95249872\n",
      "validation accuracy 0.8260869565217391 train loss 6996814.243401949 val loss 29159431.927741855\n",
      "validation accuracy 0.8260869565217391 train loss 7699911.977468042 val loss 33172398.15946327\n",
      "validation accuracy 0.8260869565217391 train loss 7562759.410351107 val loss 32321261.637532026\n",
      "validation accuracy 0.8260869565217391 train loss 7425606.843234187 val loss 31508691.094716422\n",
      "validation accuracy 0.8260869565217391 train loss 7288454.276117279 val loss 30696416.415287603\n",
      "validation accuracy 0.8695652173913043 train loss 1333388.7854069797 val loss 11036953.969565734\n",
      "validation accuracy 0.8913043478260869 train loss 1275342.0708224475 val loss 11211235.292166885\n",
      "validation accuracy 0.8913043478260869 train loss 1222941.026705054 val loss 11386686.343648424\n",
      "validation accuracy 0.8260869565217391 train loss 7344820.991743048 val loss 31022111.06919673\n",
      "validation accuracy 0.8260869565217391 train loss 7207668.424626128 val loss 30227977.848532334\n",
      "validation accuracy 0.8260869565217391 train loss 7910766.158692213 val loss 34389410.1984905\n",
      "validation accuracy 0.8695652173913043 train loss 1343191.8588352608 val loss 10992288.156057265\n",
      "validation accuracy 0.8913043478260869 train loss 1286313.3111946983 val loss 11166940.30474164\n",
      "validation accuracy 0.8913043478260869 train loss 1233912.2670773063 val loss 11342391.356223185\n",
      "validation accuracy 0.8260869565217391 train loss 7359185.37680924 val loss 31098969.924199264\n",
      "validation accuracy 0.8260869565217391 train loss 7222032.809692328 val loss 30302150.16169037\n",
      "validation accuracy 0.8260869565217391 train loss 7925130.543758417 val loss 34469495.82247341\n",
      "validation accuracy 0.8260869565217391 train loss 7180030.479132767 val loss 30058002.604413226\n",
      "validation accuracy 0.8260869565217391 train loss 7883128.213198851 val loss 34210643.87113192\n",
      "validation accuracy 0.8260869565217391 train loss 7138028.148573187 val loss 29820184.708799284\n",
      "validation accuracy 0.8260869565217391 train loss 7841125.882639265 val loss 33951791.919790305\n",
      "validation accuracy 0.8260869565217391 train loss 7096025.818013596 val loss 29582698.018646985\n",
      "validation accuracy 0.8260869565217391 train loss 7799123.552079678 val loss 33692939.9684487\n",
      "validation accuracy 0.8260869565217391 train loss 7054023.487454017 val loss 29345211.328494683\n",
      "validation accuracy 0.8260869565217391 train loss 7757121.221520089 val loss 33435655.064230293\n",
      "validation accuracy 0.8260869565217391 train loss 7619968.654403181 val loss 32602801.41389022\n",
      "validation accuracy 0.8260869565217391 train loss 7482816.087286266 val loss 31771514.810673326\n",
      "validation accuracy 0.8260869565217391 train loss 7345663.5201693615 val loss 30944464.655523926\n",
      "validation accuracy 0.8913043478260869 train loss 1389111.857097575 val loss 10896075.504932567\n",
      "validation accuracy 0.8913043478260869 train loss 1336710.8129801834 val loss 11061798.286893146\n",
      "validation accuracy 0.8913043478260869 train loss 1284309.7688627895 val loss 11237249.338374691\n",
      "validation accuracy 0.8260869565217391 train loss 7402030.235795139 val loss 31282334.11927392\n",
      "validation accuracy 0.8913043478260869 train loss 1423045.2481338875 val loss 10844113.101204388\n",
      "validation accuracy 0.8913043478260869 train loss 1370644.204016496 val loss 10980605.56568159\n",
      "validation accuracy 0.8913043478260869 train loss 1318243.159899102 val loss 11156056.617163133\n",
      "validation accuracy 0.8260869565217391 train loss 7458396.951420915 val loss 31620203.58302391\n",
      "validation accuracy 0.8913043478260869 train loss 1456978.6391701978 val loss 10798643.161211655\n",
      "validation accuracy 0.8913043478260869 train loss 1404577.5950528085 val loss 10915542.552900292\n",
      "validation accuracy 0.8913043478260869 train loss 1352176.5509354132 val loss 11074863.895951573\n",
      "validation accuracy 0.8913043478260869 train loss 1299775.5068180235 val loss 11250314.947433122\n",
      "validation accuracy 0.8260869565217391 train loss 7376620.92879133 val loss 31143776.120675497\n",
      "validation accuracy 0.8913043478260869 train loss 1438510.9860891197 val loss 10861489.656115254\n",
      "validation accuracy 0.8913043478260869 train loss 1386109.941971724 val loss 10997495.056614771\n",
      "validation accuracy 0.8913043478260869 train loss 1333708.8978543351 val loss 11169122.22622155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8260869565217391 train loss 7432987.644417103 val loss 31481645.584425483\n",
      "validation accuracy 0.8913043478260869 train loss 1472444.3771254318 val loss 10817842.727399841\n",
      "validation accuracy 0.8260869565217391 train loss 7858282.033147042 val loss 34027982.290902466\n",
      "validation accuracy 0.8913043478260869 train loss 1395812.2066311194 val loss 10963997.85586792\n",
      "validation accuracy 0.8913043478260869 train loss 1343411.1625137306 val loss 11129708.632090112\n",
      "validation accuracy 0.8913043478260869 train loss 1402625.2553899242 val loss 10880917.202887183\n",
      "validation accuracy 0.8913043478260869 train loss 1350224.2112725317 val loss 11042724.863806734\n",
      "validation accuracy 0.8913043478260869 train loss 1297823.1671551447 val loss 11218175.915288284\n",
      "validation accuracy 0.8260869565217391 train loss 7353276.700447208 val loss 31023986.03721395\n",
      "validation accuracy 0.8913043478260869 train loss 1436558.6464262386 val loss 10824097.721992891\n",
      "validation accuracy 0.8260869565217391 train loss 7778571.089177147 val loss 33564944.418440565\n",
      "validation accuracy 0.8913043478260869 train loss 1359926.4759319308 val loss 11003311.269675294\n",
      "validation accuracy 0.8913043478260869 train loss 1307525.4318145397 val loss 11178762.321156852\n",
      "validation accuracy 0.8260869565217391 train loss 7365133.045549533 val loss 31097806.19962144\n",
      "validation accuracy 0.8913043478260869 train loss 1446260.9110856336 val loss 10799490.519517804\n",
      "validation accuracy 0.8260869565217391 train loss 7790427.434279471 val loss 33638857.60908266\n",
      "validation accuracy 0.8913043478260869 train loss 1369628.7405913295 val loss 10968374.777230995\n",
      "validation accuracy 0.8913043478260869 train loss 1317227.696473932 val loss 11139348.727025405\n",
      "validation accuracy 0.8913043478260869 train loss 1376441.7893501278 val loss 10885294.124250252\n",
      "validation accuracy 0.8913043478260869 train loss 1324040.7452327362 val loss 11052364.958742034\n",
      "validation accuracy 0.8260869565217391 train loss 7138478.268020097 val loss 29885149.327766076\n",
      "validation accuracy 0.8260869565217391 train loss 7841576.002086181 val loss 33984874.85868427\n",
      "validation accuracy 0.8913043478260869 train loss 1239500.9358273344 val loss 11332669.495082278\n",
      "validation accuracy 0.8260869565217391 train loss 7960598.015450026 val loss 34755856.58372556\n",
      "validation accuracy 0.8260869565217391 train loss 7157589.188894497 val loss 29968234.926629063\n",
      "validation accuracy 0.8260869565217391 train loss 7860686.9229605775 val loss 34124388.004367135\n",
      "validation accuracy 0.8260869565217391 train loss 7057678.096405041 val loss 29404416.98572745\n",
      "validation accuracy 0.8260869565217391 train loss 7760775.830471128 val loss 33492919.42500872\n",
      "validation accuracy 0.8260869565217391 train loss 7338536.691539305 val loss 30952197.616612405\n",
      "validation accuracy 0.8260869565217391 train loss 7969436.460087543 val loss 34814221.072879955\n",
      "validation accuracy 0.8260869565217391 train loss 7166427.633532 val loss 29975052.74925495\n",
      "validation accuracy 0.8260869565217391 train loss 7869525.367598076 val loss 34182752.49352145\n",
      "validation accuracy 0.8260869565217391 train loss 7447286.228666255 val loss 31580203.936280977\n",
      "validation accuracy 0.8260869565217391 train loss 8567529.562085817 val loss 38583082.45876242\n",
      "validation accuracy 0.8913043478260869 train loss 1238387.7041835058 val loss 11111023.630917095\n",
      "validation accuracy 0.8260869565217391 train loss 8032458.20610229 val loss 35228089.440086305\n",
      "validation accuracy 0.8260869565217391 train loss 7229449.379546754 val loss 30317228.66724091\n",
      "validation accuracy 0.8260869565217391 train loss 7932547.113612826 val loss 34596620.86072782\n",
      "validation accuracy 0.8260869565217391 train loss 7510307.974681 val loss 31963735.651439548\n",
      "validation accuracy 0.8260869565217391 train loss 8630551.308100568 val loss 39024418.698444605\n",
      "validation accuracy 0.8913043478260869 train loss 1248727.9636865635 val loss 10961647.890789084\n",
      "validation accuracy 0.8260869565217391 train loss 8584823.516988367 val loss 38735188.99533417\n",
      "validation accuracy 0.8913043478260869 train loss 1232882.3028360633 val loss 10999481.794498505\n",
      "validation accuracy 0.8260869565217391 train loss 8049752.1610048385 val loss 35365993.10598634\n",
      "validation accuracy 0.8260869565217391 train loss 7246743.334449297 val loss 30430451.2981277\n",
      "validation accuracy 0.8260869565217391 train loss 7704304.9553667735 val loss 33190992.955530792\n",
      "validation accuracy 0.8260869565217391 train loss 7934701.5458959825 val loss 34641791.32620171\n",
      "validation accuracy 0.8260869565217391 train loss 7512462.4069641605 val loss 31998616.15083639\n",
      "validation accuracy 0.8260869565217391 train loss 8632705.74038374 val loss 39088191.412226826\n",
      "validation accuracy 0.8913043478260869 train loss 1234104.050700609 val loss 10885585.140762372\n",
      "validation accuracy 0.8260869565217391 train loss 7924296.236769455 val loss 34594422.02783533\n",
      "validation accuracy 0.8260869565217391 train loss 7502057.097837642 val loss 31944109.618072838\n",
      "validation accuracy 0.8913043478260869 train loss 1133284.4482951607 val loss 11278895.344263457\n",
      "validation accuracy 0.8260869565217391 train loss 7652202.791240352 val loss 32899567.008421224\n",
      "validation accuracy 0.8260869565217391 train loss 7882599.381769561 val loss 34350365.37909214\n",
      "validation accuracy 0.8260869565217391 train loss 8112995.972298772 val loss 35801163.749763064\n",
      "validation accuracy 0.8260869565217391 train loss 7309987.14574323 val loss 30772153.04681799\n",
      "validation accuracy 0.8260869565217391 train loss 7767548.766660742 val loss 33626163.599307716\n",
      "validation accuracy 0.8260869565217391 train loss 7997945.357189954 val loss 35076961.96997865\n",
      "validation accuracy 0.8913043478260869 train loss 1150293.4806284658 val loss 11140933.036182676\n",
      "validation accuracy 0.8260869565217391 train loss 7730070.919208568 val loss 33378492.279302087\n",
      "validation accuracy 0.8260869565217391 train loss 7960467.509737783 val loss 34829290.649973035\n",
      "validation accuracy 0.8260869565217391 train loss 7538228.370805973 val loss 32170366.889435288\n",
      "validation accuracy 0.8260869565217391 train loss 8251264.219380177 val loss 36648125.9163456\n",
      "validation accuracy 0.8478260869565217 train loss 6779275.923401789 val loss 27690189.385367792\n",
      "validation accuracy 0.8260869565217391 train loss 6901783.5879137525 val loss 28419360.705733262\n",
      "validation accuracy 0.8260869565217391 train loss 7036625.329185857 val loss 29156042.731332295\n",
      "validation accuracy 0.8260869565217391 train loss 7171467.070457975 val loss 29912045.903641496\n",
      "validation accuracy 0.8260869565217391 train loss 7306308.811730094 val loss 30683254.583293207\n",
      "validation accuracy 0.8260869565217391 train loss 7763870.432647579 val loss 33521494.656898327\n",
      "validation accuracy 0.8260869565217391 train loss 7994267.023176799 val loss 34972293.02756926\n",
      "validation accuracy 0.8913043478260869 train loss 1150520.2724333345 val loss 11157586.37897134\n",
      "validation accuracy 0.8260869565217391 train loss 7726392.585195399 val loss 33273823.336892642\n",
      "validation accuracy 0.8260869565217391 train loss 7956789.175724614 val loss 34724621.70756357\n",
      "validation accuracy 0.8260869565217391 train loss 7534550.03679281 val loss 32065697.947025843\n",
      "validation accuracy 0.8913043478260869 train loss 1197532.7632289985 val loss 10888343.961865034\n",
      "validation accuracy 0.8913043478260869 train loss 1189838.433809158 val loss 10919395.00084357\n",
      "validation accuracy 0.8260869565217391 train loss 7344655.093047083 val loss 30888928.187596597\n",
      "validation accuracy 0.8260869565217391 train loss 7262933.254578705 val loss 30407477.145964038\n",
      "validation accuracy 0.8913043478260869 train loss 1170352.4219785458 val loss 11138016.649144547\n",
      "validation accuracy 0.8260869565217391 train loss 7742097.140314773 val loss 33323808.571790468\n",
      "validation accuracy 0.8260869565217391 train loss 7972493.73084398 val loss 34774606.9424614\n",
      "validation accuracy 0.8913043478260869 train loss 1151179.2973250034 val loss 11217872.460307432\n",
      "validation accuracy 0.8913043478260869 train loss 1164550.4662155043 val loss 11197195.232130267\n",
      "validation accuracy 0.8260869565217391 train loss 7708559.1725491285 val loss 33080446.83638682\n",
      "validation accuracy 0.8260869565217391 train loss 7938955.76307834 val loss 34531245.20705774\n",
      "validation accuracy 0.8260869565217391 train loss 7516716.624146535 val loss 31872321.446519993\n",
      "validation accuracy 0.8913043478260869 train loss 1115626.0990793966 val loss 11347089.086716555\n",
      "validation accuracy 0.8260869565217391 train loss 7461901.915899521 val loss 31546815.286337517\n",
      "validation accuracy 0.8913043478260869 train loss 1117856.4895818783 val loss 11314633.636676323\n",
      "validation accuracy 0.8260869565217391 train loss 7479285.173170343 val loss 31668967.215021405\n",
      "validation accuracy 0.8260869565217391 train loss 8420343.325217571 val loss 37553408.804442994\n",
      "validation accuracy 0.8913043478260869 train loss 1158063.194426178 val loss 11162810.032538015\n",
      "validation accuracy 0.8913043478260869 train loss 1171434.3633166738 val loss 11142132.804360839\n",
      "validation accuracy 0.8913043478260869 train loss 1173500.3698901208 val loss 11110036.50276309\n",
      "validation accuracy 0.8478260869565217 train loss 6738716.15195208 val loss 27279015.444405146\n",
      "validation accuracy 0.8478260869565217 train loss 6850503.087079055 val loss 27961838.987986833\n",
      "validation accuracy 0.8260869565217391 train loss 6967036.8912254665 val loss 28688582.47460538\n",
      "validation accuracy 0.8260869565217391 train loss 7101878.632497579 val loss 29420264.62027061\n",
      "validation accuracy 0.8260869565217391 train loss 7426677.8172832355 val loss 31301903.03236101\n",
      "validation accuracy 0.8260869565217391 train loss 7344955.978814858 val loss 30812736.27795717\n",
      "validation accuracy 0.8260869565217391 train loss 7263234.140346486 val loss 30331285.2363246\n",
      "validation accuracy 0.8913043478260869 train loss 1183145.2891747858 val loss 11125949.216549989\n",
      "validation accuracy 0.8260869565217391 train loss 7195680.436651693 val loss 29956356.81498237\n",
      "validation accuracy 0.8913043478260869 train loss 1180087.8677047263 val loss 11228912.244292608\n",
      "validation accuracy 0.8913043478260869 train loss 1182153.8742781794 val loss 11196815.942694869\n",
      "validation accuracy 0.8913043478260869 train loss 1184219.880851637 val loss 11164719.641097104\n",
      "validation accuracy 0.8913043478260869 train loss 1186285.8874250916 val loss 11132623.339499336\n",
      "validation accuracy 0.8260869565217391 train loss 7178065.267322629 val loss 29870958.922374386\n",
      "validation accuracy 0.8913043478260869 train loss 1183228.4659550255 val loss 11235586.367241973\n",
      "validation accuracy 0.8913043478260869 train loss 1185294.4725284749 val loss 11203490.065644229\n",
      "validation accuracy 0.8913043478260869 train loss 1187360.4791019303 val loss 11171393.764046488\n",
      "validation accuracy 0.8913043478260869 train loss 1189426.4856753834 val loss 11139297.462448724\n",
      "validation accuracy 0.8260869565217391 train loss 7160450.0979935415 val loss 29787834.62219654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8913043478260869 train loss 1186369.0642053154 val loss 11242260.490191372\n",
      "validation accuracy 0.8913043478260869 train loss 1188435.0707787648 val loss 11210164.188593619\n",
      "validation accuracy 0.8913043478260869 train loss 1201806.2396692652 val loss 11189486.960416466\n",
      "validation accuracy 0.8913043478260869 train loss 1203872.2462427085 val loss 11157390.658818716\n",
      "validation accuracy 0.8260869565217391 train loss 7150269.058709443 val loss 29729653.30359658\n",
      "validation accuracy 0.8913043478260869 train loss 1200814.824772649 val loss 11260474.556061335\n",
      "validation accuracy 0.8913043478260869 train loss 1202880.8313461044 val loss 11228257.384963617\n",
      "validation accuracy 0.8913043478260869 train loss 1204946.8379195477 val loss 11196161.083365861\n",
      "validation accuracy 0.8913043478260869 train loss 1207012.8444930008 val loss 11164064.781768115\n",
      "validation accuracy 0.8260869565217391 train loss 7957509.866008808 val loss 34588755.08512299\n",
      "validation accuracy 0.8260869565217391 train loss 7154501.039453267 val loss 29748912.360563766\n",
      "validation accuracy 0.8913043478260869 train loss 1200901.4623345956 val loss 11240061.71644701\n",
      "validation accuracy 0.8913043478260869 train loss 1202967.4689080468 val loss 11207965.414849255\n",
      "validation accuracy 0.8913043478260869 train loss 1205033.4754814967 val loss 11175869.113251515\n",
      "validation accuracy 0.8260869565217391 train loss 7947573.711979058 val loss 34509697.29724669\n",
      "validation accuracy 0.8260869565217391 train loss 7144564.885423515 val loss 29681365.45040696\n",
      "validation accuracy 0.8913043478260869 train loss 1198922.0933230906 val loss 11251866.04793039\n",
      "validation accuracy 0.8913043478260869 train loss 1212293.2622135836 val loss 11231188.819753248\n",
      "validation accuracy 0.8913043478260869 train loss 1214359.2687870376 val loss 11199092.518155493\n",
      "validation accuracy 0.8260869565217391 train loss 7945071.687994296 val loss 34436815.83650257\n",
      "validation accuracy 0.8260869565217391 train loss 7142062.861438754 val loss 29633853.077125322\n",
      "validation accuracy 0.8913043478260869 train loss 1208247.886628631 val loss 11275521.272594135\n",
      "validation accuracy 0.8913043478260869 train loss 1210313.8932020832 val loss 11242993.151236633\n",
      "validation accuracy 0.8260869565217391 train loss 8568936.377469473 val loss 38345239.251858376\n",
      "validation accuracy 0.8913043478260869 train loss 1282685.6292528203 val loss 10884741.924695004\n",
      "validation accuracy 0.8260869565217391 train loss 7918077.087624748 val loss 34267865.91762623\n",
      "validation accuracy 0.8260869565217391 train loss 7115068.261069212 val loss 29470442.58932379\n",
      "validation accuracy 0.8913043478260869 train loss 1191231.984432994 val loss 11298974.27097997\n",
      "validation accuracy 0.8913043478260869 train loss 1204603.1533234874 val loss 11277141.02790925\n",
      "validation accuracy 0.8913043478260869 train loss 1217974.3222139794 val loss 11255307.784838516\n",
      "validation accuracy 0.8913043478260869 train loss 1220040.3287874325 val loss 11222836.931492612\n",
      "validation accuracy 0.8260869565217391 train loss 7937177.3284585485 val loss 34305582.30800284\n",
      "validation accuracy 0.8260869565217391 train loss 7134168.501903004 val loss 29529770.72004134\n",
      "validation accuracy 0.8913043478260869 train loss 1213928.9466290316 val loss 11302310.537390316\n",
      "validation accuracy 0.8260869565217391 train loss 8546873.883160153 val loss 38109584.199370116\n",
      "validation accuracy 0.8913043478260869 train loss 1286300.6826797645 val loss 10937979.430549681\n",
      "validation accuracy 0.8260869565217391 train loss 7838464.379545856 val loss 33672657.1943967\n",
      "validation accuracy 0.8478260869565217 train loss 7044684.642784125 val loss 28967497.724884454\n",
      "validation accuracy 0.8260869565217391 train loss 7170297.294262414 val loss 29699179.870549537\n",
      "validation accuracy 0.8260869565217391 train loss 7913431.432422491 val loss 34115708.24933164\n",
      "validation accuracy 0.8260869565217391 train loss 7110422.60586695 val loss 29351180.52214553\n",
      "validation accuracy 0.8260869565217391 train loss 7245264.347139042 val loss 30087152.36490965\n",
      "validation accuracy 0.8260869565217391 train loss 8186322.499186271 val loss 35789445.49695783\n",
      "validation accuracy 0.8260869565217391 train loss 8397286.356319524 val loss 37116501.15869802\n",
      "validation accuracy 0.8913043478260869 train loss 1235391.9402777688 val loss 11161619.386092324\n",
      "validation accuracy 0.8260869565217391 train loss 7688876.852705231 val loss 32695726.338060036\n",
      "validation accuracy 0.8260869565217391 train loss 8322713.390339304 val loss 36638798.86324263\n",
      "validation accuracy 0.8913043478260869 train loss 1209801.6553566614 val loss 11275902.69604415\n",
      "validation accuracy 0.8260869565217391 train loss 7614303.886725011 val loss 32237828.007516045\n",
      "validation accuracy 0.8260869565217391 train loss 8255141.769781357 val loss 36228805.55901373\n",
      "validation accuracy 0.8260869565217391 train loss 8466105.626914605 val loss 37555861.22075394\n",
      "validation accuracy 0.8913043478260869 train loss 1246477.2253472907 val loss 11018255.074739989\n",
      "validation accuracy 0.8260869565217391 train loss 7757696.123300316 val loss 33126205.993806355\n",
      "validation accuracy 0.8260869565217391 train loss 8398534.006356679 val loss 37145867.9165251\n",
      "validation accuracy 0.8913043478260869 train loss 1212847.0658606389 val loss 11108005.608876886\n",
      "validation accuracy 0.8260869565217391 train loss 7367404.623097008 val loss 30766176.678462245\n",
      "validation accuracy 0.8260869565217391 train loss 7824966.244014484 val loss 33550490.653460782\n",
      "validation accuracy 0.8260869565217391 train loss 8458802.781648556 val loss 37509715.362978786\n",
      "validation accuracy 0.8913043478260869 train loss 1230958.6634711544 val loss 11005466.06353151\n",
      "validation accuracy 0.8260869565217391 train loss 7750393.278034282 val loss 33075492.414849054\n",
      "validation accuracy 0.8260869565217391 train loss 8391231.161090657 val loss 37099722.05875017\n",
      "validation accuracy 0.8913043478260869 train loss 1197328.5039845128 val loss 11095216.597668357\n",
      "validation accuracy 0.8260869565217391 train loss 7360101.777830985 val loss 30718493.604774978\n",
      "validation accuracy 0.8913043478260869 train loss 1233336.0570962946 val loss 10919004.577057835\n",
      "validation accuracy 0.8260869565217391 train loss 8283172.359998451 val loss 36391335.716544084\n",
      "validation accuracy 0.8260869565217391 train loss 8494136.217131697 val loss 37718391.378284276\n",
      "validation accuracy 0.8913043478260869 train loss 1237986.9135637796 val loss 10927390.11556878\n",
      "validation accuracy 0.8260869565217391 train loss 7785726.713517427 val loss 33281464.37331099\n",
      "validation accuracy 0.8260869565217391 train loss 8426564.596573796 val loss 37308398.07405564\n",
      "validation accuracy 0.8913043478260869 train loss 1204356.754077136 val loss 11017140.649705634\n",
      "validation accuracy 0.8260869565217391 train loss 7395435.213314137 val loss 30906735.80407393\n",
      "validation accuracy 0.8260869565217391 train loss 7780798.868713784 val loss 33262195.484825775\n",
      "validation accuracy 0.8260869565217391 train loss 8421636.75177015 val loss 37289129.18557036\n",
      "validation accuracy 0.8913043478260869 train loss 1189600.8226466076 val loss 11005044.192258446\n",
      "validation accuracy 0.8260869565217391 train loss 7390507.3685105145 val loss 30886280.694999866\n",
      "validation accuracy 0.8913043478260869 train loss 1310950.638419813 val loss 10553659.480755873\n",
      "validation accuracy 0.8913043478260869 train loss 1278428.6545244665 val loss 10660988.557686668\n",
      "validation accuracy 0.8913043478260869 train loss 1245906.6706291148 val loss 10783977.686357412\n",
      "validation accuracy 0.8260869565217391 train loss 8361229.183355822 val loss 36855646.513144374\n",
      "validation accuracy 0.8695652173913043 train loss 1113541.445825627 val loss 11537351.18449314\n",
      "validation accuracy 0.8260869565217391 train loss 7096230.4628815465 val loss 29138144.74923023\n",
      "validation accuracy 0.8260869565217391 train loss 7231072.20415365 val loss 29907685.802279226\n",
      "validation accuracy 0.8260869565217391 train loss 7365913.945425761 val loss 30684410.035183683\n",
      "validation accuracy 0.8260869565217391 train loss 8306972.097472983 val loss 36472540.19476788\n",
      "validation accuracy 0.8913043478260869 train loss 1171564.844291524 val loss 11214066.439002877\n",
      "validation accuracy 0.8260869565217391 train loss 7275842.714213319 val loss 30142783.387941718\n",
      "validation accuracy 0.8260869565217391 train loss 7410684.455485404 val loss 30919507.620846108\n",
      "validation accuracy 0.8260869565217391 train loss 7796048.110885062 val loss 33267887.347446978\n",
      "validation accuracy 0.8260869565217391 train loss 8436885.993941415 val loss 37294821.04819156\n",
      "validation accuracy 0.8913043478260869 train loss 1200510.7953926153 val loss 10986167.426008772\n",
      "validation accuracy 0.8913043478260869 train loss 1278158.7286341903 val loss 10696375.40534354\n",
      "validation accuracy 0.8913043478260869 train loss 1245636.7447388414 val loss 10825246.386614094\n",
      "validation accuracy 0.8260869565217391 train loss 8331822.219652567 val loss 36590810.695336476\n",
      "validation accuracy 0.8260869565217391 train loss 8542786.076785818 val loss 37917866.35707666\n",
      "validation accuracy 0.8913043478260869 train loss 1250287.6012063222 val loss 10829798.083318606\n",
      "validation accuracy 0.8260869565217391 train loss 7762178.60765371 val loss 33036707.358615976\n",
      "validation accuracy 0.8260869565217391 train loss 8403016.490710072 val loss 37057047.72668235\n",
      "validation accuracy 0.8913043478260869 train loss 1191829.7872441695 val loss 11026459.524381042\n",
      "validation accuracy 0.8260869565217391 train loss 7371887.107450414 val loss 30678566.138068534\n",
      "validation accuracy 0.8260869565217391 train loss 8312945.259497642 val loss 36468858.11979079\n",
      "validation accuracy 0.8913043478260869 train loss 1169876.7967659882 val loss 11176573.433046345\n",
      "validation accuracy 0.8260869565217391 train loss 7281815.876237979 val loss 30136939.490826637\n",
      "validation accuracy 0.8260869565217391 train loss 7416657.617510081 val loss 30913663.7237311\n",
      "validation accuracy 0.8260869565217391 train loss 7802021.272909749 val loss 33264205.27247011\n",
      "validation accuracy 0.8260869565217391 train loss 8442859.155966109 val loss 37291138.9732147\n",
      "validation accuracy 0.8913043478260869 train loss 1198822.7478670967 val loss 10945932.809270421\n",
      "validation accuracy 0.8260869565217391 train loss 7411729.772706477 val loss 30893208.614657108\n",
      "validation accuracy 0.8260869565217391 train loss 8352787.9247536985 val loss 36702949.3663233\n",
      "validation accuracy 0.8913043478260869 train loss 1176869.7573889229 val loss 11094325.406904183\n",
      "validation accuracy 0.8260869565217391 train loss 7321658.541494043 val loss 30351581.967415206\n",
      "validation accuracy 0.8913043478260869 train loss 1171783.1134885806 val loss 11163615.656329636\n",
      "validation accuracy 0.8260869565217391 train loss 7288145.104071182 val loss 30128843.010402046\n",
      "validation accuracy 0.8260869565217391 train loss 7422986.845343284 val loss 30905567.24330651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8260869565217391 train loss 7808350.500742933 val loss 33257164.41345018\n",
      "validation accuracy 0.8260869565217391 train loss 8449188.383799298 val loss 37281264.609948784\n",
      "validation accuracy 0.8913043478260869 train loss 1200729.0645896755 val loss 10933836.713778535\n",
      "validation accuracy 0.8260869565217391 train loss 7418059.000539665 val loss 30885112.13423244\n",
      "validation accuracy 0.8260869565217391 train loss 8359117.152586891 val loss 36693075.0030574\n",
      "validation accuracy 0.8913043478260869 train loss 1178776.074111505 val loss 11082229.311412316\n",
      "validation accuracy 0.8260869565217391 train loss 7327987.769327231 val loss 30343485.486990537\n",
      "validation accuracy 0.8913043478260869 train loss 1173689.4302111603 val loss 11150659.931031086\n",
      "validation accuracy 0.8260869565217391 train loss 7294474.3319043685 val loss 30120746.529977374\n",
      "validation accuracy 0.8260869565217391 train loss 7752035.952821864 val loss 32869430.3724788\n",
      "validation accuracy 0.8260869565217391 train loss 8392873.835878227 val loss 36880665.8309395\n",
      "validation accuracy 0.8913043478260869 train loss 1183761.1532561467 val loss 11023587.609870702\n",
      "validation accuracy 0.8260869565217391 train loss 7361744.452618592 val loss 30516435.281737834\n",
      "validation accuracy 0.8913043478260869 train loss 1305110.9690293535 val loss 10552785.686874207\n",
      "validation accuracy 0.8913043478260869 train loss 1272588.9851340046 val loss 10663572.161361298\n",
      "validation accuracy 0.8913043478260869 train loss 1240067.0012386558 val loss 10792692.73879362\n",
      "validation accuracy 0.8260869565217391 train loss 8332466.2674639 val loss 36447183.1585135\n",
      "validation accuracy 0.8695652173913043 train loss 1105949.5991467065 val loss 11566280.329557363\n",
      "validation accuracy 0.8260869565217391 train loss 7067467.546989633 val loss 28779659.210932724\n",
      "validation accuracy 0.8260869565217391 train loss 7202309.288261727 val loss 29537840.389017224\n",
      "validation accuracy 0.8260869565217391 train loss 7337151.029533829 val loss 30314564.62192169\n",
      "validation accuracy 0.8260869565217391 train loss 8278209.181581061 val loss 36064076.84013702\n",
      "validation accuracy 0.8260869565217391 train loss 8489173.03871431 val loss 37391132.5018772\n",
      "validation accuracy 0.8913043478260869 train loss 1236030.904378348 val loss 10885204.222787106\n",
      "validation accuracy 0.8260869565217391 train loss 7708565.5695822025 val loss 32537913.36845923\n",
      "validation accuracy 0.8260869565217391 train loss 8349403.452638575 val loss 36530313.87148291\n",
      "validation accuracy 0.8913043478260869 train loss 1177573.0904161935 val loss 11089930.620637134\n",
      "validation accuracy 0.8260869565217391 train loss 7318274.06937892 val loss 30195124.28282555\n",
      "validation accuracy 0.8260869565217391 train loss 7703637.724778568 val loss 32509809.275326185\n",
      "validation accuracy 0.8913043478260869 train loss 1306119.9652370405 val loss 10477481.618689856\n",
      "validation accuracy 0.8913043478260869 train loss 1273597.9813416917 val loss 10588393.028338907\n",
      "validation accuracy 0.8913043478260869 train loss 1241075.9974463414 val loss 10711054.14522532\n",
      "validation accuracy 0.8260869565217391 train loss 8374139.270633014 val loss 36665751.917463094\n",
      "validation accuracy 0.8695652173913043 train loss 1102343.3762593316 val loss 11472571.985351402\n",
      "validation accuracy 0.8260869565217391 train loss 7109140.550158767 val loss 28960976.695368335\n",
      "validation accuracy 0.8260869565217391 train loss 7243982.291430885 val loss 29737700.928272873\n",
      "validation accuracy 0.8260869565217391 train loss 7378824.032703008 val loss 30514425.161177408\n",
      "validation accuracy 0.8260869565217391 train loss 8319882.184750225 val loss 36282645.59908692\n",
      "validation accuracy 0.8478260869565217 train loss 1099142.3929200252 val loss 11577133.988298152\n",
      "validation accuracy 0.8260869565217391 train loss 7054883.464275952 val loss 28627387.02480443\n",
      "validation accuracy 0.8260869565217391 train loss 7189725.205548062 val loss 29384757.25343346\n",
      "validation accuracy 0.8260869565217391 train loss 7324566.946820171 val loss 30161481.486337904\n",
      "validation accuracy 0.8260869565217391 train loss 7709930.602219818 val loss 32469623.800915234\n",
      "validation accuracy 0.8260869565217391 train loss 8350768.485276178 val loss 36468459.999116704\n",
      "validation accuracy 0.8478260869565217 train loss 1095361.3112845486 val loss 11514787.31168538\n",
      "validation accuracy 0.8260869565217391 train loss 7085769.764801929 val loss 28780151.980773382\n",
      "validation accuracy 0.8260869565217391 train loss 7220611.506074049 val loss 29552985.11676184\n",
      "validation accuracy 0.8260869565217391 train loss 7355453.247346162 val loss 30329709.34966639\n",
      "validation accuracy 0.8913043478260869 train loss 1299860.1297522327 val loss 10541493.242530748\n",
      "validation accuracy 0.8913043478260869 train loss 1267338.1458568806 val loss 10656717.624884976\n",
      "validation accuracy 0.8913043478260869 train loss 1234816.1619615336 val loss 10788824.014190698\n",
      "validation accuracy 0.8260869565217391 train loss 8326175.062191472 val loss 36240060.615205936\n",
      "validation accuracy 0.8478260869565217 train loss 1098871.5516066789 val loss 11567891.178809207\n",
      "validation accuracy 0.8260869565217391 train loss 7061176.341717195 val loss 28595965.456285782\n",
      "validation accuracy 0.8260869565217391 train loss 7196018.082989301 val loss 29351968.628594913\n",
      "validation accuracy 0.8260869565217391 train loss 7330859.824261421 val loss 30127838.689850308\n",
      "validation accuracy 0.8260869565217391 train loss 7716223.479661072 val loss 32429438.326504305\n",
      "validation accuracy 0.8260869565217391 train loss 8357061.362717431 val loss 36425875.01523581\n",
      "validation accuracy 0.8478260869565217 train loss 1095090.4699712005 val loss 11505427.6428869\n",
      "validation accuracy 0.8260869565217391 train loss 7092062.64224318 val loss 28748730.41225483\n",
      "validation accuracy 0.8260869565217391 train loss 7226904.383515298 val loss 29519342.32027424\n",
      "validation accuracy 0.8260869565217391 train loss 7361746.124787417 val loss 30296066.55317877\n",
      "validation accuracy 0.8260869565217391 train loss 8302804.276834646 val loss 36042768.69685964\n",
      "validation accuracy 0.8478260869565217 train loss 1093587.73774597 val loss 11613542.87065756\n",
      "validation accuracy 0.8260869565217391 train loss 7037805.556360384 val loss 28417827.787865445\n",
      "validation accuracy 0.8260869565217391 train loss 7172647.297632488 val loss 29171847.843282077\n",
      "validation accuracy 0.8260869565217391 train loss 7307489.038904607 val loss 29943122.87833941\n",
      "validation accuracy 0.8260869565217391 train loss 7692852.694304255 val loss 32241209.884138238\n",
      "validation accuracy 0.8260869565217391 train loss 8333690.577360617 val loss 36228583.09688958\n",
      "validation accuracy 0.8478260869565217 train loss 1089806.6561104944 val loss 11547642.96922082\n",
      "validation accuracy 0.8260869565217391 train loss 7068691.856886371 val loss 28568609.626941994\n",
      "validation accuracy 0.8260869565217391 train loss 7203533.598158487 val loss 29334626.508763354\n",
      "validation accuracy 0.8260869565217391 train loss 7338375.339430608 val loss 30111350.741667897\n",
      "validation accuracy 0.8913043478260869 train loss 1165673.9403137327 val loss 11119202.5602698\n",
      "validation accuracy 0.8260869565217391 train loss 7304861.902007731 val loss 29888611.784654655\n",
      "validation accuracy 0.8260869565217391 train loss 7690225.557407385 val loss 32183705.30259639\n",
      "validation accuracy 0.8260869565217391 train loss 8331063.440463744 val loss 36170431.88712126\n",
      "validation accuracy 0.8478260869565217 train loss 1085463.2404107866 val loss 11559151.463647753\n",
      "validation accuracy 0.8260869565217391 train loss 7066064.7199894935 val loss 28512283.104312684\n",
      "validation accuracy 0.8260869565217391 train loss 7200906.461261614 val loss 29280115.415078588\n",
      "validation accuracy 0.8260869565217391 train loss 7335748.202533727 val loss 30056839.647983126\n",
      "validation accuracy 0.8913043478260869 train loss 1159097.4021332562 val loss 11124424.69308016\n",
      "validation accuracy 0.8260869565217391 train loss 7302234.765110859 val loss 29834100.690969914\n",
      "validation accuracy 0.8260869565217391 train loss 7687598.420510512 val loss 32126200.721054547\n",
      "validation accuracy 0.8260869565217391 train loss 8328436.303566883 val loss 36112280.67735301\n",
      "validation accuracy 0.8478260869565217 train loss 1081119.8247110853 val loss 11572887.806216259\n",
      "validation accuracy 0.8260869565217391 train loss 7063437.583092617 val loss 28455956.581683375\n",
      "validation accuracy 0.8260869565217391 train loss 7198279.324364731 val loss 29225604.32139383\n",
      "validation accuracy 0.8260869565217391 train loss 7333121.065636855 val loss 30002328.55429837\n",
      "validation accuracy 0.8913043478260869 train loss 1152520.863952781 val loss 11129646.825890535\n",
      "validation accuracy 0.8260869565217391 train loss 7299607.628213983 val loss 29779589.597285166\n",
      "validation accuracy 0.8260869565217391 train loss 7434449.3694861 val loss 30556313.830189683\n",
      "validation accuracy 0.8260869565217391 train loss 7819813.024885752 val loss 32892187.03273309\n",
      "validation accuracy 0.8260869565217391 train loss 7023805.543752507 val loss 28205035.465598278\n",
      "validation accuracy 0.8260869565217391 train loss 7158647.285024607 val loss 28971421.11403604\n",
      "validation accuracy 0.8260869565217391 train loss 7293489.026296721 val loss 29748145.34694054\n",
      "validation accuracy 0.8260869565217391 train loss 7211767.187828364 val loss 29266694.305308122\n",
      "validation accuracy 0.8260869565217391 train loss 7346608.9291004725 val loss 30043418.538212568\n",
      "validation accuracy 0.8260869565217391 train loss 8287667.081147723 val loss 35768897.43490106\n",
      "validation accuracy 0.8478260869565217 train loss 1076775.1186250187 val loss 11671035.107603442\n",
      "validation accuracy 0.8260869565217391 train loss 7022668.360673471 val loss 28154401.58827818\n",
      "validation accuracy 0.8260869565217391 train loss 7157510.10194559 val loss 28913750.630468965\n",
      "validation accuracy 0.8260869565217391 train loss 7292351.84321771 val loss 29690474.8633735\n",
      "validation accuracy 0.8260869565217391 train loss 7427193.584489823 val loss 30467199.09627805\n",
      "validation accuracy 0.8913043478260869 train loss 1200746.289634014 val loss 10852014.620437209\n",
      "validation accuracy 0.8260869565217391 train loss 8278066.201139467 val loss 35656318.79695403\n",
      "validation accuracy 0.8478260869565217 train loss 1078145.6728471983 val loss 11695250.383839639\n",
      "validation accuracy 0.8260869565217391 train loss 7013067.480665207 val loss 28068347.616693445\n",
      "validation accuracy 0.8260869565217391 train loss 7147909.221937322 val loss 28819095.503819596\n",
      "validation accuracy 0.8260869565217391 train loss 7282750.963209439 val loss 29589650.238164917\n",
      "validation accuracy 0.8260869565217391 train loss 7417592.704481558 val loss 30366374.471069455\n",
      "validation accuracy 0.8260869565217391 train loss 7802956.359881203 val loss 32689450.095909648\n",
      "validation accuracy 0.8260869565217391 train loss 7006948.87874796 val loss 28028119.247893773\n",
      "validation accuracy 0.8260869565217391 train loss 7141790.620020063 val loss 28781481.75491581\n",
      "validation accuracy 0.8260869565217391 train loss 7276632.361292169 val loss 29558205.987820316\n",
      "validation accuracy 0.8260869565217391 train loss 7194910.522823818 val loss 29076754.94618787\n",
      "validation accuracy 0.8260869565217391 train loss 7329752.264095922 val loss 29853479.179092333\n",
      "validation accuracy 0.8260869565217391 train loss 8270810.416143174 val loss 35556901.16430046\n",
      "validation accuracy 0.8478260869565217 train loss 1074363.3008253514 val loss 11716947.87203366\n",
      "validation accuracy 0.8260869565217391 train loss 7005811.695668922 val loss 27974459.704204585\n",
      "validation accuracy 0.8260869565217391 train loss 7140653.436941036 val loss 28726631.989012748\n",
      "validation accuracy 0.8260869565217391 train loss 7275495.178213159 val loss 29500535.50425326\n",
      "validation accuracy 0.8260869565217391 train loss 7410336.919485272 val loss 30277259.737157796\n",
      "validation accuracy 0.8260869565217391 train loss 7795700.574884924 val loss 32590825.52991037\n",
      "validation accuracy 0.8260869565217391 train loss 6999693.093751677 val loss 27955947.619041275\n",
      "validation accuracy 0.8260869565217391 train loss 7134534.8350237785 val loss 28698932.797194377\n",
      "validation accuracy 0.8260869565217391 train loss 7269376.576295894 val loss 29469091.253908653\n",
      "validation accuracy 0.8260869565217391 train loss 7187654.7378275385 val loss 28987640.21227622\n",
      "validation accuracy 0.8260869565217391 train loss 7322496.479099641 val loss 29764364.445180677\n",
      "validation accuracy 0.8260869565217391 train loss 7240774.640631287 val loss 29282913.403548256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8260869565217391 train loss 7375616.381903388 val loss 30059637.636452697\n",
      "validation accuracy 0.8913043478260869 train loss 1249758.608449337 val loss 10628324.323667636\n",
      "validation accuracy 0.8260869565217391 train loss 6990999.283988965 val loss 27879904.671239648\n",
      "validation accuracy 0.8260869565217391 train loss 7125841.025261069 val loss 28608498.520525865\n",
      "validation accuracy 0.8260869565217391 train loss 7260682.76653318 val loss 29385222.753430374\n",
      "validation accuracy 0.8260869565217391 train loss 7178960.928064826 val loss 28903771.711797953\n",
      "validation accuracy 0.8260869565217391 train loss 7313802.669336935 val loss 29680495.944702428\n",
      "validation accuracy 0.8260869565217391 train loss 8254860.821384189 val loss 35357705.69071014\n",
      "validation accuracy 0.8478260869565217 train loss 1078390.6407993217 val loss 11745120.289544482\n",
      "validation accuracy 0.8913043478260869 train loss 1303396.6849102513 val loss 10477683.766670382\n",
      "validation accuracy 0.8913043478260869 train loss 1270874.7010149043 val loss 10597704.14661699\n",
      "validation accuracy 0.8913043478260869 train loss 1238352.7171195573 val loss 10728734.98509277\n",
      "validation accuracy 0.8913043478260869 train loss 1273404.4547371631 val loss 10568134.422061332\n",
      "validation accuracy 0.8913043478260869 train loss 1240882.4708418166 val loss 10695816.862168694\n",
      "validation accuracy 0.8478260869565217 train loss 6955629.20703658 val loss 27619752.45099599\n",
      "validation accuracy 0.8260869565217391 train loss 7076487.464445186 val loss 28301511.1928514\n",
      "validation accuracy 0.8260869565217391 train loss 7211329.205717303 val loss 29063586.996830773\n",
      "validation accuracy 0.8260869565217391 train loss 7129607.367248947 val loss 28586164.063050088\n",
      "validation accuracy 0.8260869565217391 train loss 7264449.108521058 val loss 29358860.188102815\n",
      "validation accuracy 0.8913043478260869 train loss 1173725.2024134435 val loss 11082731.765108097\n",
      "validation accuracy 0.8260869565217391 train loss 7319897.225751405 val loss 29669270.225737233\n",
      "validation accuracy 0.8913043478260869 train loss 1252478.8225626787 val loss 10702975.13659601\n",
      "validation accuracy 0.8913043478260869 train loss 1287530.5601802766 val loss 10543326.146957107\n",
      "validation accuracy 0.8478260869565217 train loss 7046459.922354942 val loss 28131651.733433954\n",
      "validation accuracy 0.8260869565217391 train loss 7181291.9833370065 val loss 28873056.24820473\n",
      "validation accuracy 0.8260869565217391 train loss 7099570.144868638 val loss 28407536.757517457\n",
      "validation accuracy 0.8260869565217391 train loss 7234411.886140761 val loss 29168329.439476766\n",
      "validation accuracy 0.8913043478260869 train loss 1176671.4092202722 val loss 11124983.822857935\n",
      "validation accuracy 0.8260869565217391 train loss 7289860.0033710785 val loss 29478739.477111056\n",
      "validation accuracy 0.8913043478260869 train loss 1255425.029369501 val loss 10734659.774788298\n",
      "validation accuracy 0.8913043478260869 train loss 1290476.766987102 val loss 10570078.221206151\n",
      "validation accuracy 0.8478260869565217 train loss 7025503.7025462715 val loss 27972125.259229824\n",
      "validation accuracy 0.8260869565217391 train loss 7151254.760956656 val loss 28688897.624437265\n",
      "validation accuracy 0.8478260869565217 train loss 7070503.040296057 val loss 28248381.861326445\n",
      "validation accuracy 0.8260869565217391 train loss 7204374.663760387 val loss 28984912.624293696\n",
      "validation accuracy 0.8913043478260869 train loss 1179617.6160270837 val loss 11167235.880607886\n",
      "validation accuracy 0.8260869565217391 train loss 7259822.780990711 val loss 29293868.60375444\n",
      "validation accuracy 0.8913043478260869 train loss 1258371.236176311 val loss 10766344.412980653\n",
      "validation accuracy 0.8913043478260869 train loss 1293422.9737939108 val loss 10601048.786544256\n",
      "validation accuracy 0.8913043478260869 train loss 1328474.7114115092 val loss 10456593.093548702\n",
      "validation accuracy 0.8260869565217391 train loss 8234171.006730854 val loss 35168026.21265572\n",
      "validation accuracy 0.8260869565217391 train loss 8445134.863864128 val loss 36495081.87439608\n",
      "validation accuracy 0.8913043478260869 train loss 1134391.0204307921 val loss 11309844.763127547\n",
      "validation accuracy 0.8260869565217391 train loss 7156561.225962285 val loss 28733145.645201672\n",
      "validation accuracy 0.8260869565217391 train loss 7291402.967234404 val loss 29499191.89874538\n",
      "validation accuracy 0.8913043478260869 train loss 1256846.5231116535 val loss 10666625.257409954\n",
      "validation accuracy 0.8913043478260869 train loss 1291898.2607292566 val loss 10503437.127501423\n",
      "validation accuracy 0.8478260869565217 train loss 7025125.577963289 val loss 27975208.559048925\n",
      "validation accuracy 0.8260869565217391 train loss 7152797.724819973 val loss 28702977.921212714\n",
      "validation accuracy 0.8260869565217391 train loss 7287639.466092096 val loss 29479702.154117245\n",
      "validation accuracy 0.8260869565217391 train loss 8228697.618139341 val loss 35127816.86984578\n",
      "validation accuracy 0.8478260869565217 train loss 1085714.8198687835 val loss 11711310.80791067\n",
      "validation accuracy 0.8913043478260869 train loss 1318263.0450190855 val loss 10432553.521451853\n",
      "validation accuracy 0.8260869565217391 train loss 8211494.107091778 val loss 35033917.13442786\n",
      "validation accuracy 0.8478260869565217 train loss 1081981.5261625499 val loss 11736982.852191754\n",
      "validation accuracy 0.8913043478260869 train loss 1310599.54808125 val loss 10432815.214873934\n",
      "validation accuracy 0.8913043478260869 train loss 1278077.564185902 val loss 10550721.52090009\n",
      "validation accuracy 0.8478260869565217 train loss 6981870.724607721 val loss 27704034.156392284\n",
      "validation accuracy 0.8260869565217391 train loss 7102321.706720014 val loss 28404894.469240054\n",
      "validation accuracy 0.8260869565217391 train loss 7237163.447992131 val loss 29163511.354525734\n",
      "validation accuracy 0.8260869565217391 train loss 8178221.600039362 val loss 34798994.95162198\n",
      "validation accuracy 0.8260869565217391 train loss 8389185.457172628 val loss 36107386.96440744\n",
      "validation accuracy 0.8913043478260869 train loss 1118022.1544328565 val loss 11385714.037274895\n",
      "validation accuracy 0.8260869565217391 train loss 7100611.819270793 val loss 28399143.692630276\n",
      "validation accuracy 0.8260869565217391 train loss 7235453.560542919 val loss 29141348.761991322\n",
      "validation accuracy 0.8260869565217391 train loss 7370295.301815041 val loss 29911331.907353107\n",
      "validation accuracy 0.8260869565217391 train loss 7755658.957214695 val loss 32209739.7822038\n",
      "validation accuracy 0.8478260869565217 train loss 6969320.61793368 val loss 27631933.37218015\n",
      "validation accuracy 0.8260869565217391 train loss 7094493.217353556 val loss 28326441.103383355\n",
      "validation accuracy 0.8260869565217391 train loss 7229334.958625676 val loss 29103163.424104072\n",
      "validation accuracy 0.8913043478260869 train loss 1257831.1584077338 val loss 10573389.780638903\n",
      "validation accuracy 0.8478260869565217 train loss 6959518.885336396 val loss 27539514.248176266\n",
      "validation accuracy 0.8260869565217391 train loss 7078424.221348715 val loss 28206319.0581987\n",
      "validation accuracy 0.8260869565217391 train loss 7213265.962620831 val loss 28974106.78425864\n",
      "validation accuracy 0.8913043478260869 train loss 1259337.4557400653 val loss 10614583.496025555\n",
      "validation accuracy 0.8913043478260869 train loss 1294389.193357674 val loss 10453828.340110626\n",
      "validation accuracy 0.8478260869565217 train loss 7042370.148323681 val loss 27991856.742190324\n",
      "validation accuracy 0.8260869565217391 train loss 7173525.339571831 val loss 28723251.049903207\n",
      "validation accuracy 0.8913043478260869 train loss 1252193.6081583775 val loss 10680303.575177101\n",
      "validation accuracy 0.8913043478260869 train loss 1287245.345775979 val loss 10515007.948740683\n",
      "validation accuracy 0.8478260869565217 train loss 7013434.476183983 val loss 27798999.672779962\n",
      "validation accuracy 0.8260869565217391 train loss 7133784.716522801 val loss 28476902.62339053\n",
      "validation accuracy 0.8695652173913043 train loss 1093016.328857137 val loss 11708939.312166452\n",
      "validation accuracy 0.8913043478260869 train loss 1329241.8250670573 val loss 10433460.915750355\n",
      "validation accuracy 0.8260869565217391 train loss 8138705.100899309 val loss 34481277.45548717\n",
      "validation accuracy 0.8260869565217391 train loss 8349668.958032588 val loss 35787099.12375221\n",
      "validation accuracy 0.8913043478260869 train loss 1099934.1660828507 val loss 11446090.545010768\n",
      "validation accuracy 0.8260869565217391 train loss 7061095.320130733 val loss 28105429.246861428\n",
      "validation accuracy 0.8260869565217391 train loss 7195937.061402853 val loss 28851126.72882633\n",
      "validation accuracy 0.8260869565217391 train loss 7330778.802674972 val loss 29622148.87522544\n",
      "validation accuracy 0.8913043478260869 train loss 1266091.5512953466 val loss 10533837.420623442\n",
      "validation accuracy 0.8478260869565217 train loss 6963108.149548585 val loss 27518876.19406703\n",
      "validation accuracy 0.8260869565217391 train loss 7081003.446032662 val loss 28190732.2908983\n",
      "validation accuracy 0.8260869565217391 train loss 7215845.187304779 val loss 28947733.99220322\n",
      "validation accuracy 0.8913043478260869 train loss 1267597.8486276842 val loss 10575031.136010066\n",
      "validation accuracy 0.8913043478260869 train loss 1302649.5862452863 val loss 10417642.077606138\n",
      "validation accuracy 0.8478260869565217 train loss 7045959.4125358695 val loss 27971218.688081086\n",
      "validation accuracy 0.8260869565217391 train loss 7176104.564255754 val loss 28700699.712650307\n",
      "validation accuracy 0.8695652173913043 train loss 1102527.5388494194 val loss 11585647.152570108\n",
      "validation accuracy 0.8913043478260869 train loss 1344646.065536365 val loss 10346723.949832352\n",
      "validation accuracy 0.8913043478260869 train loss 1269527.8860170427 val loss 10565849.023857886\n",
      "validation accuracy 0.8913043478260869 train loss 1304579.6236346466 val loss 10408776.401380118\n",
      "validation accuracy 0.8478260869565217 train loss 7043378.417401943 val loss 27956114.144737028\n",
      "validation accuracy 0.8260869565217391 train loss 7168242.070543129 val loss 28668609.586254068\n",
      "validation accuracy 0.8695652173913043 train loss 1106608.6048581325 val loss 11577720.858773109\n",
      "validation accuracy 0.8913043478260869 train loss 1346576.102925722 val loss 10342159.18792774\n",
      "validation accuracy 0.8913043478260869 train loss 1271457.9234063998 val loss 10556666.911705699\n",
      "validation accuracy 0.8913043478260869 train loss 1306509.6610240075 val loss 10403780.701835044\n",
      "validation accuracy 0.8478260869565217 train loss 7040797.422268026 val loss 27941009.601392984\n",
      "validation accuracy 0.8260869565217391 train loss 7160379.576830532 val loss 28636519.459857963\n",
      "validation accuracy 0.8695652173913043 train loss 1110689.6708668424 val loss 11569794.56497604\n",
      "validation accuracy 0.8913043478260869 train loss 1348506.140315089 val loss 10337594.426023092\n",
      "validation accuracy 0.8260869565217391 train loss 8165299.96120703 val loss 34649132.4530135\n",
      "validation accuracy 0.8260869565217391 train loss 8376263.818340314 val loss 35954963.03034776\n",
      "validation accuracy 0.8913043478260869 train loss 1119198.481330879 val loss 11310661.229127372\n",
      "validation accuracy 0.8260869565217391 train loss 7087690.180438459 val loss 28258810.516390577\n",
      "validation accuracy 0.8260869565217391 train loss 7222531.921710577 val loss 29013250.358430162\n",
      "validation accuracy 0.8913043478260869 train loss 1284250.1796357227 val loss 10478816.83604176\n",
      "validation accuracy 0.8913043478260869 train loss 1319301.9172533294 val loss 10337561.203216404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8260869565217391 train loss 8184574.652588186 val loss 34769691.89190636\n",
      "validation accuracy 0.8260869565217391 train loss 8395538.50972146 val loss 36074376.019830555\n",
      "validation accuracy 0.8913043478260869 train loss 1125218.2262726082 val loss 11256842.095474847\n",
      "validation accuracy 0.8260869565217391 train loss 7106964.871819604 val loss 28363185.13627573\n",
      "validation accuracy 0.8260869565217391 train loss 7241806.613091723 val loss 29119422.366696186\n",
      "validation accuracy 0.8260869565217391 train loss 7376648.354363848 val loss 29883741.31560505\n",
      "validation accuracy 0.8913043478260869 train loss 1291375.6114851069 val loss 10360432.463758402\n",
      "validation accuracy 0.8478260869565217 train loss 7001522.8438673355 val loss 27729476.39844023\n",
      "validation accuracy 0.8260869565217391 train loss 7126872.997721533 val loss 28453202.162585996\n",
      "validation accuracy 0.8260869565217391 train loss 7261714.738993647 val loss 29213452.4660473\n",
      "validation accuracy 0.8913043478260869 train loss 1250285.7131934632 val loss 10518424.665341768\n",
      "validation accuracy 0.8913043478260869 train loss 1285337.4508110685 val loss 10356340.732987706\n",
      "validation accuracy 0.8478260869565217 train loss 7001247.4582611015 val loss 27718413.00905226\n",
      "validation accuracy 0.8260869565217391 train loss 7123109.496579298 val loss 28426156.49194657\n",
      "validation accuracy 0.8260869565217391 train loss 7257951.237851415 val loss 29189836.68795514\n",
      "validation accuracy 0.8913043478260869 train loss 1286843.748143408 val loss 10395970.942723192\n",
      "validation accuracy 0.8478260869565217 train loss 6991445.725663822 val loss 27625993.88504835\n",
      "validation accuracy 0.8260869565217391 train loss 7107040.500574449 val loss 28302775.146410123\n",
      "validation accuracy 0.8260869565217391 train loss 7241882.24184657 val loss 29063025.449871413\n",
      "validation accuracy 0.8260869565217391 train loss 8182940.393893837 val loss 34689914.779066905\n",
      "validation accuracy 0.8260869565217391 train loss 8393904.251027096 val loss 35994598.90699103\n",
      "validation accuracy 0.8913043478260869 train loss 1126788.3383903694 val loss 11253748.072440043\n",
      "validation accuracy 0.8260869565217391 train loss 7105330.61312525 val loss 28293567.608902805\n",
      "validation accuracy 0.8260869565217391 train loss 7240172.354397369 val loss 29049151.183927357\n",
      "validation accuracy 0.8260869565217391 train loss 7375014.095669484 val loss 29809401.487388648\n",
      "validation accuracy 0.8913043478260869 train loss 1292945.7236028686 val loss 10350477.334521616\n",
      "validation accuracy 0.8478260869565217 train loss 7001444.929064845 val loss 27665580.950186174\n",
      "validation accuracy 0.8260869565217391 train loss 7125238.739027172 val loss 28382930.979817167\n",
      "validation accuracy 0.8260869565217391 train loss 7260080.480299294 val loss 29143181.283278473\n",
      "validation accuracy 0.8913043478260869 train loss 1251855.8253112216 val loss 10511162.178561047\n",
      "validation accuracy 0.8913043478260869 train loss 1286907.5629288289 val loss 10347289.694883935\n",
      "validation accuracy 0.8478260869565217 train loss 7001169.543458613 val loss 27660047.435832173\n",
      "validation accuracy 0.8260869565217391 train loss 7121475.237884939 val loss 28355885.30917772\n",
      "validation accuracy 0.8260869565217391 train loss 7256316.979157051 val loss 29116135.612639025\n",
      "validation accuracy 0.8913043478260869 train loss 1288413.8602611618 val loss 10386912.869333073\n",
      "validation accuracy 0.8478260869565217 train loss 6991367.810861327 val loss 27567628.31182825\n",
      "validation accuracy 0.8260869565217391 train loss 7105406.241880091 val loss 28235583.35220108\n",
      "validation accuracy 0.8260869565217391 train loss 7240247.983152205 val loss 28992754.26710255\n",
      "validation accuracy 0.8913043478260869 train loss 1117386.298563704 val loss 11383288.340302764\n",
      "validation accuracy 0.8478260869565217 train loss 7049831.720315183 val loss 27903893.744771224\n",
      "validation accuracy 0.8260869565217391 train loss 7175520.498319045 val loss 28623646.056933045\n",
      "validation accuracy 0.8260869565217391 train loss 7310362.239591165 val loss 29383896.360394347\n",
      "validation accuracy 0.8913043478260869 train loss 1283543.6837761975 val loss 10443887.66621011\n",
      "validation accuracy 0.8913043478260869 train loss 1318595.4213938033 val loss 10306708.958667487\n",
      "validation accuracy 0.8478260869565217 train loss 7049556.334708936 val loss 27881067.129461847\n",
      "validation accuracy 0.8260869565217391 train loss 7171756.997176788 val loss 28596600.38629348\n",
      "validation accuracy 0.8695652173913043 train loss 1108065.2761743069 val loss 11518072.286468701\n",
      "validation accuracy 0.8913043478260869 train loss 1346164.998286453 val loss 10250681.837418381\n",
      "validation accuracy 0.8260869565217391 train loss 8153102.464125694 val loss 34476242.18537806\n",
      "validation accuracy 0.8260869565217391 train loss 8364066.321258966 val loss 35780926.313302256\n",
      "validation accuracy 0.8913043478260869 train loss 1116857.3393022479 val loss 11256781.024694422\n",
      "validation accuracy 0.8260869565217391 train loss 7075492.683357115 val loss 28094932.714785736\n",
      "validation accuracy 0.8260869565217391 train loss 7210334.424629236 val loss 28852695.15019945\n",
      "validation accuracy 0.8260869565217391 train loss 7345176.16590135 val loss 29612945.45366074\n",
      "validation accuracy 0.8913043478260869 train loss 1283014.7245147517 val loss 10324063.26199992\n",
      "validation accuracy 0.8478260869565217 train loss 6977925.423295258 val loss 27495084.338946976\n",
      "validation accuracy 0.8260869565217391 train loss 7095400.809259062 val loss 28188811.44239807\n",
      "validation accuracy 0.8260869565217391 train loss 7230242.550531182 val loss 28946725.249550693\n",
      "validation accuracy 0.8913043478260869 train loss 1111987.1628172938 val loss 11346870.1999796\n",
      "validation accuracy 0.8478260869565217 train loss 7036389.332749106 val loss 27843907.395079963\n",
      "validation accuracy 0.8260869565217391 train loss 7165515.065698017 val loss 28577617.03938117\n",
      "validation accuracy 0.8260869565217391 train loss 7300356.806970136 val loss 29337867.34284246\n",
      "validation accuracy 0.8913043478260869 train loss 1278144.5480297916 val loss 10385095.313246883\n",
      "validation accuracy 0.8913043478260869 train loss 1313196.2856473993 val loss 10243995.731441515\n",
      "validation accuracy 0.8478260869565217 train loss 7036113.947142888 val loss 27818010.2296794\n",
      "validation accuracy 0.8260869565217391 train loss 7161751.56455578 val loss 28550571.36874172\n",
      "validation accuracy 0.8260869565217391 train loss 7296593.305827899 val loss 29310821.67220302\n",
      "validation accuracy 0.8913043478260869 train loss 1272106.3873557625 val loss 10382045.772185769\n",
      "validation accuracy 0.8478260869565217 train loss 6943185.565952099 val loss 27258228.02420876\n",
      "validation accuracy 0.8478260869565217 train loss 7054972.501079073 val loss 27913715.255418964\n",
      "validation accuracy 0.8260869565217391 train loss 7181659.690457729 val loss 28644601.46809297\n",
      "validation accuracy 0.8695652173913043 train loss 1105031.3222968918 val loss 11437108.128004685\n",
      "validation accuracy 0.8478260869565217 train loss 7001649.475405947 val loss 27577898.918396004\n",
      "validation accuracy 0.8260869565217391 train loss 7116932.2056245655 val loss 28280388.449212816\n",
      "validation accuracy 0.8260869565217391 train loss 7251773.946896684 val loss 29035743.56138476\n",
      "validation accuracy 0.8913043478260869 train loss 1137298.547464985 val loss 11292717.012873687\n",
      "validation accuracy 0.8478260869565217 train loss 7060113.3848598115 val loss 27943740.961396027\n",
      "validation accuracy 0.8260869565217391 train loss 7187046.462063521 val loss 28666726.1529112\n",
      "validation accuracy 0.8260869565217391 train loss 7321888.2033356335 val loss 29426885.65467656\n",
      "validation accuracy 0.8913043478260869 train loss 1303455.9326774748 val loss 10355407.264952455\n",
      "validation accuracy 0.8913043478260869 train loss 1338507.6702950778 val loss 10217649.004407683\n",
      "validation accuracy 0.8260869565217391 train loss 8118185.798475386 val loss 34229303.91710372\n",
      "validation accuracy 0.8260869565217391 train loss 8329149.655608646 val loss 35533988.04502784\n",
      "validation accuracy 0.8913043478260869 train loss 1109200.0113108624 val loss 11265906.041515717\n",
      "validation accuracy 0.8478260869565217 train loss 7041805.499119202 val loss 27877659.99725297\n",
      "validation accuracy 0.8260869565217391 train loss 7175417.758978911 val loss 28625618.12118037\n",
      "validation accuracy 0.8260869565217391 train loss 7310259.500251023 val loss 29385868.42464166\n",
      "validation accuracy 0.8913043478260869 train loss 1275357.3965233737 val loss 10304659.372473141\n",
      "validation accuracy 0.8478260869565217 train loss 6948877.11792842 val loss 27286121.996733822\n",
      "validation accuracy 0.8478260869565217 train loss 7060664.053055401 val loss 27971538.724865295\n",
      "validation accuracy 0.8260869565217391 train loss 7195325.884880854 val loss 28719648.220531613\n",
      "validation accuracy 0.8260869565217391 train loss 7330167.626152976 val loss 29479898.523992907\n",
      "validation accuracy 0.8913043478260869 train loss 1277969.3807633691 val loss 10277325.941680733\n",
      "validation accuracy 0.8478260869565217 train loss 6967735.671864616 val loss 27381026.427786343\n",
      "validation accuracy 0.8260869565217391 train loss 7080392.269510693 val loss 28069425.886198666\n",
      "validation accuracy 0.8260869565217391 train loss 7215234.010782807 val loss 28813678.31988287\n",
      "validation accuracy 0.8695652173913043 train loss 1107041.272938415 val loss 11314149.192201288\n",
      "validation accuracy 0.8478260869565217 train loss 7026199.581318471 val loss 27713290.99561458\n",
      "validation accuracy 0.8260869565217391 train loss 7150506.52594964 val loss 28444570.109713335\n",
      "validation accuracy 0.8260869565217391 train loss 7285348.26722176 val loss 29204820.413174633\n",
      "validation accuracy 0.8913043478260869 train loss 1273099.2042784123 val loss 10343377.211194776\n",
      "validation accuracy 0.8478260869565217 train loss 6933271.200127697 val loss 27149703.62740216\n",
      "validation accuracy 0.8478260869565217 train loss 7045058.135254666 val loss 27809832.32292385\n",
      "validation accuracy 0.8260869565217391 train loss 7170414.651851589 val loss 28538600.209064543\n",
      "validation accuracy 0.8260869565217391 train loss 7305256.3931236975 val loss 29298850.512525834\n",
      "validation accuracy 0.8260869565217391 train loss 8179434.02879848 val loss 34553428.91609332\n",
      "validation accuracy 0.8695652173913043 train loss 1078043.3170698956 val loss 11541223.634278359\n",
      "validation accuracy 0.8478260869565217 train loss 6940598.8229997065 val loss 27185536.417974293\n",
      "validation accuracy 0.8478260869565217 train loss 7052385.758126685 val loss 27859118.330344416\n",
      "validation accuracy 0.8260869565217391 train loss 7184118.790868485 val loss 28600507.05245708\n",
      "validation accuracy 0.8260869565217391 train loss 7318960.5321406005 val loss 29360757.355918378\n",
      "validation accuracy 0.8913043478260869 train loss 1275502.1497187917 val loss 10279411.28843014\n",
      "validation accuracy 0.8478260869565217 train loss 6959457.376935905 val loss 27280440.849026844\n",
      "validation accuracy 0.8478260869565217 train loss 7071244.312062881 val loss 27962014.937505085\n",
      "validation accuracy 0.8260869565217391 train loss 7204026.916770431 val loss 28694537.15180833\n",
      "validation accuracy 0.8695652173913043 train loss 1104934.5667037582 val loss 11327869.322852345\n",
      "validation accuracy 0.8478260869565217 train loss 7017921.286389759 val loss 27605071.91368549\n",
      "validation accuracy 0.8260869565217391 train loss 7139299.431937269 val loss 28330876.505440935\n",
      "validation accuracy 0.8260869565217391 train loss 7274141.173209391 val loss 29085679.245100126\n",
      "validation accuracy 0.8913043478260869 train loss 1314728.3275003699 val loss 10216943.812980141\n",
      "validation accuracy 0.8478260869565217 train loss 7012223.434705143 val loss 27542231.06122008\n",
      "validation accuracy 0.8260869565217391 train loss 7127039.802943943 val loss 28251436.439111203\n",
      "validation accuracy 0.8260869565217391 train loss 7261881.544216056 val loss 28999880.20216418\n",
      "validation accuracy 0.8913043478260869 train loss 1143700.7658029096 val loss 11211582.9635994\n",
      "validation accuracy 0.8478260869565217 train loss 7070687.344158993 val loss 27905064.55918219\n",
      "validation accuracy 0.8260869565217391 train loss 7197154.059382892 val loss 28630771.99199466\n",
      "validation accuracy 0.8260869565217391 train loss 7567518.023530418 val loss 30736473.876238603\n",
      "validation accuracy 0.8478260869565217 train loss 7077785.746756355 val loss 27932579.554412395\n",
      "validation accuracy 0.8260869565217391 train loss 7206572.552660162 val loss 28670828.835196443\n",
      "validation accuracy 0.8260869565217391 train loss 7576936.516807691 val loss 30782498.128731262\n",
      "validation accuracy 0.8478260869565217 train loss 7084884.149353712 val loss 27971573.281230286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8260869565217391 train loss 7215991.045937431 val loss 28710885.678398233\n",
      "validation accuracy 0.8913043478260869 train loss 1295772.3249199 val loss 10297754.24877996\n",
      "validation accuracy 0.8478260869565217 train loss 6967399.362542113 val loss 27239022.3076716\n",
      "validation accuracy 0.8478260869565217 train loss 7079186.297669083 val loss 27897510.428599857\n",
      "validation accuracy 0.8260869565217391 train loss 7203731.416944104 val loss 28629017.46347584\n",
      "validation accuracy 0.8260869565217391 train loss 8180583.038995724 val loss 34431811.10627955\n",
      "validation accuracy 0.8695652173913043 train loss 1097935.1244467506 val loss 11519074.527700448\n",
      "validation accuracy 0.8913043478260869 train loss 1331009.495472787 val loss 10164922.160842126\n",
      "validation accuracy 0.8478260869565217 train loss 7042823.575377858 val loss 27672214.336838074\n",
      "validation accuracy 0.8260869565217391 train loss 7161596.174021535 val loss 28396216.331303805\n",
      "validation accuracy 0.8260869565217391 train loss 7296437.915293656 val loss 29146338.870038196\n",
      "validation accuracy 0.8913043478260869 train loss 1159981.933775323 val loss 11129196.886653537\n",
      "validation accuracy 0.8478260869565217 train loss 7101287.484831709 val loss 28046245.033624534\n",
      "validation accuracy 0.8260869565217391 train loss 7231710.4304604875 val loss 28777230.65986869\n",
      "validation accuracy 0.8260869565217391 train loss 7602074.394608014 val loss 30884551.0331619\n",
      "validation accuracy 0.8260869565217391 train loss 7241573.881467392 val loss 28830006.982330278\n",
      "validation accuracy 0.8913043478260869 train loss 1308408.1047363966 val loss 10247659.891174316\n",
      "validation accuracy 0.8478260869565217 train loss 6990908.596304952 val loss 27343839.634453148\n",
      "validation accuracy 0.8478260869565217 train loss 7102695.531431929 val loss 28010575.400648143\n",
      "validation accuracy 0.8260869565217391 train loss 7229314.252474052 val loss 28747593.670335937\n",
      "validation accuracy 0.8913043478260869 train loss 1137380.5430389303 val loss 11255287.547440916\n",
      "validation accuracy 0.8478260869565217 train loss 7049372.505758802 val loss 27675309.293001276\n",
      "validation accuracy 0.8260869565217391 train loss 7164586.767640889 val loss 28389335.407381687\n",
      "validation accuracy 0.8260869565217391 train loss 7299428.508913008 val loss 29135350.032686062\n",
      "validation accuracy 0.8913043478260869 train loss 1347634.2825179691 val loss 10185257.86046347\n",
      "validation accuracy 0.8478260869565217 train loss 7043674.654074195 val loss 27610105.292715\n",
      "validation accuracy 0.8478260869565217 train loss 7155461.589201168 val loss 28309895.341051973\n",
      "validation accuracy 0.8260869565217391 train loss 8129178.760699179 val loss 34050129.94904375\n",
      "validation accuracy 0.8260869565217391 train loss 8340142.617832455 val loss 35354814.07696795\n",
      "validation accuracy 0.8913043478260869 train loss 1152749.3764963471 val loss 11203105.097819708\n",
      "validation accuracy 0.8913043478260869 train loss 1395048.397618094 val loss 9987497.412505642\n",
      "validation accuracy 0.8260869565217391 train loss 7298025.793160293 val loss 29143140.911061153\n",
      "validation accuracy 0.8913043478260869 train loss 1343467.4339902285 val loss 10146484.047374781\n",
      "validation accuracy 0.8478260869565217 train loss 7039201.616996936 val loss 27605053.187038444\n",
      "validation accuracy 0.8478260869565217 train loss 7150988.552123911 val loss 28315879.03681574\n",
      "validation accuracy 0.8260869565217391 train loss 7285766.164166951 val loss 29057341.86812511\n",
      "validation accuracy 0.8913043478260869 train loss 1346473.8899651174 val loss 10175639.365849843\n",
      "validation accuracy 0.8913043478260869 train loss 1381525.6275827265 val loss 10046522.264381083\n",
      "validation accuracy 0.8478260869565217 train loss 7123472.735511953 val loss 28128490.93685823\n",
      "validation accuracy 0.8260869565217391 train loss 8091324.1596005205 val loss 33850693.23192242\n",
      "validation accuracy 0.8260869565217391 train loss 8302288.016733791 val loss 35155377.35984661\n",
      "validation accuracy 0.8913043478260869 train loss 1132798.0371904252 val loss 11224632.268531414\n",
      "validation accuracy 0.8478260869565217 train loss 7032037.058847269 val loss 27595457.463631872\n",
      "validation accuracy 0.8260869565217391 train loss 7148556.120104054 val loss 28317670.03449304\n",
      "validation accuracy 0.8260869565217391 train loss 7283397.861376176 val loss 29062553.201315783\n",
      "validation accuracy 0.8913043478260869 train loss 1343051.7766694692 val loss 10139998.588929001\n",
      "validation accuracy 0.8478260869565217 train loss 7026339.207162677 val loss 27529403.188422695\n",
      "validation accuracy 0.8478260869565217 train loss 7138126.142289656 val loss 28238229.968163457\n",
      "validation accuracy 0.8260869565217391 train loss 7271138.23238287 val loss 28976754.158379983\n",
      "validation accuracy 0.8913043478260869 train loss 1346058.2326443573 val loss 10168952.85471623\n",
      "validation accuracy 0.8913043478260869 train loss 1381109.970261965 val loss 10026329.702666529\n",
      "validation accuracy 0.8478260869565217 train loss 7113294.351062601 val loss 28044430.86002435\n",
      "validation accuracy 0.8260869565217391 train loss 7235206.976345326 val loss 28779125.97933211\n",
      "validation accuracy 0.8913043478260869 train loss 1340414.543705206 val loss 10225447.245905083\n",
      "validation accuracy 0.8913043478260869 train loss 1375466.2813228066 val loss 10069911.2421412\n",
      "validation accuracy 0.8478260869565217 train loss 7088462.559835503 val loss 27867482.349177316\n",
      "validation accuracy 0.8478260869565217 train loss 7200249.494962454 val loss 28583003.720812764\n",
      "validation accuracy 0.8913043478260869 train loss 1334770.8547660494 val loss 10281941.637093987\n",
      "validation accuracy 0.8913043478260869 train loss 1328449.5592891183 val loss 10416142.86326376\n",
      "validation accuracy 0.8913043478260869 train loss 1407597.6511732498 val loss 10112341.941032771\n",
      "validation accuracy 0.8478260869565217 train loss 7202111.921047368 val loss 28547999.49473343\n",
      "validation accuracy 0.8913043478260869 train loss 1356016.6875453675 val loss 10314717.436176997\n",
      "validation accuracy 0.8913043478260869 train loss 1392707.6713870647 val loss 10114116.375144847\n",
      "validation accuracy 0.9130434782608695 train loss 1527371.6833034912 val loss 9712649.947095184\n",
      "validation accuracy 0.8260869565217391 train loss 8169618.316686563 val loss 34323967.09801893\n",
      "validation accuracy 0.8260869565217391 train loss 8357007.256392209 val loss 35502939.61293781\n",
      "validation accuracy 0.8260869565217391 train loss 7954043.653307245 val loss 33032053.23905402\n",
      "validation accuracy 0.8478260869565217 train loss 7151377.013809554 val loss 28365106.57443548\n",
      "validation accuracy 0.8260869565217391 train loss 7285355.9387153275 val loss 29104299.245173864\n",
      "validation accuracy 0.8913043478260869 train loss 1361633.664234728 val loss 10087113.48409088\n",
      "validation accuracy 0.8478260869565217 train loss 7033892.226997909 val loss 27580259.959341027\n",
      "validation accuracy 0.8478260869565217 train loss 7145679.162124855 val loss 28285666.508105285\n",
      "validation accuracy 0.8260869565217391 train loss 7273096.30972191 val loss 29022684.77779291\n",
      "validation accuracy 0.8913043478260869 train loss 1364640.1202095863 val loss 10116067.749878235\n",
      "validation accuracy 0.8913043478260869 train loss 1399691.8578271861 val loss 9965643.814486645\n",
      "validation accuracy 0.8478260869565217 train loss 7118163.345512829 val loss 28101023.755648024\n",
      "validation accuracy 0.8260869565217391 train loss 7236644.424375881 val loss 28835296.677834872\n",
      "validation accuracy 0.8913043478260869 train loss 1348855.6294313855 val loss 10154765.844055757\n",
      "validation accuracy 0.8478260869565217 train loss 7000678.558701178 val loss 27338328.445961695\n",
      "validation accuracy 0.8478260869565217 train loss 7112465.493828128 val loss 28028715.889703125\n",
      "validation accuracy 0.8260869565217391 train loss 8066394.676161985 val loss 33696152.66879454\n",
      "validation accuracy 0.8260869565217391 train loss 8253783.615867634 val loss 34875125.18371351\n",
      "validation accuracy 0.8260869565217391 train loss 7850820.012782669 val loss 32411726.317670524\n",
      "validation accuracy 0.8913043478260869 train loss 1326986.7885400485 val loss 10185778.616863757\n",
      "validation accuracy 0.8913043478260869 train loss 1320665.4930631155 val loss 10319979.843033534\n",
      "validation accuracy 0.8913043478260869 train loss 1399813.584947254 val loss 10002901.133574188\n",
      "validation accuracy 0.8478260869565217 train loss 7191359.946279668 val loss 28571295.567853197\n",
      "validation accuracy 0.8913043478260869 train loss 1348232.6213193713 val loss 10218554.415946761\n",
      "validation accuracy 0.8913043478260869 train loss 1341911.3258424383 val loss 10352755.642116545\n",
      "validation accuracy 0.8913043478260869 train loss 1421059.4177265726 val loss 10035433.042170964\n",
      "validation accuracy 0.8478260869565217 train loss 7193222.372364565 val loss 28536636.07742507\n",
      "validation accuracy 0.8913043478260869 train loss 1369478.4540986884 val loss 10251330.215029787\n",
      "validation accuracy 0.8913043478260869 train loss 1406169.4379403875 val loss 10039598.79453688\n",
      "validation accuracy 0.9130434782608695 train loss 1542962.4254103051 val loss 9641516.555594979\n",
      "validation accuracy 0.8478260869565217 train loss 7159873.5376257 val loss 28373320.628542706\n",
      "validation accuracy 0.8260869565217391 train loss 8118310.179610925 val loss 34074080.430800386\n",
      "validation accuracy 0.8260869565217391 train loss 8305699.11931657 val loss 35253052.9457193\n",
      "validation accuracy 0.8260869565217391 train loss 7902735.516231605 val loss 32792062.968420126\n",
      "validation accuracy 0.8478260869565217 train loss 7111608.927303603 val loss 28151776.308869682\n",
      "validation accuracy 0.8260869565217391 train loss 7234047.801639688 val loss 28888794.578557316\n",
      "validation accuracy 0.8913043478260869 train loss 1447114.2387032462 val loss 9663502.999150358\n",
      "validation accuracy 0.8260869565217391 train loss 8089940.478678463 val loss 33953408.478991374\n",
      "validation accuracy 0.8913043478260869 train loss 1371872.4894638676 val loss 9866651.306596683\n",
      "validation accuracy 0.8478260869565217 train loss 7087811.935063556 val loss 28001745.047015727\n",
      "validation accuracy 0.8260869565217391 train loss 7208935.446487469 val loss 28738763.31670329\n",
      "validation accuracy 0.8913043478260869 train loss 1331177.0629071104 val loss 10085301.28526336\n",
      "validation accuracy 0.8478260869565217 train loss 6970327.148251901 val loss 27218723.338787016\n",
      "validation accuracy 0.8478260869565217 train loss 7082114.0833788505 val loss 27926781.749788485\n",
      "validation accuracy 0.8260869565217391 train loss 7196675.817494053 val loss 28659323.25037311\n",
      "validation accuracy 0.8913043478260869 train loss 1334183.5188819715 val loss 10114255.55105071\n",
      "validation accuracy 0.8913043478260869 train loss 1327862.223405039 val loss 10248456.77722049\n",
      "validation accuracy 0.8913043478260869 train loss 1407010.3152891747 val loss 9932495.752686247\n",
      "validation accuracy 0.8478260869565217 train loss 7081284.988391342 val loss 27890713.526832648\n",
      "validation accuracy 0.8478260869565217 train loss 7193071.923518305 val loss 28613872.85037431\n",
      "validation accuracy 0.8913043478260869 train loss 1356174.0868933727 val loss 10144495.967180774\n",
      "validation accuracy 0.8913043478260869 train loss 1453827.7867659894 val loss 9779030.172824658\n",
      "validation accuracy 0.8260869565217391 train loss 7307355.364776777 val loss 29290977.950980745\n",
      "validation accuracy 0.8913043478260869 train loss 1402246.8231381136 val loss 9959516.010266218\n",
      "validation accuracy 0.8478260869565217 train loss 7068451.910327095 val loss 27775152.483862355\n",
      "validation accuracy 0.8260869565217391 train loss 8002263.87529076 val loss 33356940.170428135\n",
      "validation accuracy 0.8478260869565217 train loss 7208244.933620304 val loss 28696053.30621802\n",
      "validation accuracy 0.8913043478260869 train loss 1372979.4041055273 val loss 10119183.549206724\n",
      "validation accuracy 0.8913043478260869 train loss 1409670.3879472218 val loss 9909840.936685897\n",
      "validation accuracy 0.8478260869565217 train loss 7082198.055768677 val loss 27885331.43514999\n",
      "validation accuracy 0.8478260869565217 train loss 7193984.990895637 val loss 28597245.569088988\n",
      "validation accuracy 0.8913043478260869 train loss 1368974.9613904655 val loss 10140054.945138544\n",
      "validation accuracy 0.8913043478260869 train loss 1466628.6612630691 val loss 9785284.262040855\n",
      "validation accuracy 0.8913043478260869 train loss 1399550.3563092826 val loss 10027342.759412711\n",
      "validation accuracy 0.8478260869565217 train loss 7034226.402391638 val loss 27595467.290819503\n",
      "validation accuracy 0.8478260869565217 train loss 7146013.337518601 val loss 28287443.473267853\n",
      "validation accuracy 0.8478260869565217 train loss 7257800.27264556 val loss 29018973.003252674\n",
      "validation accuracy 0.8913043478260869 train loss 1404196.0585082355 val loss 10020674.029943014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.8478260869565217 train loss 7039100.399209356 val loss 27613553.716697365\n",
      "validation accuracy 0.8478260869565217 train loss 7150887.334336315 val loss 28305529.89914569\n",
      "validation accuracy 0.8478260869565217 train loss 7262674.269463274 val loss 29036847.458312564\n",
      "validation accuracy 0.8913043478260869 train loss 1408841.7607071833 val loss 10014005.30047333\n",
      "validation accuracy 0.8913043478260869 train loss 1445532.7445488814 val loss 9822939.67855128\n",
      "validation accuracy 0.8478260869565217 train loss 7248421.822426088 val loss 28950966.57783141\n",
      "validation accuracy 0.8913043478260869 train loss 1393951.7809209968 val loss 10019615.782346707\n",
      "validation accuracy 0.8478260869565217 train loss 7029721.948989901 val loss 27541211.120240703\n",
      "validation accuracy 0.8478260869565217 train loss 7141508.884116858 val loss 28240335.19650777\n",
      "validation accuracy 0.8260869565217391 train loss 8091194.432274004 val loss 33906343.023633674\n",
      "validation accuracy 0.8260869565217391 train loss 7688230.829189054 val loss 31492638.744375393\n",
      "validation accuracy 0.8913043478260869 train loss 1393773.379695178 val loss 9939998.14453048\n",
      "validation accuracy 0.8478260869565217 train loss 7051439.202773254 val loss 27698487.844418004\n",
      "validation accuracy 0.8478260869565217 train loss 7163226.1379002165 val loss 28416408.273698796\n",
      "validation accuracy 0.8913043478260869 train loss 1354717.1993625225 val loss 10136977.96569809\n",
      "validation accuracy 0.8913043478260869 train loss 1452370.899235142 val loss 9768943.145702386\n",
      "validation accuracy 0.8260869565217391 train loss 7269641.309354701 val loss 29093513.37430523\n",
      "validation accuracy 0.8913043478260869 train loss 1419295.5435957368 val loss 9877643.443918213\n",
      "validation accuracy 0.8478260869565217 train loss 7080851.59948981 val loss 27860550.93070937\n",
      "validation accuracy 0.8478260869565217 train loss 7192638.534616772 val loss 28573656.065019194\n",
      "validation accuracy 0.8913043478260869 train loss 1378600.1170389694 val loss 10099767.317437697\n",
      "validation accuracy 0.8913043478260869 train loss 1476253.8169115833 val loss 9757827.366271958\n",
      "validation accuracy 0.8913043478260869 train loss 1399264.13109471 val loss 9985452.832344307\n",
      "validation accuracy 0.8478260869565217 train loss 7034532.981957422 val loss 27562803.28921455\n",
      "validation accuracy 0.8478260869565217 train loss 7146319.917084379 val loss 28264906.6539326\n",
      "validation accuracy 0.8260869565217391 train loss 8095539.02536521 val loss 33932509.84172091\n",
      "validation accuracy 0.8260869565217391 train loss 7692575.422280256 val loss 31516355.070960008\n",
      "validation accuracy 0.8913043478260869 train loss 1399085.7298689117 val loss 9905682.364792597\n",
      "validation accuracy 0.8478260869565217 train loss 7056250.235740816 val loss 27723059.301843062\n",
      "validation accuracy 0.8478260869565217 train loss 7168037.170867762 val loss 28441583.621518716\n",
      "validation accuracy 0.8913043478260869 train loss 1360029.549536252 val loss 10097827.048427356\n",
      "validation accuracy 0.8913043478260869 train loss 1457683.2494088658 val loss 9744118.03909209\n",
      "validation accuracy 0.8913043478260869 train loss 1380693.5635920004 val loss 9980191.859079177\n",
      "validation accuracy 0.8913043478260869 train loss 1478347.2634646166 val loss 9668320.069280073\n",
      "validation accuracy 0.8913043478260869 train loss 1411268.958510825 val loss 9881853.368069883\n",
      "validation accuracy 0.8478260869565217 train loss 7068872.903140649 val loss 27807810.942603253\n",
      "validation accuracy 0.8478260869565217 train loss 7180659.8382676095 val loss 28528778.64662703\n",
      "validation accuracy 0.8913043478260869 train loss 1372212.7781781722 val loss 10067354.70963917\n",
      "validation accuracy 0.8913043478260869 train loss 1469866.4780507851 val loss 9723246.415565953\n",
      "validation accuracy 0.8913043478260869 train loss 1392876.792233917 val loss 9956362.86235645\n",
      "validation accuracy 0.8478260869565217 train loss 7022554.285608303 val loss 27507390.29306326\n",
      "validation accuracy 0.8478260869565217 train loss 7134341.220735259 val loss 28216520.02825526\n",
      "validation accuracy 0.8913043478260869 train loss 1353820.6119012628 val loss 10144335.224999832\n",
      "validation accuracy 0.8913043478260869 train loss 1451474.3117738757 val loss 9784048.683260757\n",
      "validation accuracy 0.8913043478260869 train loss 1500409.0041169608 val loss 9619050.999454923\n",
      "validation accuracy 0.8478260869565217 train loss 7006669.066196339 val loss 27395767.749924038\n",
      "validation accuracy 0.8478260869565217 train loss 7118456.0013233 val loss 28092177.22047072\n",
      "validation accuracy 0.8478260869565217 train loss 7230242.936450264 val loss 28819411.63720227\n",
      "validation accuracy 0.8913043478260869 train loss 1419791.025508908 val loss 9920311.137007635\n",
      "validation accuracy 0.8478260869565217 train loss 7053788.53779489 val loss 27691333.41577641\n",
      "validation accuracy 0.8478260869565217 train loss 7165575.472921852 val loss 28396868.849306505\n",
      "validation accuracy 0.8260869565217391 train loss 7280544.70532326 val loss 29133887.118994135\n",
      "validation accuracy 0.8913043478260869 train loss 1442942.3356963405 val loss 9839565.583992671\n",
      "validation accuracy 0.8478260869565217 train loss 7202130.591705459 val loss 28626956.666356076\n",
      "validation accuracy 0.8913043478260869 train loss 1391361.3720684643 val loss 10038863.050881403\n",
      "validation accuracy 0.8913043478260869 train loss 1428052.355910161 val loss 9841453.674393462\n",
      "validation accuracy 0.8478260869565217 train loss 7086655.562356283 val loss 27899263.854451306\n",
      "validation accuracy 0.8478260869565217 train loss 7198442.497483237 val loss 28625463.45061736\n",
      "validation accuracy 0.8260869565217391 train loss 7290812.275234973 val loss 29196683.71732349\n",
      "validation accuracy 0.8913043478260869 train loss 1451416.1137434398 val loss 9817865.77137497\n",
      "validation accuracy 0.8478260869565217 train loss 7209513.54389123 val loss 28689753.26468536\n",
      "validation accuracy 0.8260869565217391 train loss 7302364.100361876 val loss 29260973.531391446\n",
      "validation accuracy 0.8913043478260869 train loss 1462255.088281495 val loss 9807936.975088513\n",
      "validation accuracy 0.8478260869565217 train loss 7220584.590299211 val loss 28754043.078753293\n",
      "validation accuracy 0.8260869565217391 train loss 7313915.925488775 val loss 29325263.345459376\n",
      "validation accuracy 0.8913043478260869 train loss 1473094.0628195452 val loss 9798962.140681906\n",
      "validation accuracy 0.8478260869565217 train loss 7231655.636707198 val loss 28818332.892821267\n",
      "validation accuracy 0.8260869565217391 train loss 7325467.750615681 val loss 29389553.159527328\n",
      "validation accuracy 0.8913043478260869 train loss 1483933.0373575976 val loss 9789987.306275308\n",
      "validation accuracy 0.8478260869565217 train loss 7242726.683115182 val loss 28882622.706889216\n",
      "validation accuracy 0.8260869565217391 train loss 7337019.575742588 val loss 29453842.97359528\n",
      "validation accuracy 0.8913043478260869 train loss 1433809.2958647353 val loss 9942490.179790528\n",
      "validation accuracy 0.8478260869565217 train loss 7134893.573738611 val loss 28189178.106825296\n",
      "validation accuracy 0.8478260869565217 train loss 7246680.5088655725 val loss 28920944.809557423\n",
      "validation accuracy 0.8913043478260869 train loss 1444435.8227569447 val loss 9875540.752095027\n",
      "validation accuracy 0.8478260869565217 train loss 7171448.69252221 val loss 28414014.3569193\n",
      "validation accuracy 0.8478260869565217 train loss 7257751.555273559 val loss 28985234.623625383\n",
      "validation accuracy 0.8913043478260869 train loss 1455274.797295 val loss 9865211.292587325\n",
      "validation accuracy 0.8478260869565217 train loss 7182519.738930192 val loss 28478304.170987222\n",
      "validation accuracy 0.8478260869565217 train loss 7268822.60168154 val loss 29049524.437693305\n",
      "validation accuracy 0.8913043478260869 train loss 1466113.7718330491 val loss 9854881.83307964\n",
      "validation accuracy 0.8478260869565217 train loss 7193590.785338174 val loss 28542593.985055156\n",
      "validation accuracy 0.8478260869565217 train loss 7279893.648089514 val loss 29113814.251761224\n",
      "validation accuracy 0.8913043478260869 train loss 1476952.7463710979 val loss 9844552.373571968\n",
      "validation accuracy 0.8478260869565217 train loss 7204661.831746159 val loss 28609432.34898582\n",
      "validation accuracy 0.8478260869565217 train loss 7290964.694497504 val loss 29178104.065829184\n",
      "validation accuracy 0.8913043478260869 train loss 1426829.004878236 val loss 10002134.001661047\n",
      "validation accuracy 0.8913043478260869 train loss 1463519.988719937 val loss 9805439.59239583\n",
      "validation accuracy 0.8478260869565217 train loss 7200053.566456578 val loss 28625838.23099248\n",
      "validation accuracy 0.8260869565217391 train loss 7286520.201902898 val loss 29197058.497698583\n",
      "validation accuracy 0.8913043478260869 train loss 1474358.9632579866 val loss 9796308.032224037\n",
      "validation accuracy 0.8478260869565217 train loss 7211124.612864556 val loss 28690128.045060426\n",
      "validation accuracy 0.8260869565217391 train loss 7298072.027029794 val loss 29261348.31176649\n",
      "validation accuracy 0.8913043478260869 train loss 1485197.9377960335 val loss 9787176.472052265\n",
      "validation accuracy 0.8478260869565217 train loss 7222195.65927254 val loss 28754417.859128382\n",
      "validation accuracy 0.8260869565217391 train loss 7309623.852156704 val loss 29325638.12583444\n",
      "validation accuracy 0.8913043478260869 train loss 1435074.1963031758 val loss 9941047.81911537\n",
      "validation accuracy 0.8478260869565217 train loss 7114362.549895971 val loss 28068671.09249563\n",
      "validation accuracy 0.8478260869565217 train loss 7226149.485022934 val loss 28792739.961796604\n",
      "validation accuracy 0.8913043478260869 train loss 1445700.723195389 val loss 9874098.39141985\n",
      "validation accuracy 0.8478260869565217 train loss 7049695.086367552 val loss 27672260.57589959\n",
      "validation accuracy 0.8478260869565217 train loss 7161482.021494501 val loss 28370197.17390071\n",
      "validation accuracy 0.8478260869565217 train loss 7273268.956621467 val loss 29107215.44358835\n",
      "validation accuracy 0.8913043478260869 train loss 1468852.0333828195 val loss 9795506.11034799\n",
      "validation accuracy 0.8478260869565217 train loss 7198037.140278096 val loss 28600284.990950197\n",
      "validation accuracy 0.8478260869565217 train loss 7284340.003029446 val loss 29171505.257656276\n",
      "validation accuracy 0.8913043478260869 train loss 1479691.0079208668 val loss 9786374.550176214\n",
      "validation accuracy 0.8478260869565217 train loss 7209108.18668608 val loss 28664574.80501812\n",
      "validation accuracy 0.8478260869565217 train loss 7295411.049437424 val loss 29235795.0717242\n",
      "validation accuracy 0.8913043478260869 train loss 1429567.2664280063 val loss 9939773.36277598\n",
      "validation accuracy 0.8913043478260869 train loss 1466258.2502697078 val loss 9749295.733272536\n",
      "validation accuracy 0.8478260869565217 train loss 7204499.9213964995 val loss 28683529.236887515\n",
      "validation accuracy 0.8913043478260869 train loss 1501871.9339785003 val loss 9525235.631373473\n",
      "validation accuracy 0.8913043478260869 train loss 1424882.2481616363 val loss 9723950.967667148\n",
      "validation accuracy 0.8913043478260869 train loss 1473816.940504732 val loss 9548930.19499227\n",
      "validation accuracy 0.9130434782608695 train loss 1528007.3414830244 val loss 9406204.791088928\n",
      "validation accuracy 0.8478260869565217 train loss 7088884.692472592 val loss 27934135.5142662\n",
      "validation accuracy 0.8913043478260869 train loss 1354517.4415306752 val loss 9951464.556560105\n",
      "validation accuracy 0.8913043478260869 train loss 1452171.1414032835 val loss 9616110.646391725\n",
      "validation accuracy 0.8913043478260869 train loss 1501105.833746381 val loss 9453137.87143964\n",
      "validation accuracy 0.8478260869565217 train loss 7028942.085090082 val loss 27541545.828003816\n",
      "validation accuracy 0.8913043478260869 train loss 1332871.6424292312 val loss 10058826.675001726\n",
      "validation accuracy 0.8913043478260869 train loss 1430525.3423018446 val loss 9690075.975323703\n",
      "validation accuracy 0.8913043478260869 train loss 1479460.0346449385 val loss 9516224.781886304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 0.9130434782608695 train loss 1535625.9522493528 val loss 9378261.550934806\n",
      "validation accuracy 0.8478260869565217 train loss 7104310.755759656 val loss 28022059.964798763\n",
      "validation accuracy 0.8913043478260869 train loss 1360160.5356708835 val loss 9910837.115911327\n",
      "validation accuracy 0.8913043478260869 train loss 1457814.2355434923 val loss 9583027.221806962\n",
      "validation accuracy 0.8913043478260869 train loss 1506748.9278865885 val loss 9420568.157812037\n",
      "validation accuracy 0.8478260869565217 train loss 7044368.148377146 val loss 27629470.27853639\n",
      "validation accuracy 0.8913043478260869 train loss 1338514.7365694414 val loss 10018199.234352954\n",
      "validation accuracy 0.8913043478260869 train loss 1436168.4364420502 val loss 9656254.785019252\n",
      "validation accuracy 0.8913043478260869 train loss 1485103.1287851473 val loss 9483519.368780352\n",
      "validation accuracy 0.8478260869565217 train loss 6908607.9505636385 val loss 26730842.832526527\n",
      "validation accuracy 0.8478260869565217 train loss 7020394.885690594 val loss 27439475.381753013\n",
      "validation accuracy 0.8478260869565217 train loss 7132181.820817548 val loss 28155497.558830164\n",
      "validation accuracy 0.8913043478260869 train loss 1393578.305083416 val loss 9862002.82022462\n",
      "validation accuracy 0.8913043478260869 train loss 1442512.997426508 val loss 9654783.645737259\n",
      "validation accuracy 0.8913043478260869 train loss 1491447.6897696052 val loss 9495197.875294043\n",
      "validation accuracy 0.8478260869565217 train loss 7009313.274335915 val loss 27319315.934921354\n",
      "validation accuracy 0.8913043478260869 train loss 1323213.4984524576 val loss 10106557.459991395\n",
      "validation accuracy 0.8913043478260869 train loss 1420867.1983250654 val loss 9729541.283772606\n",
      "validation accuracy 0.8913043478260869 train loss 1469801.890668156 val loss 9555389.854507217\n",
      "validation accuracy 0.9130434782608695 train loss 1523515.468113 val loss 9413428.960837789\n",
      "validation accuracy 0.8478260869565217 train loss 7084681.945005481 val loss 27799830.07171627\n",
      "validation accuracy 0.8260869565217391 train loss 8023834.625537926 val loss 33423594.635612555\n",
      "validation accuracy 0.8913043478260869 train loss 1409887.188449259 val loss 9738575.075301357\n",
      "validation accuracy 0.8913043478260869 train loss 1458821.8807923533 val loss 9555362.1187689\n",
      "validation accuracy 0.9130434782608695 train loss 1509560.9323358545 val loss 9403909.131824205\n",
      "validation accuracy 0.8478260869565217 train loss 7065970.357848313 val loss 27685588.92339811\n",
      "validation accuracy 0.8913043478260869 train loss 1339522.3818182945 val loss 9972762.326645404\n",
      "validation accuracy 0.8913043478260869 train loss 1437176.081690908 val loss 9625429.140509326\n",
      "validation accuracy 0.8913043478260869 train loss 1486110.774034 val loss 9457590.003562886\n",
      "validation accuracy 0.8478260869565217 train loss 7006027.750465806 val loss 27292999.237135738\n",
      "validation accuracy 0.8478260869565217 train loss 7117814.685592749 val loss 28009324.085028134\n",
      "validation accuracy 0.8913043478260869 train loss 1361578.4652484749 val loss 9877202.024855163\n",
      "validation accuracy 0.8913043478260869 train loss 1459232.1651210818 val loss 9556268.771131076\n",
      "validation accuracy 0.9130434782608695 train loss 1508931.995321082 val loss 9401763.777335074\n",
      "validation accuracy 0.8478260869565217 train loss 7057872.0782102505 val loss 27613106.793917038\n",
      "validation accuracy 0.8913043478260869 train loss 1339932.6661470314 val loss 9984564.143296804\n",
      "validation accuracy 0.8913043478260869 train loss 1437586.366019643 val loss 9629496.334343357\n",
      "validation accuracy 0.8913043478260869 train loss 1486521.058362734 val loss 9458100.40799277\n",
      "validation accuracy 0.8478260869565217 train loss 6922111.880396742 val loss 26712767.594012447\n",
      "validation accuracy 0.8478260869565217 train loss 7033898.815523689 val loss 27423111.897133626\n",
      "validation accuracy 0.8478260869565217 train loss 7145685.750650652 val loss 28144085.021418698\n",
      "validation accuracy 0.8913043478260869 train loss 1394996.2346610017 val loss 9831167.027991839\n",
      "validation accuracy 0.8913043478260869 train loss 1443930.9270040942 val loss 9628039.726872105\n",
      "validation accuracy 0.8913043478260869 train loss 1492865.6193471877 val loss 9471791.810252301\n",
      "validation accuracy 0.8478260869565217 train loss 7022817.204169009 val loss 27302952.45030193\n",
      "validation accuracy 0.8913043478260869 train loss 1324631.4280300387 val loss 10072922.368935265\n",
      "validation accuracy 0.8913043478260869 train loss 1422285.1279026484 val loss 9701252.758273682\n",
      "validation accuracy 0.8913043478260869 train loss 1471219.8202457433 val loss 9531451.984369919\n",
      "validation accuracy 0.8478260869565217 train loss 6962874.596786499 val loss 26910362.76403956\n",
      "validation accuracy 0.8478260869565217 train loss 7074661.531913444 val loss 27623060.007083148\n",
      "validation accuracy 0.8260869565217391 train loss 8011143.716677961 val loss 33226944.27324611\n",
      "validation accuracy 0.8913043478260869 train loss 1358115.5189843997 val loss 9908993.524374869\n",
      "validation accuracy 0.8913043478260869 train loss 1455769.21885701 val loss 9577281.978315331\n",
      "validation accuracy 0.8913043478260869 train loss 1504703.9112000996 val loss 9417764.623883635\n",
      "validation accuracy 0.8478260869565217 train loss 7042725.012697216 val loss 27439793.263920292\n",
      "validation accuracy 0.8478260869565217 train loss 7154511.947824167 val loss 28165514.15802874\n",
      "validation accuracy 0.8913043478260869 train loss 1397832.9725326272 val loss 9806691.956876427\n",
      "validation accuracy 0.8913043478260869 train loss 1446767.6648757188 val loss 9605980.10287584\n",
      "validation accuracy 0.8913043478260869 train loss 1495702.3572188178 val loss 9450566.772597369\n",
      "validation accuracy 0.8478260869565217 train loss 7031643.401342529 val loss 27319633.817088533\n",
      "validation accuracy 0.8913043478260869 train loss 1327468.1659016623 val loss 10047185.786684982\n",
      "validation accuracy 0.8913043478260869 train loss 1425121.865774278 val loss 9679207.666088136\n",
      "validation accuracy 0.8913043478260869 train loss 1474056.5581173734 val loss 9509127.879490582\n",
      "validation accuracy 0.8478260869565217 train loss 6971700.793960011 val loss 26927044.13082615\n",
      "validation accuracy 0.8478260869565217 train loss 7083487.729086965 val loss 27639741.373869732\n",
      "validation accuracy 0.8913043478260869 train loss 1349524.2493318422 val loss 9951625.484894745\n",
      "validation accuracy 0.8913043478260869 train loss 1447177.9492044551 val loss 9610047.296709877\n",
      "validation accuracy 0.8913043478260869 train loss 1496112.6415475486 val loss 9448699.968697991\n",
      "validation accuracy 0.8478260869565217 train loss 7023545.121704457 val loss 27247151.687607393\n",
      "validation accuracy 0.8478260869565217 train loss 7135332.0568314 val loss 27965409.60338348\n",
      "validation accuracy 0.8260869565217391 train loss 8086359.74665378 val loss 33620329.90694088\n",
      "validation accuracy 0.8260869565217391 train loss 7683396.143568824 val loss 31198822.237019155\n",
      "validation accuracy 0.9130434782608695 train loss 1520570.161734458 val loss 9368068.144273557\n",
      "validation accuracy 0.8478260869565217 train loss 7075695.773608261 val loss 27599907.47306976\n",
      "validation accuracy 0.8478260869565217 train loss 7187482.708735211 val loss 28336925.74275732\n",
      "validation accuracy 0.8478260869565217 train loss 7126828.452320442 val loss 27920849.974996343\n",
      "validation accuracy 0.8913043478260869 train loss 1368577.6662509018 val loss 9856532.259593457\n",
      "validation accuracy 0.8913043478260869 train loss 1466231.3661235129 val loss 9533442.519578466\n",
      "validation accuracy 0.9130434782608695 train loss 1516362.1330002449 val loss 9377918.923124615\n",
      "validation accuracy 0.8478260869565217 train loss 7066885.844937923 val loss 27516503.40990497\n",
      "validation accuracy 0.8478260869565217 train loss 7178672.780064873 val loss 28253521.679592542\n",
      "validation accuracy 0.8913043478260869 train loss 1459535.2366751288 val loss 9485129.186026648\n",
      "validation accuracy 0.8913043478260869 train loss 1400055.6946161361 val loss 9695490.415393177\n",
      "validation accuracy 0.8913043478260869 train loss 1448990.386959223 val loss 9518096.960331386\n",
      "validation accuracy 0.8260869565217391 train loss 8156803.833460822 val loss 33974466.92250468\n",
      "validation accuracy 0.8260869565217391 train loss 7753840.230375881 val loss 31552959.252583034\n",
      "validation accuracy 0.8913043478260869 train loss 1409944.2373042926 val loss 9634530.385036318\n",
      "validation accuracy 0.8913043478260869 train loss 1458878.9296473758 val loss 9465206.042327523\n",
      "validation accuracy 0.8913043478260869 train loss 1399399.387588386 val loss 9664644.119365366\n",
      "validation accuracy 0.8913043478260869 train loss 1448334.079931472 val loss 9496485.187057422\n",
      "validation accuracy 0.8478260869565217 train loss 7019378.608060179 val loss 27100719.81264202\n",
      "validation accuracy 0.8478260869565217 train loss 7131165.543187116 val loss 27806985.444381025\n",
      "validation accuracy 0.8260869565217391 train loss 8081151.53857165 val loss 33434569.55819761\n",
      "validation accuracy 0.8260869565217391 train loss 7678187.93548672 val loss 31013061.88827607\n",
      "validation accuracy 0.8260869565217391 train loss 8150373.475977188 val loss 33864454.86649992\n",
      "validation accuracy 0.8260869565217391 train loss 7747409.87289224 val loss 31442947.196578294\n",
      "validation accuracy 0.8913043478260869 train loss 1394891.2409370616 val loss 9645067.353550842\n",
      "validation accuracy 0.8913043478260869 train loss 1443825.9332801513 val loss 9473497.596172743\n",
      "validation accuracy 0.8478260869565217 train loss 7011621.607122705 val loss 27055848.165040225\n",
      "validation accuracy 0.8478260869565217 train loss 7123408.542249642 val loss 27778683.040515125\n",
      "validation accuracy 0.8913043478260869 train loss 1349365.0831174292 val loss 9853856.705944853\n",
      "validation accuracy 0.8913043478260869 train loss 1447018.7829900477 val loss 9536085.230890173\n",
      "validation accuracy 0.8913043478260869 train loss 1495953.475333136 val loss 9388509.899292573\n",
      "validation accuracy 0.8478260869565217 train loss 7063465.934867134 val loss 27374336.475423776\n",
      "validation accuracy 0.8478260869565217 train loss 7175252.869994081 val loss 28111354.745111294\n",
      "validation accuracy 0.8260869565217391 train loss 8142543.333263933 val loss 33789237.98584592\n",
      "validation accuracy 0.8260869565217391 train loss 7739579.730179002 val loss 31367730.315924354\n",
      "validation accuracy 0.8913043478260869 train loss 1392005.7853118489 val loss 9645045.166372059\n",
      "validation accuracy 0.8913043478260869 train loss 1440940.477654947 val loss 9471541.036368161\n",
      "validation accuracy 0.8478260869565217 train loss 7004549.368112717 val loss 26987106.48828949\n",
      "validation accuracy 0.8478260869565217 train loss 7116336.303239665 val loss 27708538.37045296\n",
      "validation accuracy 0.8913043478260869 train loss 1346479.6274922285 val loss 9857772.196698064\n",
      "validation accuracy 0.8913043478260869 train loss 1444133.3273648424 val loss 9535974.978128945\n",
      "validation accuracy 0.8913043478260869 train loss 1493068.0197079247 val loss 9385746.608329013\n",
      "validation accuracy 0.8478260869565217 train loss 7056393.695857129 val loss 27304191.80536145\n",
      "validation accuracy 0.8478260869565217 train loss 7168180.630984076 val loss 28041210.07504897\n",
      "validation accuracy 0.8478260869565217 train loss 7107526.374569292 val loss 27625134.30728794\n"
     ]
    }
   ],
   "source": [
    "std = 0.0001\n",
    "num_iters = 1500\n",
    "loss_history = []\n",
    "loss_val_history = []\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, n_features, std):\n",
    "        self.n_features = n_features\n",
    "        self.weights = np.random.normal(0, std, self.n_features)\n",
    "    def predict_a_data(self, X_i):\n",
    "        y_i = np.dot(self.weights.T, X_i)\n",
    "        if y_i > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        return y_i\n",
    "    def update_weights(self, X_i, y_i):\n",
    "        self.weights += y_i * X_i\n",
    "    def predict_all_data(self, X):\n",
    "        y = np.where(np.dot(X, self.weights.T) > 0, 1, -1)\n",
    "        return y\n",
    "    def loss(self, X, y):\n",
    "        term0 = np.dot(self.weights.T, X.T)\n",
    "        term1 = 1 - np.multiply(term0, y.T)\n",
    "        term2 = np.where(term1 > 0, term1, 0)\n",
    "        return np.sum(term2, 0)\n",
    "    \n",
    "classifier = Perceptron(X_train.shape[1], std)\n",
    "for i in range(num_iters):\n",
    "    for j in range(X_train.shape[0]):\n",
    "        y_label = classifier.predict_a_data(X_train[j])\n",
    "        if y_train[j] != y_label:\n",
    "            classifier.update_weights(X_train[j], y_train[j])\n",
    "            \n",
    "    loss_history.append(classifier.loss(X_train, y_train))\n",
    "    loss_val_history.append(classifier.loss(X_val, y_val))\n",
    "    val_preds = classifier.predict_all_data(X_val)\n",
    "    array = confusion_matrix(val_preds, y_val)\n",
    "    print(\"validation accuracy\", accuracy(array), \"train loss\", loss_val_history[-1], \"val loss\", loss_history[-1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2792b84a048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss_val_history, label='validation')\n",
    "plt.plot(loss_history, label='train')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix [[27  0]\n",
      " [13 74]]\n",
      "test accuracy 0.8859649122807017\n"
     ]
    }
   ],
   "source": [
    "val_preds = classifier.predict_all_data(X_test)\n",
    "array = confusion_matrix(val_preds, y_test)\n",
    "print(\"confusion matrix\", array)\n",
    "print(\"test accuracy\", accuracy(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
