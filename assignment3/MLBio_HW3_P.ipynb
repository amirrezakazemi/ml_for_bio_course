{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLBio_HW3_P.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "efrE1Lc0Otrq"
      },
      "source": [
        "# **Machine Learning in Bioinformatics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GtzvENmYOtxi"
      },
      "source": [
        "**Homework 3:**<br/>\n",
        "!!! If you don't fill these fields, your homework does not count !!!<br/>\n",
        "first name and last name :amirreza kazemi<br/>\n",
        "student number :95105827"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KG3vb72VOt4G"
      },
      "source": [
        "You can run cells by hitting `Shift` + `Enter` or `ctrl` + `Enter`.<br/>\n",
        "We highly recommend you to read each line of code carefully and try to \n",
        "understand what it exactly does.<br/>\n",
        "Just alter the parts that is between green comments and specified for you. <br/>\n",
        "Please do not change other parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v0dotHjRO5x_",
        "colab": {}
      },
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_5C6lgUtO-bg"
      },
      "source": [
        "\n",
        "### about the Data:<br/>\n",
        "The purpose of this project is to classify tumors into malignant or benign. The following dataset is constructed based on images of tumors. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
        "For more details about the features of this dataset you can visit this link:\n",
        "https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset<br/>\n",
        "This dataset contains 30 features and 1 label called target.\n",
        "The original dataset labels are 0 and 1 and in the following code boxes we change it to -1 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-7_g8ApcO7tm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "f6381100-49b0-477b-fc75-7f109ef9e654"
      },
      "source": [
        "cancer = load_breast_cancer()  ## change if the data set changed\n",
        "df = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
        "features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  ...  worst fractal dimension  target\n",
              "0        17.99         10.38  ...                  0.11890     0.0\n",
              "1        20.57         17.77  ...                  0.08902     0.0\n",
              "2        19.69         21.25  ...                  0.08758     0.0\n",
              "3        11.42         20.38  ...                  0.17300     0.0\n",
              "4        20.29         14.34  ...                  0.07678     0.0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmmK95OVPDyg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "1711cced-5202-4137-e019-4d7a90600fec"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "cancer.target = np.where(cancer.target==0, -1, cancer.target)\n",
        "X_train ,X_test ,X_val ,y_train ,y_test ,y_val = None ,None ,None ,None ,None ,None\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# 1- Normalize tha data.                                                       #\n",
        "# 2- using train_test_split package, split your data into 3 numpy array        #\n",
        "# called X_train, X_test, and X_val and also split the corresponding labels as #\n",
        "# y_train, y_test, and y_val. After spliting, the ratio of your data should be # \n",
        "# approximately like this:                                                     #\n",
        "#  Train : 70%     test : 20%       validation : 10%                           #\n",
        "################################################################################\n",
        "#X_scaled = (cancer.data - np.mean(cancer.data, axis = 0) ) / np.sqrt(np.var(cancer.data, axis = 0))\n",
        "normalizer = preprocessing.MinMaxScaler()\n",
        "X_scaled = normalizer.fit_transform(cancer.data)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X_scaled, cancer.target, test_size = 0.2, random_state = 1)\n",
        "X_train, X_val , y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.125, random_state = 1)\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################\n",
        "print((X_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((y_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((X_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((y_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((X_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((y_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69.94727592267135 %\n",
            "69.94727592267135 %\n",
            "20.035149384885763 %\n",
            "20.035149384885763 %\n",
            "10.017574692442881 %\n",
            "10.017574692442881 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EZ3-Fm4uPIdf"
      },
      "source": [
        "# Ensemble Methods\n",
        "\n",
        "## Problem 1. Bagging (15 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZzSuFIANPPRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "9845dca6-2cda-4da8-bf4e-4d8ed6cd09eb"
      },
      "source": [
        "from sklearn import model_selection \n",
        "from sklearn.ensemble import BaggingClassifier \n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# TODO : initialize the base classifier. You can choose one of the classifiers #\n",
        "# you have learned in this course.(SVM/Decision tree)                          #\n",
        "# IMPORTANT: if you are using SVM as base classifier don't forget to add column#\n",
        "# of '1' s for bias and be careful to use the right datset in next parts.      #\n",
        "################################################################################\n",
        "base_cls = tree.DecisionTreeClassifier()\n",
        "  \n",
        "##################################################################################\n",
        "# TODO: Number of classifiers is a hyperparameter. Choose it by using validation #\n",
        "# data to have the best accuracy                                                 #\n",
        "# For different number of classifiers, train the model with training data and    #\n",
        "# compute accuracy for validation data. Plot accuracy-number of classifiers plot.#\n",
        "##################################################################################\n",
        "num_cls = None\n",
        "seed = None\n",
        "best_val_acc = 0\n",
        "best_model = None\n",
        "acc_val_history = list()\n",
        "for num_cls in range(1, 70):\n",
        "    val_acc = 0\n",
        "    for seed in range(0, 5):\n",
        "        model = BaggingClassifier(base_estimator = base_cls, \n",
        "                          n_estimators = num_cls, \n",
        "                          random_state = seed)\n",
        "        model.fit(X_train,y_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_acc += accuracy_score(y_val, y_val_pred)\n",
        "    acc_val_history.append(val_acc / 5)\n",
        "    if val_acc >= best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        print(num_cls, val_acc/5)\n",
        "            \n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(acc_val_history, label='validation')\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"num_cls\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "################################################################################\n",
        "# compute and report the accuracy for test data.                               #\n",
        "################################################################################\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "accuracy_score(y_test, y_test_pred)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 0.9473684210526316\n",
            "3 0.9578947368421054\n",
            "4 0.9614035087719298\n",
            "53 0.9614035087719298\n",
            "61 0.9614035087719298\n",
            "63 0.9614035087719298\n",
            "69 0.9614035087719298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.956140350877193"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ELyA0acXgK2w"
      },
      "source": [
        "## Problem 2. Random Forest(25 points)</br>\n",
        "In this part, you should write your own code to classify the data, using random forest from sklearn package in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JE4eKUybgQUb",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "9d1bbe08-230a-4909-c0ba-1c8b904a0d9f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "#################################################################################\n",
        "# TODO:use the validation data to determine hyperparameters(number and depth of #\n",
        "# trees) for the best accuracy                                                  # \n",
        "#################################################################################\n",
        "n_estimators , max_depth, random_state = None, None, None\n",
        "best_val_acc , best_model = 0, None\n",
        "for max_depth in range(1, 15):\n",
        "    for n_estimators in range(1 , 35):\n",
        "        for k in range(0,10, 3):\n",
        "            classifier = RandomForestClassifier(max_depth=max_depth, random_state=k, n_estimators =n_estimators)\n",
        "            classifier.fit(X_train, y_train)\n",
        "            y_val_predict = classifier.predict(X_val)\n",
        "            y_val_acc = accuracy_score(y_val, y_val_predict)\n",
        "            if y_val_acc > best_val_acc :\n",
        "                best_val_acc = y_val_acc\n",
        "                best_model = model\n",
        "                print(\" num of trees \", \"max_depth \", \"random_state bootstrapping \",n_estimators, max_depth, k)\n",
        "                print(accuracy_score(y_val, y_val_predict))\n",
        "            #if y_val_acc == best_val_acc :\n",
        "            #    best_val_acc = y_val_acc\n",
        "            #    best_model = model\n",
        "\n",
        "\n",
        "#######################################################################################\n",
        "#TODO:report accuracy, presition,recall and confusion matrix for train and test data  #\n",
        "#######################################################################################\n",
        "print(\"accuracy, precision,recall and confusion matrix for train data:\\n\")\n",
        "y_pred_train = best_model.predict(X_train)\n",
        "print(accuracy_score(y_train, y_pred_train))\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(confusion_matrix(y_train, y_pred_train))\n",
        "\n",
        "\n",
        "print(\"accuracy, precision,recall and confusion matrix for train data:\\n\")\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred_test))\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "print(confusion_matrix(y_test, y_pred_test))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " num of trees  max_depth  random_state bootstrapping  1 1 0\n",
            "0.8771929824561403\n",
            " num of trees  max_depth  random_state bootstrapping  1 1 3\n",
            "0.9473684210526315\n",
            " num of trees  max_depth  random_state bootstrapping  3 1 3\n",
            "0.9649122807017544\n",
            " num of trees  max_depth  random_state bootstrapping  3 3 6\n",
            "1.0\n",
            "accuracy, precision,recall and confusion matrix for train data:\n",
            "\n",
            "1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      1.00      1.00       151\n",
            "           1       1.00      1.00      1.00       247\n",
            "\n",
            "    accuracy                           1.00       398\n",
            "   macro avg       1.00      1.00      1.00       398\n",
            "weighted avg       1.00      1.00      1.00       398\n",
            "\n",
            "[[151   0]\n",
            " [  0 247]]\n",
            "accuracy, precision,recall and confusion matrix for train data:\n",
            "\n",
            "0.956140350877193\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.88      0.94        42\n",
            "           1       0.94      1.00      0.97        72\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.94      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "[[37  5]\n",
            " [ 0 72]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pSr7G0fdgmyf"
      },
      "source": [
        "Question:\n",
        "Explain how you did choose the hyperparameters.</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "etxNZ36Ugnp7"
      },
      "source": [
        "there are 3 paramaters that should be tune. depth of tree, number of trees and random seed in RandomForest Class . the last hyper parameter used in bootstrpping process . forech one I set an interval and the best combination picked and also the quality of different models measured by their accuracy on validation data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fxpbM42MPg6m"
      },
      "source": [
        "## Problem 3. Boosting : AdaBoost (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rUodQdBvPrKQ"
      },
      "source": [
        "In this part you should implement adaptive boosting algorithm. </br>\n",
        "<picture>\n",
        "  <img src=\"http://uupload.ir/files/b919_adaboost.png\" alt=\"Adaboost\" width=\"600\" height=\"300\">\n",
        "</picture>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r9TL5FGqRIoL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5573c236-c6d2-482d-bac0-14376ee470ca"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "import math\n",
        "X_train ,X_test ,y_train ,y_test = None ,None ,None ,None\n",
        "###################################################################\n",
        "# TODO: use 80% of normalized data as train and 20% as test data. #\n",
        "###################################################################\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, cancer.target, test_size = 0.2, random_state=1)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#TODO : define a weak decision tree.                                 #\n",
        "# initialize these parameters: criterion=\"entropy\" and max_depth = 1 #\n",
        "######################################################################\n",
        "Tree_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 1)\n",
        "#############################################################################################\n",
        "#TODO : report accuracy of your weak model on train and test data by using cross validation #\n",
        "#############################################################################################\n",
        "train_accuracy = np.mean(cross_validate(Tree_model, X_train, y_train)['test_score']) \n",
        "print('The training data accuracy is:' ,train_accuracy * 100 , '%')\n",
        "\n",
        "test_accuracy = np.mean(cross_validate(Tree_model, X_test, y_test)['test_score']) \n",
        "print('The test data accuracy is:' ,test_accuracy * 100 , '%')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training data accuracy is: 89.8901098901099 %\n",
            "The test data accuracy is: 85.9288537549407 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elsmjgbNRSdH",
        "colab": {}
      },
      "source": [
        "class AdaBoost:\n",
        "    \n",
        "    def __init__(self,train_data_X,train_data_y,tree_num,test_data_X,test_data_y):\n",
        "        self.train_data_X = train_data_X\n",
        "        self.train_data_y = train_data_y\n",
        "        self.tree_num = tree_num\n",
        "        self.test_data_X = test_data_X\n",
        "        self.test_data_y = test_data_y\n",
        "        self.alphas = None\n",
        "        self.models = None\n",
        "        self.accuracy = []\n",
        "        self.predictions = None\n",
        "        \n",
        "    def fit(self):\n",
        "        Evaluation = pd.DataFrame(self.train_data_y.copy())\n",
        "        Evaluation.columns = ['target']\n",
        "        ## TODO:Set the initial weights w = 1/N\n",
        "        Evaluation['weights'] = [1/np.size(self.train_data_X[:, 0])] * np.size(self.train_data_X[:, 0])\n",
        "        alphas = [] #list of alphas \n",
        "        models = [] # list of trained models\n",
        "        for t in range(self.tree_num):\n",
        "\n",
        "            ## TODO: create a weak decisiontree classifier\n",
        "            Tree_model =  DecisionTreeClassifier(criterion=\"entropy\", max_depth = 1)\n",
        "            ## TODO: fit the model with train data. set the sample_weight parameter to the 'weights' columns in Evaluation dataframe\n",
        "            model = Tree_model.fit(self.train_data_X, self.train_data_y, sample_weight = np.array(Evaluation['weights']))\n",
        "            \n",
        "            models.append(model)\n",
        "            predictions = model.predict(self.train_data_X)\n",
        "            score = model.score(self.train_data_X,self.train_data_y)\n",
        "\n",
        "            ## Add this columns to the Evaluation DataFrame\n",
        "            Evaluation['predictions'] = predictions\n",
        "            ## TODO: In each row if the prediction and the target are equal,this column must be '1' and '0' O.W. \n",
        "            Evaluation['evaluation'] = np.array(Evaluation['predictions']) * np.array(Evaluation['target'])\n",
        "            Evaluation['evaluation'] = np.where(np.array(Evaluation['evaluation'])< 0, 0, 1)\n",
        "            ## TODO: In each row if the tha data is missclassified, this column must be 1.\n",
        "            Evaluation['misclassified'] = np.logical_xor(np.array(Evaluation['evaluation']), 1)\n",
        "            Evaluation['misclassified'] = np.where(np.array(Evaluation['misclassified']) == False, 0, 1)\n",
        "            ## TODO: Calculate the misclassification rate and accuracy and then use them to calculate error\n",
        "            accuracy = np.count_nonzero(np.array(Evaluation['evaluation']) == 1) / np.size(self.train_data_X[:, 0])\n",
        "            misclassification = np.count_nonzero(np.array(Evaluation['misclassified']) == 1) / np.size(self.train_data_X[:, 0])\n",
        "            err = np.sum(np.array(Evaluation['misclassified']) *  np.array(Evaluation['weights']))\n",
        "            ## TODO: Calculate the alpha values from the adaboost algorithm\n",
        "            alpha = 1/2 * math.log((1- err)/err)\n",
        "            alphas.append(alpha)\n",
        "            ## TODO: update the weights\n",
        "            Evaluation['weights'] = np.array(Evaluation['weights']) * np.exp(-1 * np.array(Evaluation['predictions']) * np.array(Evaluation['target']) * alpha)\n",
        "            Evaluation['weights'] /= np.sum(np.array(Evaluation['weights']))\n",
        "\n",
        "        self.alphas = alphas\n",
        "        self.models = models\n",
        "        \n",
        "        \n",
        "    def predict(self):\n",
        "        \n",
        "        accuracy = []\n",
        "        predictions = []\n",
        "        #####################################################################################\n",
        "        #TODO:                                                                              #\n",
        "        # 1- predict target for test data and append each prediction to the predictions list#\n",
        "        # 2- Create a list of accuracies which can be used to plot the accuracy against the #\n",
        "        # number of base learners used for the model                                        #\n",
        "        #####################################################################################\n",
        "        prediction = 0\n",
        "        for alpha,model in zip(self.alphas,self.models):\n",
        "            prediction += alpha * model.predict(self.test_data_X)\n",
        "            predictions.append(prediction)\n",
        "            self.accuracy.append(accuracy_score(self.test_data_y, np.sign(prediction)))\n",
        "\n",
        "            \n",
        "        self.predictions = np.sign(np.sum(np.array(predictions),axis=0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-by9WfOXRVQG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "8431a8cb-77ee-44ab-f262-70b795bc26d8"
      },
      "source": [
        "# Accuracy - number of base learners plot for training data\n",
        "\n",
        "number_of_base_learners = 100\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax0 = fig.add_subplot(111)\n",
        "\n",
        "\n",
        "#for i in range(number_of_base_learners):\n",
        "model = AdaBoost(X_train,y_train,number_of_base_learners,X_train,y_train)\n",
        "model.fit()\n",
        "model.predict()\n",
        "\n",
        "ax0.plot(range(len(model.accuracy)),model.accuracy,'-b')\n",
        "ax0.set_xlabel('# models used for Boosting ')\n",
        "ax0.set_ylabel('accuracy')\n",
        "print('With a number of ',number_of_base_learners,'base models we receive an accuracy of ',model.accuracy[-1]*100,'%')    \n",
        "                 \n",
        "plt.show()   \n",
        "#################################################################### \n",
        "# TODO: Plot Accuracy - number of base learners plot for test data #\n",
        "####################################################################  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With a number of  100 base models we receive an accuracy of  100.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aHIp-4p9Ravu",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2olecphWRi4L"
      },
      "source": [
        "# Feature Selction </br>\n",
        "\n",
        "## problem4. Filtering : correlation coefficient (25 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oe0ynxveRmXS",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "4b94871a-38a7-4fb7-d4b8-e743e6031ad6"
      },
      "source": [
        "################################################################################# \n",
        "# TODO:                                                                         #\n",
        "# use 80% of normalized data as train and 20% as test data.(just use the data   # \n",
        "# from last part)                                                               #\n",
        "# 1- compute the correlation coefficient between each feature and target.       #\n",
        "# 2- Report the features that their correlation is more than 0.5                #\n",
        "# 3- compute the correlation between the features you reported in 2nd           #\n",
        "# section and report features that their correlation with other features        #\n",
        "# is less than 0.5                                                              #\n",
        "# 4- use perceptron from sklearn package to classify the data. Report accurracy #\n",
        "# for test data and sort the features based on their weights in perceptron.     #\n",
        "# IMPORTANT: Don't forget to add 1s to the end of feature vectors to be         #\n",
        "# multiplied by bias term of weight in perceptron.                              #\n",
        "# 5- compare the features you reported in section 2 and 3 with the features     #\n",
        "# that have the most weights in perceptron and write your analysis below        #\n",
        "# 6 - Classify data with perceptron and use only the features you repoted in    # \n",
        "# section 2 and report accuracy for test data.                                  #\n",
        "# 7 - Do the same with section 3 and compare accuracies.                        #\n",
        "#################################################################################\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.linear_model import Perceptron\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, cancer.target, test_size = 0.2, random_state=1)\n",
        "num_features = np.size(X_train[0, :])\n",
        "best_features = list()\n",
        "for i in range(num_features):\n",
        "    corr, _ = pearsonr(X_train[:, i], y_train)\n",
        "    if abs(corr) > 0.5:\n",
        "        best_features.append(i)\n",
        "print(\"section2 features:\", best_features)\n",
        "correlated_features = [0] * len(best_features)\n",
        "for i in range(len(best_features)):\n",
        "    for j in range(len(best_features)):\n",
        "        if i is not j :\n",
        "            corr, _ = pearsonr(X_train[:, i], X_train[:, j])\n",
        "            if abs(corr) < 0.5:\n",
        "                correlated_features[i] += 1\n",
        "correlated_features = np.array(correlated_features)\n",
        "sort = np.argsort(correlated_features)\n",
        "best_features1 = [best_features[sort[-3]], best_features[sort[-2]], best_features[sort[-1]]]\n",
        "print(\"setion3 features:\", best_features1)\n",
        "\n",
        "clf = Perceptron()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "weights = np.array([ abs(x) for x in clf.coef_])\n",
        "features = np.argsort(weights[0])\n",
        "print(\"most weighted features:\", np.flip(features, axis = 0))\n",
        "\n",
        "print(\"Perceptron accuracy with all features\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "clf1 = Perceptron()\n",
        "clf1 = clf1.fit(X_train[:, best_features], y_train)\n",
        "y_pred1 = clf1.predict(X_test[:, best_features])\n",
        "print(\"Perceptron accracy with section2 features\", accuracy_score(y_test, y_pred1))\n",
        "\n",
        "\n",
        "clf2 = Perceptron()\n",
        "clf2 = clf2.fit(X_train[:, best_features1], y_train)\n",
        "y_pred2 = clf2.predict(X_test[:, best_features1])\n",
        "print(\"Perceptron accracy with section3 features\", accuracy_score(y_test, y_pred2))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "section2 features: [0, 2, 3, 5, 6, 7, 10, 12, 13, 20, 22, 23, 25, 26, 27]\n",
            "setion3 features: [2, 23, 27]\n",
            "most weighted features: [ 7 21 20  6 23 10 22 27 24  9 28 12 26 13  1 15 14 19  3 11 29  5  8 16\n",
            " 25  2 17 18  0  4]\n",
            "Perceptron accuracy with all features 0.9649122807017544\n",
            "Perceptron accracy with section2 features 0.8859649122807017\n",
            "Perceptron accracy with section3 features 0.9035087719298246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o0QJrI36-GNR"
      },
      "source": [
        "explanation of part 5 and 6:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iTC2KZ1h-QO6"
      },
      "source": [
        "همان طور که در خروجی های گزارش شده در بالا مشاهده میشود ویژگی هایی که در بخش 2 انتخاب شدند 15 تا هستند که 9 تای آنها در میان 15 تا ویژگی هستند که در الگوریتم پرسپترون بیشترین وزن را دارند . هم چنین ویژگی های گزارش شده در بخش سوم 3 عدد هستند که دو تا از این 3 ویژگی در میان 4 تا پر وزنترین ویژگی ها هستند . علاوه بر این دقتی که بر روی داده تست با استفاده از ویژگی های بخش 2 و 3 بدست آورده ایم دقت مناسبی بوده و نزدیک حالتی است که از همه فیچر ها استفاده کردیم . از آن جا که میدانیم ارتباطی میان وزن ویژگی ها در دسته بند خطی و اهمیت ویژگی وجود دارد (این مساله نیز به راحتی قابل بررسی است زیرا هرچه وزن یک ویژگی بیشتر باشد مقدار آن ویژگی تاثیر بیشتری در دسته بندی میگذارد ) اینکه دقت ما بر روی داده تست نزدیک به حالت کلی  است و اینکه فیچر های استخراج شده تقریبا همان ویژگی های با وزن زیاد هستند نشان میدهد فرایند انتخاب ویژگی را به درستی انجام داده ایم .  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k_XG0iKSRqnE"
      },
      "source": [
        "Question: Is it important to extract features before classifying using methods like decision tree and SVM? why? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ro6v3JSBRsd5"
      },
      "source": [
        "استخراج ویژگی ها هم به جهت کاهش احتمال اورفیت برروی داده و هم به جهت کاهش بار سنگین محاسبات تکنیک بسیار مناسبی است . به عنوان مثال مدل هایی مانند درخت تصمیم ممکن است بر روی داده ترین اورفیت داشته باشند بنابراین لازم است تا با استخراج ویژگی ها یا به عبارتی کوچک کردن فضای فرضیه پیچیدگی مدل را کاهش دهیم . چنین مفهومی برای ماشین بردار پشتیبان نیز قابل بررسی است زیرا استخراج فیچر به معنای صفر کردن برخی مولفه های بردار وزن است در نتیجه پیچیدگی مدل کاهش و اورفیت کاهش میابد . در نتیجه اگر قبل از فرایند اموزش ویزگی ها را استخراج کنیم میتوانیم مدلی با سربار محسبات کمتر و پیچیدگی کمتر اموزش دهیم "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eQjRm5a6j8KZ"
      },
      "source": [
        "## problem 5. mRMR (10 bonus points) </br>\n",
        "In this part you should write your own code and classify the data using mRMR method.You can use \"pymrmr\" package for this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xQs-LPupRoDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3c26c92d-efa1-468b-dad0-7c5560e247e5"
      },
      "source": [
        "import pymrmr\n",
        "\n",
        "data = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
        "data.insert(loc=0, column='t', value=cancer[\"target\"])\n",
        "data = data.drop(data.columns[[31]], axis=1)\n",
        "pymrmr.mRMR(data, 'MIQ', 10)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['worst area',\n",
              " 'radius error',\n",
              " 'worst compactness',\n",
              " 'worst radius',\n",
              " 'perimeter error',\n",
              " 'mean radius',\n",
              " 'worst perimeter',\n",
              " 'area error',\n",
              " 'mean perimeter',\n",
              " 'mean area']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI1Wsvy3hL9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "6bff983f-0139-4286-8f1f-7e917628edca"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymrmr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/ab/903712947a2f5cd1af249132885dbd81ae8bf8cfd30fb3b3f2beddab23e8/pymrmr-0.1.8.tar.gz (65kB)\n",
            "\r\u001b[K     |█████                           | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pymrmr) (1.18.3)\n",
            "Building wheels for collected packages: pymrmr\n",
            "  Building wheel for pymrmr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymrmr: filename=pymrmr-0.1.8-cp36-cp36m-linux_x86_64.whl size=256761 sha256=35e8482943f0872dc90e70706b414be02e534b17ff54ea5231cb35b2e9387692\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ce/3a/bc9b80047f68973d909a35bb8e3062b7c7377510607ec35998\n",
            "Successfully built pymrmr\n",
            "Installing collected packages: pymrmr\n",
            "Successfully installed pymrmr-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1sT4NuliqeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}