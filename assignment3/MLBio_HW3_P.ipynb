{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLBio_HW3_P.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "efrE1Lc0Otrq"
      },
      "source": [
        "# **Machine Learning in Bioinformatics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GtzvENmYOtxi"
      },
      "source": [
        "**Homework 3:**<br/>\n",
        "!!! If you don't fill these fields, your homework does not count !!!<br/>\n",
        "first name and last name :amirreza kazemi<br/>\n",
        "student number :95105827"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KG3vb72VOt4G"
      },
      "source": [
        "You can run cells by hitting `Shift` + `Enter` or `ctrl` + `Enter`.<br/>\n",
        "We highly recommend you to read each line of code carefully and try to \n",
        "understand what it exactly does.<br/>\n",
        "Just alter the parts that is between green comments and specified for you. <br/>\n",
        "Please do not change other parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v0dotHjRO5x_",
        "colab": {}
      },
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_5C6lgUtO-bg"
      },
      "source": [
        "\n",
        "### about the Data:<br/>\n",
        "The purpose of this project is to classify tumors into malignant or benign. The following dataset is constructed based on images of tumors. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
        "For more details about the features of this dataset you can visit this link:\n",
        "https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset<br/>\n",
        "This dataset contains 30 features and 1 label called target.\n",
        "The original dataset labels are 0 and 1 and in the following code boxes we change it to -1 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-7_g8ApcO7tm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "f6381100-49b0-477b-fc75-7f109ef9e654"
      },
      "source": [
        "cancer = load_breast_cancer()  ## change if the data set changed\n",
        "df = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
        "features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  ...  worst fractal dimension  target\n",
              "0        17.99         10.38  ...                  0.11890     0.0\n",
              "1        20.57         17.77  ...                  0.08902     0.0\n",
              "2        19.69         21.25  ...                  0.08758     0.0\n",
              "3        11.42         20.38  ...                  0.17300     0.0\n",
              "4        20.29         14.34  ...                  0.07678     0.0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmmK95OVPDyg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "1711cced-5202-4137-e019-4d7a90600fec"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "cancer.target = np.where(cancer.target==0, -1, cancer.target)\n",
        "X_train ,X_test ,X_val ,y_train ,y_test ,y_val = None ,None ,None ,None ,None ,None\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# 1- Normalize tha data.                                                       #\n",
        "# 2- using train_test_split package, split your data into 3 numpy array        #\n",
        "# called X_train, X_test, and X_val and also split the corresponding labels as #\n",
        "# y_train, y_test, and y_val. After spliting, the ratio of your data should be # \n",
        "# approximately like this:                                                     #\n",
        "#  Train : 70%     test : 20%       validation : 10%                           #\n",
        "################################################################################\n",
        "#X_scaled = (cancer.data - np.mean(cancer.data, axis = 0) ) / np.sqrt(np.var(cancer.data, axis = 0))\n",
        "normalizer = preprocessing.MinMaxScaler()\n",
        "X_scaled = normalizer.fit_transform(cancer.data)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X_scaled, cancer.target, test_size = 0.2, random_state = 1)\n",
        "X_train, X_val , y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.125, random_state = 1)\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################\n",
        "print((X_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((y_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((X_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((y_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((X_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
        "print((y_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69.94727592267135 %\n",
            "69.94727592267135 %\n",
            "20.035149384885763 %\n",
            "20.035149384885763 %\n",
            "10.017574692442881 %\n",
            "10.017574692442881 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EZ3-Fm4uPIdf"
      },
      "source": [
        "# Ensemble Methods\n",
        "\n",
        "## Problem 1. Bagging (15 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZzSuFIANPPRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "9845dca6-2cda-4da8-bf4e-4d8ed6cd09eb"
      },
      "source": [
        "from sklearn import model_selection \n",
        "from sklearn.ensemble import BaggingClassifier \n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# TODO : initialize the base classifier. You can choose one of the classifiers #\n",
        "# you have learned in this course.(SVM/Decision tree)                          #\n",
        "# IMPORTANT: if you are using SVM as base classifier don't forget to add column#\n",
        "# of '1' s for bias and be careful to use the right datset in next parts.      #\n",
        "################################################################################\n",
        "base_cls = tree.DecisionTreeClassifier()\n",
        "  \n",
        "##################################################################################\n",
        "# TODO: Number of classifiers is a hyperparameter. Choose it by using validation #\n",
        "# data to have the best accuracy                                                 #\n",
        "# For different number of classifiers, train the model with training data and    #\n",
        "# compute accuracy for validation data. Plot accuracy-number of classifiers plot.#\n",
        "##################################################################################\n",
        "num_cls = None\n",
        "seed = None\n",
        "best_val_acc = 0\n",
        "best_model = None\n",
        "acc_val_history = list()\n",
        "for num_cls in range(1, 70):\n",
        "    val_acc = 0\n",
        "    for seed in range(0, 5):\n",
        "        model = BaggingClassifier(base_estimator = base_cls, \n",
        "                          n_estimators = num_cls, \n",
        "                          random_state = seed)\n",
        "        model.fit(X_train,y_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_acc += accuracy_score(y_val, y_val_pred)\n",
        "    acc_val_history.append(val_acc / 5)\n",
        "    if val_acc >= best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = model\n",
        "        print(num_cls, val_acc/5)\n",
        "            \n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(acc_val_history, label='validation')\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"num_cls\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "################################################################################\n",
        "# compute and report the accuracy for test data.                               #\n",
        "################################################################################\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "accuracy_score(y_test, y_test_pred)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 0.9473684210526316\n",
            "3 0.9578947368421054\n",
            "4 0.9614035087719298\n",
            "53 0.9614035087719298\n",
            "61 0.9614035087719298\n",
            "63 0.9614035087719298\n",
            "69 0.9614035087719298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.956140350877193"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ELyA0acXgK2w"
      },
      "source": [
        "## Problem 2. Random Forest(25 points)</br>\n",
        "In this part, you should write your own code to classify the data, using random forest from sklearn package in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JE4eKUybgQUb",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "9d1bbe08-230a-4909-c0ba-1c8b904a0d9f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "#################################################################################\n",
        "# TODO:use the validation data to determine hyperparameters(number and depth of #\n",
        "# trees) for the best accuracy                                                  # \n",
        "#################################################################################\n",
        "n_estimators , max_depth, random_state = None, None, None\n",
        "best_val_acc , best_model = 0, None\n",
        "for max_depth in range(1, 15):\n",
        "    for n_estimators in range(1 , 35):\n",
        "        for k in range(0,10, 3):\n",
        "            classifier = RandomForestClassifier(max_depth=max_depth, random_state=k, n_estimators =n_estimators)\n",
        "            classifier.fit(X_train, y_train)\n",
        "            y_val_predict = classifier.predict(X_val)\n",
        "            y_val_acc = accuracy_score(y_val, y_val_predict)\n",
        "            if y_val_acc > best_val_acc :\n",
        "                best_val_acc = y_val_acc\n",
        "                best_model = model\n",
        "                print(\" num of trees \", \"max_depth \", \"random_state bootstrapping \",n_estimators, max_depth, k)\n",
        "                print(accuracy_score(y_val, y_val_predict))\n",
        "            #if y_val_acc == best_val_acc :\n",
        "            #    best_val_acc = y_val_acc\n",
        "            #    best_model = model\n",
        "\n",
        "\n",
        "#######################################################################################\n",
        "#TODO:report accuracy, presition,recall and confusion matrix for train and test data  #\n",
        "#######################################################################################\n",
        "print(\"accuracy, precision,recall and confusion matrix for train data:\\n\")\n",
        "y_pred_train = best_model.predict(X_train)\n",
        "print(accuracy_score(y_train, y_pred_train))\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(confusion_matrix(y_train, y_pred_train))\n",
        "\n",
        "\n",
        "print(\"accuracy, precision,recall and confusion matrix for train data:\\n\")\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred_test))\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "print(confusion_matrix(y_test, y_pred_test))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " num of trees  max_depth  random_state bootstrapping  1 1 0\n",
            "0.8771929824561403\n",
            " num of trees  max_depth  random_state bootstrapping  1 1 3\n",
            "0.9473684210526315\n",
            " num of trees  max_depth  random_state bootstrapping  3 1 3\n",
            "0.9649122807017544\n",
            " num of trees  max_depth  random_state bootstrapping  3 3 6\n",
            "1.0\n",
            "accuracy, precision,recall and confusion matrix for train data:\n",
            "\n",
            "1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      1.00      1.00       151\n",
            "           1       1.00      1.00      1.00       247\n",
            "\n",
            "    accuracy                           1.00       398\n",
            "   macro avg       1.00      1.00      1.00       398\n",
            "weighted avg       1.00      1.00      1.00       398\n",
            "\n",
            "[[151   0]\n",
            " [  0 247]]\n",
            "accuracy, precision,recall and confusion matrix for train data:\n",
            "\n",
            "0.956140350877193\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.88      0.94        42\n",
            "           1       0.94      1.00      0.97        72\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.94      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "[[37  5]\n",
            " [ 0 72]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pSr7G0fdgmyf"
      },
      "source": [
        "Question:\n",
        "Explain how you did choose the hyperparameters.</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "etxNZ36Ugnp7"
      },
      "source": [
        "there are 3 paramaters that should be tune. depth of tree, number of trees and random seed in RandomForest Class . the last hyper parameter used in bootstrpping process . forech one I set an interval and the best combination picked and also the quality of different models measured by their accuracy on validation data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fxpbM42MPg6m"
      },
      "source": [
        "## Problem 3. Boosting : AdaBoost (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rUodQdBvPrKQ"
      },
      "source": [
        "In this part you should implement adaptive boosting algorithm. </br>\n",
        "<picture>\n",
        "  <img src=\"http://uupload.ir/files/b919_adaboost.png\" alt=\"Adaboost\" width=\"600\" height=\"300\">\n",
        "</picture>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r9TL5FGqRIoL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5573c236-c6d2-482d-bac0-14376ee470ca"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "import math\n",
        "X_train ,X_test ,y_train ,y_test = None ,None ,None ,None\n",
        "###################################################################\n",
        "# TODO: use 80% of normalized data as train and 20% as test data. #\n",
        "###################################################################\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, cancer.target, test_size = 0.2, random_state=1)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "#TODO : define a weak decision tree.                                 #\n",
        "# initialize these parameters: criterion=\"entropy\" and max_depth = 1 #\n",
        "######################################################################\n",
        "Tree_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 1)\n",
        "#############################################################################################\n",
        "#TODO : report accuracy of your weak model on train and test data by using cross validation #\n",
        "#############################################################################################\n",
        "train_accuracy = np.mean(cross_validate(Tree_model, X_train, y_train)['test_score']) \n",
        "print('The training data accuracy is:' ,train_accuracy * 100 , '%')\n",
        "\n",
        "test_accuracy = np.mean(cross_validate(Tree_model, X_test, y_test)['test_score']) \n",
        "print('The test data accuracy is:' ,test_accuracy * 100 , '%')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training data accuracy is: 89.8901098901099 %\n",
            "The test data accuracy is: 85.9288537549407 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elsmjgbNRSdH",
        "colab": {}
      },
      "source": [
        "class AdaBoost:\n",
        "    \n",
        "    def __init__(self,train_data_X,train_data_y,tree_num,test_data_X,test_data_y):\n",
        "        self.train_data_X = train_data_X\n",
        "        self.train_data_y = train_data_y\n",
        "        self.tree_num = tree_num\n",
        "        self.test_data_X = test_data_X\n",
        "        self.test_data_y = test_data_y\n",
        "        self.alphas = None\n",
        "        self.models = None\n",
        "        self.accuracy = []\n",
        "        self.predictions = None\n",
        "        \n",
        "    def fit(self):\n",
        "        Evaluation = pd.DataFrame(self.train_data_y.copy())\n",
        "        Evaluation.columns = ['target']\n",
        "        ## TODO:Set the initial weights w = 1/N\n",
        "        Evaluation['weights'] = [1/np.size(self.train_data_X[:, 0])] * np.size(self.train_data_X[:, 0])\n",
        "        alphas = [] #list of alphas \n",
        "        models = [] # list of trained models\n",
        "        for t in range(self.tree_num):\n",
        "\n",
        "            ## TODO: create a weak decisiontree classifier\n",
        "            Tree_model =  DecisionTreeClassifier(criterion=\"entropy\", max_depth = 1)\n",
        "            ## TODO: fit the model with train data. set the sample_weight parameter to the 'weights' columns in Evaluation dataframe\n",
        "            model = Tree_model.fit(self.train_data_X, self.train_data_y, sample_weight = np.array(Evaluation['weights']))\n",
        "            \n",
        "            models.append(model)\n",
        "            predictions = model.predict(self.train_data_X)\n",
        "            score = model.score(self.train_data_X,self.train_data_y)\n",
        "\n",
        "            ## Add this columns to the Evaluation DataFrame\n",
        "            Evaluation['predictions'] = predictions\n",
        "            ## TODO: In each row if the prediction and the target are equal,this column must be '1' and '0' O.W. \n",
        "            Evaluation['evaluation'] = np.array(Evaluation['predictions']) * np.array(Evaluation['target'])\n",
        "            Evaluation['evaluation'] = np.where(np.array(Evaluation['evaluation'])< 0, 0, 1)\n",
        "            ## TODO: In each row if the tha data is missclassified, this column must be 1.\n",
        "            Evaluation['misclassified'] = np.logical_xor(np.array(Evaluation['evaluation']), 1)\n",
        "            Evaluation['misclassified'] = np.where(np.array(Evaluation['misclassified']) == False, 0, 1)\n",
        "            ## TODO: Calculate the misclassification rate and accuracy and then use them to calculate error\n",
        "            accuracy = np.count_nonzero(np.array(Evaluation['evaluation']) == 1) / np.size(self.train_data_X[:, 0])\n",
        "            misclassification = np.count_nonzero(np.array(Evaluation['misclassified']) == 1) / np.size(self.train_data_X[:, 0])\n",
        "            err = np.sum(np.array(Evaluation['misclassified']) *  np.array(Evaluation['weights']))\n",
        "            ## TODO: Calculate the alpha values from the adaboost algorithm\n",
        "            alpha = 1/2 * math.log((1- err)/err)\n",
        "            alphas.append(alpha)\n",
        "            ## TODO: update the weights\n",
        "            Evaluation['weights'] = np.array(Evaluation['weights']) * np.exp(-1 * np.array(Evaluation['predictions']) * np.array(Evaluation['target']) * alpha)\n",
        "            Evaluation['weights'] /= np.sum(np.array(Evaluation['weights']))\n",
        "\n",
        "        self.alphas = alphas\n",
        "        self.models = models\n",
        "        \n",
        "        \n",
        "    def predict(self):\n",
        "        \n",
        "        accuracy = []\n",
        "        predictions = []\n",
        "        #####################################################################################\n",
        "        #TODO:                                                                              #\n",
        "        # 1- predict target for test data and append each prediction to the predictions list#\n",
        "        # 2- Create a list of accuracies which can be used to plot the accuracy against the #\n",
        "        # number of base learners used for the model                                        #\n",
        "        #####################################################################################\n",
        "        prediction = 0\n",
        "        for alpha,model in zip(self.alphas,self.models):\n",
        "            prediction += alpha * model.predict(self.test_data_X)\n",
        "            predictions.append(prediction)\n",
        "            self.accuracy.append(accuracy_score(self.test_data_y, np.sign(prediction)))\n",
        "\n",
        "            \n",
        "        self.predictions = np.sign(np.sum(np.array(predictions),axis=0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-by9WfOXRVQG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "8431a8cb-77ee-44ab-f262-70b795bc26d8"
      },
      "source": [
        "# Accuracy - number of base learners plot for training data\n",
        "\n",
        "number_of_base_learners = 100\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax0 = fig.add_subplot(111)\n",
        "\n",
        "\n",
        "#for i in range(number_of_base_learners):\n",
        "model = AdaBoost(X_train,y_train,number_of_base_learners,X_train,y_train)\n",
        "model.fit()\n",
        "model.predict()\n",
        "\n",
        "ax0.plot(range(len(model.accuracy)),model.accuracy,'-b')\n",
        "ax0.set_xlabel('# models used for Boosting ')\n",
        "ax0.set_ylabel('accuracy')\n",
        "print('With a number of ',number_of_base_learners,'base models we receive an accuracy of ',model.accuracy[-1]*100,'%')    \n",
        "                 \n",
        "plt.show()   \n",
        "#################################################################### \n",
        "# TODO: Plot Accuracy - number of base learners plot for test data #\n",
        "####################################################################  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With a number of  100 base models we receive an accuracy of  100.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aHIp-4p9Ravu",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2olecphWRi4L"
      },
      "source": [
        "# Feature Selction </br>\n",
        "\n",
        "## problem4. Filtering : correlation coefficient (25 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oe0ynxveRmXS",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "4b94871a-38a7-4fb7-d4b8-e743e6031ad6"
      },
      "source": [
        "################################################################################# \n",
        "# TODO:                                                                         #\n",
        "# use 80% of normalized data as train and 20% as test data.(just use the data   # \n",
        "# from last part)                                                               #\n",
        "# 1- compute the correlation coefficient between each feature and target.       #\n",
        "# 2- Report the features that their correlation is more than 0.5                #\n",
        "# 3- compute the correlation between the features you reported in 2nd           #\n",
        "# section and report features that their correlation with other features        #\n",
        "# is less than 0.5                                                              #\n",
        "# 4- use perceptron from sklearn package to classify the data. Report accurracy #\n",
        "# for test data and sort the features based on their weights in perceptron.     #\n",
        "# IMPORTANT: Don't forget to add 1s to the end of feature vectors to be         #\n",
        "# multiplied by bias term of weight in perceptron.                              #\n",
        "# 5- compare the features you reported in section 2 and 3 with the features     #\n",
        "# that have the most weights in perceptron and write your analysis below        #\n",
        "# 6 - Classify data with perceptron and use only the features you repoted in    # \n",
        "# section 2 and report accuracy for test data.                                  #\n",
        "# 7 - Do the same with section 3 and compare accuracies.                        #\n",
        "#################################################################################\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.linear_model import Perceptron\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, cancer.target, test_size = 0.2, random_state=1)\n",
        "num_features = np.size(X_train[0, :])\n",
        "best_features = list()\n",
        "for i in range(num_features):\n",
        "    corr, _ = pearsonr(X_train[:, i], y_train)\n",
        "    if abs(corr) > 0.5:\n",
        "        best_features.append(i)\n",
        "print(\"section2 features:\", best_features)\n",
        "correlated_features = [0] * len(best_features)\n",
        "for i in range(len(best_features)):\n",
        "    for j in range(len(best_features)):\n",
        "        if i is not j :\n",
        "            corr, _ = pearsonr(X_train[:, i], X_train[:, j])\n",
        "            if abs(corr) < 0.5:\n",
        "                correlated_features[i] += 1\n",
        "correlated_features = np.array(correlated_features)\n",
        "sort = np.argsort(correlated_features)\n",
        "best_features1 = [best_features[sort[-3]], best_features[sort[-2]], best_features[sort[-1]]]\n",
        "print(\"setion3 features:\", best_features1)\n",
        "\n",
        "clf = Perceptron()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "weights = np.array([ abs(x) for x in clf.coef_])\n",
        "features = np.argsort(weights[0])\n",
        "print(\"most weighted features:\", np.flip(features, axis = 0))\n",
        "\n",
        "print(\"Perceptron accuracy with all features\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "clf1 = Perceptron()\n",
        "clf1 = clf1.fit(X_train[:, best_features], y_train)\n",
        "y_pred1 = clf1.predict(X_test[:, best_features])\n",
        "print(\"Perceptron accracy with section2 features\", accuracy_score(y_test, y_pred1))\n",
        "\n",
        "\n",
        "clf2 = Perceptron()\n",
        "clf2 = clf2.fit(X_train[:, best_features1], y_train)\n",
        "y_pred2 = clf2.predict(X_test[:, best_features1])\n",
        "print(\"Perceptron accracy with section3 features\", accuracy_score(y_test, y_pred2))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "section2 features: [0, 2, 3, 5, 6, 7, 10, 12, 13, 20, 22, 23, 25, 26, 27]\n",
            "setion3 features: [2, 23, 27]\n",
            "most weighted features: [ 7 21 20  6 23 10 22 27 24  9 28 12 26 13  1 15 14 19  3 11 29  5  8 16\n",
            " 25  2 17 18  0  4]\n",
            "Perceptron accuracy with all features 0.9649122807017544\n",
            "Perceptron accracy with section2 features 0.8859649122807017\n",
            "Perceptron accracy with section3 features 0.9035087719298246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o0QJrI36-GNR"
      },
      "source": [
        "explanation of part 5 and 6:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iTC2KZ1h-QO6"
      },
      "source": [
        "                 2   15    9     15            .          3        3    4       .                2  3                  .                    (                        )                                       .  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k_XG0iKSRqnE"
      },
      "source": [
        "Question: Is it important to extract features before classifying using methods like decision tree and SVM? why? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ro6v3JSBRsd5"
      },
      "source": [
        "                       .                                      .                                 .                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eQjRm5a6j8KZ"
      },
      "source": [
        "## problem 5. mRMR (10 bonus points) </br>\n",
        "In this part you should write your own code and classify the data using mRMR method.You can use \"pymrmr\" package for this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xQs-LPupRoDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3c26c92d-efa1-468b-dad0-7c5560e247e5"
      },
      "source": [
        "import pymrmr\n",
        "\n",
        "data = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
        "data.insert(loc=0, column='t', value=cancer[\"target\"])\n",
        "data = data.drop(data.columns[[31]], axis=1)\n",
        "pymrmr.mRMR(data, 'MIQ', 10)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['worst area',\n",
              " 'radius error',\n",
              " 'worst compactness',\n",
              " 'worst radius',\n",
              " 'perimeter error',\n",
              " 'mean radius',\n",
              " 'worst perimeter',\n",
              " 'area error',\n",
              " 'mean perimeter',\n",
              " 'mean area']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI1Wsvy3hL9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "6bff983f-0139-4286-8f1f-7e917628edca"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymrmr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/ab/903712947a2f5cd1af249132885dbd81ae8bf8cfd30fb3b3f2beddab23e8/pymrmr-0.1.8.tar.gz (65kB)\n",
            "\r\u001b[K     |                           | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |                      | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |                 | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |       | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |  | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     || 71kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pymrmr) (1.18.3)\n",
            "Building wheels for collected packages: pymrmr\n",
            "  Building wheel for pymrmr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymrmr: filename=pymrmr-0.1.8-cp36-cp36m-linux_x86_64.whl size=256761 sha256=35e8482943f0872dc90e70706b414be02e534b17ff54ea5231cb35b2e9387692\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ce/3a/bc9b80047f68973d909a35bb8e3062b7c7377510607ec35998\n",
            "Successfully built pymrmr\n",
            "Installing collected packages: pymrmr\n",
            "Successfully installed pymrmr-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1sT4NuliqeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}